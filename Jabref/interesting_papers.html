<!DOCTYPE HTML>
<html>
<head>
<title>JabRef references</title>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<script type="text/javascript">
<!--
// QuickSearch script for JabRef HTML export 
// Version: 3.0
//
// Copyright (c) 2006-2011, Mark Schenk
//
// This software is distributed under a Creative Commons Attribution 3.0 License
// http://creativecommons.org/licenses/by/3.0/
//
// Features:
// - intuitive find-as-you-type searching
//    ~ case insensitive
//    ~ ignore diacritics (optional)
//
// - search with/without Regular Expressions
// - match BibTeX key
//

// Search settings
var searchAbstract = true;	// search in abstract
var searchComment = true;	// search in comment

var noSquiggles = true; 	// ignore diacritics when searching
var searchRegExp = false; 	// enable RegExp searches


if (window.addEventListener) {
	window.addEventListener("load",initSearch,false); }
else if (window.attachEvent) {
	window.attachEvent("onload", initSearch); }

function initSearch() {
	// check for quick search table and searchfield
	if (!document.getElementById('qs_table')||!document.getElementById('quicksearch')) { return; }

	// load all the rows and sort into arrays
	loadTableData();
	
	//find the query field
	qsfield = document.getElementById('qs_field');

	// previous search term; used for speed optimisation
	prevSearch = '';

	//find statistics location
	stats = document.getElementById('stat');
	setStatistics(-1);
	
	// set up preferences
	initPreferences();

	// shows the searchfield
	document.getElementById('quicksearch').style.display = 'block';
	document.getElementById('qs_field').onkeyup = quickSearch;
}

function loadTableData() {
	// find table and appropriate rows
	searchTable = document.getElementById('qs_table');
	var allRows = searchTable.getElementsByTagName('tbody')[0].getElementsByTagName('tr');

	// split all rows into entryRows and infoRows (e.g. abstract, comment, bibtex)
	entryRows = new Array(); infoRows = new Array(); absRows = new Array(); revRows = new Array();

	// get data from each row
	entryRowsData = new Array(); absRowsData = new Array(); revRowsData = new Array(); 
	
	BibTeXKeys = new Array();
	
	for (var i=0, k=0, j=0; i<allRows.length;i++) {
		if (allRows[i].className.match(/entry/)) {
			entryRows[j] = allRows[i];
			entryRowsData[j] = stripDiacritics(getTextContent(allRows[i]));
			allRows[i].id ? BibTeXKeys[j] = allRows[i].id : allRows[i].id = 'autokey_'+j;
			j ++;
		} else {
			infoRows[k++] = allRows[i];
			// check for abstract/comment
			if (allRows[i].className.match(/abstract/)) {
				absRows.push(allRows[i]);
				absRowsData[j-1] = stripDiacritics(getTextContent(allRows[i]));
			} else if (allRows[i].className.match(/comment/)) {
				revRows.push(allRows[i]);
				revRowsData[j-1] = stripDiacritics(getTextContent(allRows[i]));
			}
		}
	}
	//number of entries and rows
	numEntries = entryRows.length;
	numInfo = infoRows.length;
	numAbs = absRows.length;
	numRev = revRows.length;
}

function quickSearch(){
	
	tInput = qsfield;

	if (tInput.value.length == 0) {
		showAll();
		setStatistics(-1);
		qsfield.className = '';
		return;
	} else {
		t = stripDiacritics(tInput.value);

		if(!searchRegExp) { t = escapeRegExp(t); }
			
		// only search for valid RegExp
		try {
			textRegExp = new RegExp(t,"i");
			closeAllInfo();
			qsfield.className = '';
		}
			catch(err) {
			prevSearch = tInput.value;
			qsfield.className = 'invalidsearch';
			return;
		}
	}
	
	// count number of hits
	var hits = 0;

	// start looping through all entry rows
	for (var i = 0; cRow = entryRows[i]; i++){

		// only show search the cells if it isn't already hidden OR if the search term is getting shorter, then search all
		if(cRow.className.indexOf('noshow')==-1 || tInput.value.length <= prevSearch.length){
			var found = false; 

			if (entryRowsData[i].search(textRegExp) != -1 || BibTeXKeys[i].search(textRegExp) != -1){ 
				found = true;
			} else {
				if(searchAbstract && absRowsData[i]!=undefined) {
					if (absRowsData[i].search(textRegExp) != -1){ found=true; } 
				}
				if(searchComment && revRowsData[i]!=undefined) {
					if (revRowsData[i].search(textRegExp) != -1){ found=true; } 
				}
			}
			
			if (found){
				cRow.className = 'entry show';
				hits++;
			} else {
				cRow.className = 'entry noshow';
			}
		}
	}

	// update statistics
	setStatistics(hits)
	
	// set previous search value
	prevSearch = tInput.value;
}


// Strip Diacritics from text
// http://stackoverflow.com/questions/990904/javascript-remove-accents-in-strings

// String containing replacement characters for stripping accents 
var stripstring = 
    'AAAAAAACEEEEIIII'+
    'DNOOOOO.OUUUUY..'+
    'aaaaaaaceeeeiiii'+
    'dnooooo.ouuuuy.y'+
    'AaAaAaCcCcCcCcDd'+
    'DdEeEeEeEeEeGgGg'+
    'GgGgHhHhIiIiIiIi'+
    'IiIiJjKkkLlLlLlL'+
    'lJlNnNnNnnNnOoOo'+
    'OoOoRrRrRrSsSsSs'+
    'SsTtTtTtUuUuUuUu'+
    'UuUuWwYyYZzZzZz.';

function stripDiacritics(str){

    if(noSquiggles==false){
        return str;
    }

    var answer='';
    for(var i=0;i<str.length;i++){
        var ch=str[i];
        var chindex=ch.charCodeAt(0)-192;   // Index of character code in the strip string
        if(chindex>=0 && chindex<stripstring.length){
            // Character is within our table, so we can strip the accent...
            var outch=stripstring.charAt(chindex);
            // ...unless it was shown as a '.'
            if(outch!='.')ch=outch;
        }
        answer+=ch;
    }
    return answer;
}

// http://stackoverflow.com/questions/3446170/escape-string-for-use-in-javascript-regex
// NOTE: must escape every \ in the export code because of the JabRef Export...
function escapeRegExp(str) {
  return str.replace(/[-\[\]\/\{\}\(\)\*\+\?\.\\\^\$\|]/g, "\\$&");
}

function toggleInfo(articleid,info) {

	var entry = document.getElementById(articleid);
	var abs = document.getElementById('abs_'+articleid);
	var rev = document.getElementById('rev_'+articleid);
	var bib = document.getElementById('bib_'+articleid);
	
	if (abs && info == 'abstract') {
		abs.className.indexOf('noshow') == -1?abs.className = 'abstract noshow':abs.className = 'abstract show';
	} else if (rev && info == 'comment') {
		rev.className.indexOf('noshow') == -1?rev.className = 'comment noshow':rev.className = 'comment show';
	} else if (bib && info == 'bibtex') {
		bib.className.indexOf('noshow') == -1?bib.className = 'bibtex noshow':bib.className = 'bibtex show';
	} else { 
		return;
	}

	// check if one or the other is available
	var revshow; var absshow; var bibshow;
	(abs && abs.className.indexOf('noshow') == -1)? absshow = true: absshow = false;
	(rev && rev.className.indexOf('noshow') == -1)? revshow = true: revshow = false;	
	(bib && bib.className.indexOf('noshow') == -1)? bibshow = true: bibshow = false;
	
	// highlight original entry
	if(entry) {
		if (revshow || absshow || bibshow) {
		entry.className = 'entry highlight show';
		} else {
		entry.className = 'entry show';
		}
	}
	
	// When there's a combination of abstract/comment/bibtex showing, need to add class for correct styling
	if(absshow) {
		(revshow||bibshow)?abs.className = 'abstract nextshow':abs.className = 'abstract';
	} 
	if (revshow) {
		bibshow?rev.className = 'comment nextshow': rev.className = 'comment';
	}	
	
}

function setStatistics (hits) {
	if(hits < 0) { hits=numEntries; }
	if(stats) { stats.firstChild.data = hits + '/' + numEntries}
}

function getTextContent(node) {
	// Function written by Arve Bersvendsen
	// http://www.virtuelvis.com
	
	if (node.nodeType == 3) {
	return node.nodeValue;
	} // text node
	if (node.nodeType == 1 && node.className != "infolinks") { // element node
	var text = [];
	for (var chld = node.firstChild;chld;chld=chld.nextSibling) {
		text.push(getTextContent(chld));
	}
	return text.join("");
	} return ""; // some other node, won't contain text nodes.
}

function showAll(){
	closeAllInfo();
	for (var i = 0; i < numEntries; i++){ entryRows[i].className = 'entry show'; }
}

function closeAllInfo(){
	for (var i=0; i < numInfo; i++){
		if (infoRows[i].className.indexOf('noshow') ==-1) {
			infoRows[i].className = infoRows[i].className + ' noshow';
		}
	}
}

function clearQS() {
	qsfield.value = '';
	showAll();
}

function redoQS(){
	showAll();
	quickSearch(qsfield);
}

function updateSetting(obj){
	var option = obj.id;
	var checked = obj.value;

	switch(option)
	 {
	 case "opt_searchAbs":
	   searchAbstract=!searchAbstract;
	   redoQS();
	   break;
	 case "opt_searchRev":
	   searchComment=!searchComment;
	   redoQS();
	   break;
	 case "opt_useRegExp":
	   searchRegExp=!searchRegExp;
	   redoQS();
	   break;
	 case "opt_noAccents":
	   noSquiggles=!noSquiggles;
	   loadTableData();
	   redoQS();
	   break;
	 }
}

function initPreferences(){
	if(searchAbstract){document.getElementById("opt_searchAbs").checked = true;}
	if(searchComment){document.getElementById("opt_searchRev").checked = true;}
	if(noSquiggles){document.getElementById("opt_noAccents").checked = true;}
	if(searchRegExp){document.getElementById("opt_useRegExp").checked = true;}
	
	if(numAbs==0) {document.getElementById("opt_searchAbs").parentNode.style.display = 'none';}
	if(numRev==0) {document.getElementById("opt_searchRev").parentNode.style.display = 'none';}
}

function toggleSettings(){
	var togglebutton = document.getElementById('showsettings');
	var settings = document.getElementById('settings');
	
	if(settings.className == "hidden"){
		settings.className = "show";
		togglebutton.innerText = "close settings";
		togglebutton.textContent = "close settings";
	}else{
		settings.className = "hidden";
		togglebutton.innerText = "settings...";		
		togglebutton.textContent = "settings...";
	}
}

-->
</script>
<style type="text/css">
body { background-color: white; font-family: Arial, sans-serif; font-size: 13px; line-height: 1.2; padding: 1em; color: #2E2E2E; margin: auto 2em; }

form#quicksearch { width: auto; border-style: solid; border-color: gray; border-width: 1px 0px; padding: 0.7em 0.5em; display:none; position:relative; }
span#searchstat {padding-left: 1em;}

div#settings { margin-top:0.7em; /* border-bottom: 1px transparent solid; background-color: #efefef; border: 1px grey solid; */ }
div#settings ul {margin: 0; padding: 0; }
div#settings li {margin: 0; padding: 0 1em 0 0; display: inline; list-style: none; }
div#settings li + li { border-left: 2px #efefef solid; padding-left: 0.5em;}
div#settings input { margin-bottom: 0px;}

div#settings.hidden {display:none;}

#showsettings { border: 1px grey solid; padding: 0 0.5em; float:right; line-height: 1.6em; text-align: right; }
#showsettings:hover { cursor: pointer; }

.invalidsearch { background-color: red; }
input[type="button"] { background-color: #efefef; border: 1px #2E2E2E solid;}

table { width: 100%; empty-cells: show; border-spacing: 0em 0.2em; margin: 1em 0em; border-style: none; }
th, td { border: 1px gray solid; border-width: 1px 1px; padding: 0.5em; vertical-align: top; text-align: left; }
th { background-color: #efefef; }
td + td, th + th { border-left: none; }

td a { color: navy; text-decoration: none; }
td a:hover  { text-decoration: underline; }

tr.noshow { display: none;}
tr.highlight td { background-color: #EFEFEF; border-top: 2px #2E2E2E solid; font-weight: bold; }
tr.abstract td, tr.comment td, tr.bibtex td { background-color: #EFEFEF; text-align: justify; border-bottom: 2px #2E2E2E solid; }
tr.nextshow td { border-bottom: 1px gray solid; }

tr.bibtex pre { width: 100%; overflow: auto; white-space: pre-wrap;}
p.infolinks { margin: 0.3em 0em 0em 0em; padding: 0px; }

@media print {
	p.infolinks, #qs_settings, #quicksearch, t.bibtex { display: none !important; }
	tr { page-break-inside: avoid; }
}
</style>
</head>
<body>

<form action="" id="quicksearch">
<input type="text" id="qs_field" autocomplete="off" placeholder="Type to search..." /> <input type="button" onclick="clearQS()" value="clear" />
<span id="searchstat">Matching entries: <span id="stat">0</span></span>
<div id="showsettings" onclick="toggleSettings()">settings...</div>
<div id="settings" class="hidden">
<ul>
<li><input type="checkbox" class="search_setting" id="opt_searchAbs" onchange="updateSetting(this)"><label for="opt_searchAbs"> include abstract</label></li>
<li><input type="checkbox" class="search_setting" id="opt_searchRev" onchange="updateSetting(this)"><label for="opt_searchRev"> include comment</label></li>
<li><input type="checkbox" class="search_setting" id="opt_useRegExp" onchange="updateSetting(this)"><label for="opt_useRegExp"> use RegExp</label></li>
<li><input type="checkbox" class="search_setting" id="opt_noAccents" onchange="updateSetting(this)"><label for="opt_noAccents"> ignore accents</label></li>
</ul>
</div>
</form>
<table id="qs_table" border="1">
<thead><tr><th width="20%">Author</th><th width="30%">Title</th><th width="5%">Year</th><th width="30%">Journal/Proceedings</th><th width="10%">Reftype</th><th width="5%">DOI/URL</th></tr></thead>
<tbody><tr id="Macenski2022May" class="entry">
	<td>Macenski, S., Foote, T., Gerkey, B., Lalancette, C. and Woodall, W.</td>
	<td>Robot Operating System 2: Design, architecture, and uses in the wild <p class="infolinks">[<a href="javascript:toggleInfo('Macenski2022May','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Macenski2022May','bibtex')">BibTeX</a>]</p></td>
	<td>2022</td>
	<td>Sci. Rob.<br/>Vol. 7(66), pp. eabm6074&nbsp;</td>
	<td>article</td>
	<td><a href="https://doi.org/10.1126/scirobotics.abm6074">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Macenski2022May" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: The next chapter of the robotics revolution is well underway with the deployment of robots for a broad range of commercial use cases. Even in a myriad of applications and environments, there exists a common vocabulary of components that robots share---else&mdash;&#64257;the need for a modular, scalable, and reliable architecture; sensing; planning; mobility; and autonomy. The Robot Operating System (ROS) was an integral part of the last chapter, demonstrably expediting robotics research with freely available components and a modular framework. However, ROS 1 was not designed with many necessary production-grade features and algorithms. ROS 2 and its related projects have been redesigned from the ground up to meet the challenges set forth by modern robotic systems in new and exploratory domains at all scales. In this Review, we highlight the philosophical and architectural changes of ROS 2 powering this new chapter in the robotics revolution. We also show through case studies the influence ROS 2 and its adoption has had on accelerating real robot systems to reliable deployment in an assortment of challenging environments.</td>
</tr>
<tr id="bib_Macenski2022May" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Macenski2022May,
  author = {Macenski, Steven and Foote, Tully and Gerkey, Brian and Lalancette, Chris and Woodall, William},
  title = {Robot Operating System 2: Design, architecture, and uses in the wild},
  journal = {Sci. Rob.},
  publisher = {American Association for the Advancement of Science},
  year = {2022},
  volume = {7},
  number = {66},
  pages = {eabm6074},
  url = {https://doi.org/10.1126/scirobotics.abm6074}
}
</pre></td>
</tr>
<tr id="Fayyad2020Jul" class="entry">
	<td>Fayyad, J., Jaradat, M.A., Gruyer, D. and Najjaran, H.</td>
	<td>Deep Learning Sensor Fusion for Autonomous Vehicle Perception and Localization: A Review <p class="infolinks">[<a href="javascript:toggleInfo('Fayyad2020Jul','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Fayyad2020Jul','bibtex')">BibTeX</a>]</p></td>
	<td>2020</td>
	<td>Sensors<br/>Vol. 20(15), pp. 4220&nbsp;</td>
	<td>article</td>
	<td><a href="https://www.mdpi.com/1424-8220/20/15/4220">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Fayyad2020Jul" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Autonomous vehicles (AV) are expected to improve, reshape, and revolutionize the future of ground transportation. It is anticipated that ordinary vehicles will one day be replaced with smart vehicles that are able to make decisions and perform driving tasks on their own. In order to achieve this objective, self-driving vehicles are equipped with sensors that are used to sense and perceive both their surroundings and the faraway environment, using further advances in communication technologies, such as 5G. In the meantime, local perception, as with human beings, will continue to be an effective means for controlling the vehicle at short range. In the other hand, extended perception allows for anticipation of distant events and produces smarter behavior to guide the vehicle to its destination while respecting a set of criteria (safety, energy management, traffic optimization, comfort). In spite of the remarkable advancements of sensor technologies in terms of their effectiveness and applicability for AV systems in recent years, sensors can still fail because of noise, ambient conditions, or manufacturing defects, among other factors; hence, it is not advisable to rely on a single sensor for any of the autonomous driving tasks. The practical solution is to incorporate multiple competitive and complementary sensors that work synergistically to overcome their individual shortcomings. This article provides a comprehensive review of the state-of-the-art methods utilized to improve the performance of AV systems in short-range or local vehicle environments. Specifically, it focuses on recent studies that use deep learning sensor fusion algorithms for perception, localization, and mapping. The article concludes by highlighting some of the current trends and possible future research directions. Keywords: autonomous vehicles; self-driving cars; deep learning; sensor fusion; perception; localization and mapping</td>
</tr>
<tr id="bib_Fayyad2020Jul" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Fayyad2020Jul,
  author = {Fayyad, Jamil and Jaradat, Mohammad A. and Gruyer, Dominique and Najjaran, Homayoun},
  title = {Deep Learning Sensor Fusion for Autonomous Vehicle Perception and Localization: A Review},
  journal = {Sensors},
  publisher = {Multidisciplinary Digital Publishing Institute},
  year = {2020},
  volume = {20},
  number = {15},
  pages = {4220},
  url = {https://www.mdpi.com/1424-8220/20/15/4220}
}
</pre></td>
</tr>
<tr id="Nobis2020May" class="entry">
	<td>Nobis, F., Geisslinger, M., Weber, M., Betz, J. and Lienkamp, M.</td>
	<td>A Deep Learning-based Radar and Camera Sensor Fusion Architecture for Object Detection <p class="infolinks">[<a href="javascript:toggleInfo('Nobis2020May','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Nobis2020May','bibtex')">BibTeX</a>]</p></td>
	<td>2020</td>
	<td>arXiv&nbsp;</td>
	<td>article</td>
	<td><a href="https://arxiv.org/abs/2005.07431v1">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Nobis2020May" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Object detection in camera images, using deep learning has been proven successfully in recent years. Rising detection rates and computationally efficient network structures are pushing this technique towards application in production vehicles. Nevertheless, the sensor quality of the camera is limited in severe weather conditions and through increased sensor noise in sparsely lit areas and at night. Our approach enhances current 2D object detection networks by fusing camera data and projected sparse radar data in the network layers. The proposed CameraRadarFusionNet (CRF-Net) automatically learns at which level the fusion of the sensor data is most beneficial for the detection result. Additionally, we introduce BlackIn, a training strategy inspired by Dropout, which focuses the learning on a specific sensor type. We show that the fusion network is able to outperform a state-of-the-art image-only network for two different datasets. The code for this research will be made available to the public at: this https URL.</td>
</tr>
<tr id="bib_Nobis2020May" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Nobis2020May,
  author = {Nobis, Felix and Geisslinger, Maximilian and Weber, Markus and Betz, Johannes and Lienkamp, Markus},
  title = {A Deep Learning-based Radar and Camera Sensor Fusion Architecture for Object Detection},
  journal = {arXiv},
  year = {2020},
  url = {https://arxiv.org/abs/2005.07431v1}
}
</pre></td>
</tr>
<tr id="10.1162/neco_a_01273" class="entry">
	<td>Gao, J., Li, P., Chen, Z. and Zhang, J.</td>
	<td>A Survey on Deep Learning for Multimodal Data Fusion <p class="infolinks">[<a href="javascript:toggleInfo('10.1162/neco_a_01273','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('10.1162/neco_a_01273','bibtex')">BibTeX</a>]</p></td>
	<td>2020</td>
	<td>Neural Computation<br/>Vol. 32(5), pp. 829-864&nbsp;</td>
	<td>article</td>
	<td><a href="https://direct.mit.edu/neco/article/32/5/829/95591/A-Survey-on-Deep-Learning-for-Multimodal-Data">URL</a>&nbsp;</td>
</tr>
<tr id="abs_10.1162/neco_a_01273" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: With the wide deployments of heterogeneous networks, huge amounts of data with characteristics of high volume, high variety, high velocity, and high veracity are generated. These data, referred to multimodal big data, contain abundant intermodality and cross-modality information and pose vast challenges on traditional data fusion methods. In this review, we present some pioneering deep learning models to fuse these multimodal big data. With the increasing exploration of the multimodal big data, there are still some challenges to be addressed. Thus, this review presents a survey on deep learning for multimodal data fusion to provide readers, regardless of their original community, with the fundamentals of multimodal deep learning fusion method and to motivate new multimodal data fusion techniques of deep learning. Specifically, representative architectures that are widely used are summarized as fundamental to the understanding of multimodal deep learning. Then the current pioneering multimodal data fusion deep learning models are summarized. Finally, some challenges and future topics of multimodal data fusion deep learning models are described.</td>
</tr>
<tr id="bib_10.1162/neco_a_01273" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{10.1162/neco_a_01273,
  author = {Gao, Jing and Li, Peng and Chen, Zhikui and Zhang, Jianing},
  title = {A Survey on Deep Learning for Multimodal Data Fusion},
  journal = {Neural Computation},
  year = {2020},
  volume = {32},
  number = {5},
  pages = {829-864},
  url = {https://direct.mit.edu/neco/article/32/5/829/95591/A-Survey-on-Deep-Learning-for-Multimodal-Data}
}
</pre></td>
</tr>
<tr id="Chung2019Apr" class="entry">
	<td>Chung, S., Lim, J., Noh, K.J., Kim, G. and Jeong, H.</td>
	<td>Sensor Data Acquisition and Multimodal Sensor Fusion for Human Activity Recognition Using Deep Learning <p class="infolinks">[<a href="javascript:toggleInfo('Chung2019Apr','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Chung2019Apr','bibtex')">BibTeX</a>]</p></td>
	<td>2019</td>
	<td>Sensors<br/>Vol. 19(7), pp. 1716&nbsp;</td>
	<td>article</td>
	<td><a href="https://www.mdpi.com/1424-8220/19/7/1716">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Chung2019Apr" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: In this paper, we perform a systematic study about the on-body sensor positioning and data acquisition details for Human Activity Recognition (HAR) systems. We build a testbed that consists of eight body-worn Inertial Measurement Units (IMU) sensors and an Android mobile device for activity data collection. We develop a Long Short-Term Memory (LSTM) network framework to support training of a deep learning model on human activity data, which is acquired in both real-world and controlled environments. From the experiment results, we identify that activity data with sampling rate as low as 10 Hz from four sensors at both sides of wrists, right ankle, and waist is sufficient in recognizing Activities of Daily Living (ADLs) including eating and driving activity. We adopt a two-level ensemble model to combine class-probabilities of multiple sensor modalities, and demonstrate that a classifier-level sensor fusion technique can improve the classification performance. By analyzing the accuracy of each sensor on different types of activity, we elaborate custom weights for multimodal sensor fusion that reflect the characteristic of individual activities. Keywords: mobile sensing; sensor position; human activity recognition; multimodal sensor fusion; classifier-level ensemble; Long Short-Term Memory network; deep learning</td>
</tr>
<tr id="bib_Chung2019Apr" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Chung2019Apr,
  author = {Chung, Seungeun and Lim, Jiyoun and Noh, Kyoung Ju and Kim, Gague and Jeong, Hyuntae},
  title = {Sensor Data Acquisition and Multimodal Sensor Fusion for Human Activity Recognition Using Deep Learning},
  journal = {Sensors},
  publisher = {Multidisciplinary Digital Publishing Institute},
  year = {2019},
  volume = {19},
  number = {7},
  pages = {1716},
  url = {https://www.mdpi.com/1424-8220/19/7/1716}
}
</pre></td>
</tr>
<tr id="Ge" class="entry">
	<td>Ge, C., Gu, I.Y.-H., Jakola, A.S. and Yang, J.</td>
	<td>Deep Learning and Multi-Sensor Fusion for Glioma Classification Using Multistream 2D Convolutional Networks <p class="infolinks">[<a href="javascript:toggleInfo('Ge','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Ge','bibtex')">BibTeX</a>]</p></td>
	<td>2018</td>
	<td>Published in: 2018 40th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)2018 40th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC), pp. 18-21&nbsp;</td>
	<td>incollection</td>
	<td><a href="https://ieeexplore.ieee.org/abstract/document/8513556?casa_token=Eyoe0IwA5YEAAAAA:eoRdQl19Zvhfj2dTAI_q3y2DUR1Gq95Q8GOxv5MJ8w_XlsEEI3tjl9wbTF4EKWPvoPVqY163AWw">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Ge" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: This paper addresses issues of brain tumor, glioma, grading from multi-sensor images. Different types of scanners (or sensors) like enhanced T1-MRI, T2-MRI and FLAIR, show different contrast and are sensitive to different brain tissues and fluid regions. Most existing works use 3D brain images from single sensor. In this paper, we propose a novel multistream deep Convolutional Neural Network (CNN) architecture that extracts and fuses the features from multiple sensors for glioma tumor grading/subcategory grading. The main contributions of the paper are: (a) propose a novel multistream deep CNN architecture for glioma grading; (b) apply sensor fusion from T1-MRI, T2-MRI and/or FLAIR for enhancing performance through feature aggregation; (c) mitigate overfitting by using 2D brain image slices in combination with 2D image augmentation. Two datasets were used for our experiments, one for classifying low/high grade gliomas, another for classifying glioma with/without 1p19q codeletion. Experiments using the proposed scheme have shown good results (with test accuracy of 90.87% for former case, and 89.39 % for the latter case). Comparisons with several existing methods have provided further support to the proposed scheme. keywords: brain tumor classification, glioma, 1p19q codeletion, glioma grading, deep learning, multi-stream convolutional neural networks, sensor fusion, T1-MR image, T2-MR image, FLAIR.</td>
</tr>
<tr id="bib_Ge" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@incollection{Ge,
  author = {Ge, Chenjie and Gu, Irene Yu-Hua and Jakola, Asgeir Store and Yang, Jie},
  title = {Deep Learning and Multi-Sensor Fusion for Glioma Classification Using Multistream 2D Convolutional Networks},
  booktitle = {2018 40th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)},
  journal = {Published in: 2018 40th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)},
  publisher = {IEEE},
  year = {2018},
  pages = {18--21},
  url = {https://ieeexplore.ieee.org/abstract/document/8513556?casa_token=Eyoe0IwA5YEAAAAA:eoRdQl19Zvhfj2dTAI_q3y2DUR1Gq95Q8GOxv5MJ8w_XlsEEI3tjl9wbTF4EKWPvoPVqY163AWw}
}
</pre></td>
</tr>
<tr id="Vielzeuf2018Oct" class="entry">
	<td>Vielzeuf, V., Lechervy, A., Pateux, S. and Jurie, F.</td>
	<td>Multilevel Sensor Fusion With Deep Learning <p class="infolinks">[<a href="javascript:toggleInfo('Vielzeuf2018Oct','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Vielzeuf2018Oct','bibtex')">BibTeX</a>]</p></td>
	<td>2018</td>
	<td>IEEE Sens. Lett.<br/>Vol. 3(1), pp. ArticleSequenceNumber:7100304&nbsp;</td>
	<td>article</td>
	<td><a href="https://ieeexplore.ieee.org/abstract/document/8516399?casa_token=XrtIQITSUOwAAAAA:CU4QWaWg-ERJkzwLaJBL7v110QCZ0oFotPxeBr4Qxfyl03poLraDU7_ALcLFG8u8--mCHD9nNfE">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Vielzeuf2018Oct" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: In the context of deep learning, this article presents an original deep network, namely CentralNet, for the fusion of information coming from different sensors. This approach is designed to efficiently and automatically balance the tradeoff between early and late fusion (i.e., between the fusion of low-level versus high-level information). More specifically, at each level of abstraction---else&mdash;&#64257;the different levels of deep networks---else&mdash;&#64257;unimodal representations of the data are fed to a central neural network which combines them into a common embedding. In addition, a multiobjective regularization is also introduced, helping to both optimize the central network and the unimodal networks. Experiments on four multimodal datasets not only show the state-of-the-art performance but also demonstrate that CentralNet can actually choose the best possible fusion strategy for a given problem.</td>
</tr>
<tr id="bib_Vielzeuf2018Oct" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Vielzeuf2018Oct,
  author = {Vielzeuf, Valentin and Lechervy, Alexis and Pateux, Stifmmodeeelse&eacute;&#64257;phane and Jurie, Frifmmodeeelse&eacute;&#64257;difmmodeeelse&eacute;&#64257;ric},
  title = {Multilevel Sensor Fusion With Deep Learning},
  journal = {IEEE Sens. Lett.},
  publisher = {IEEE},
  year = {2018},
  volume = {3},
  number = {1},
  pages = {ArticleSequenceNumber:7100304},
  url = {https://ieeexplore.ieee.org/abstract/document/8516399?casa_token=XrtIQITSUOwAAAAA:CU4QWaWg-ERJkzwLaJBL7v110QCZ0oFotPxeBr4Qxfyl03poLraDU7_ALcLFG8u8--mCHD9nNfE}
}
</pre></td>
</tr>
<tr id="Huang2021Aug" class="entry">
	<td>Huang, P.-M. and Lee, C.-H.</td>
	<td>Estimation of Tool Wear and Surface Roughness Development Using Deep Learning and Sensors Fusion <p class="infolinks">[<a href="javascript:toggleInfo('Huang2021Aug','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Huang2021Aug','bibtex')">BibTeX</a>]</p></td>
	<td>2021</td>
	<td>Sensors<br/>Vol. 21(16), pp. 5338&nbsp;</td>
	<td>article</td>
	<td><a href="https://www.mdpi.com/1424-8220/21/16/5338">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Huang2021Aug" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: This paper proposes an estimation approach for tool wear and surface roughness using deep learning and sensor fusion. The one-dimensional convolutional neural network (1D-CNN) is utilized as the estimation model with X- and Y-coordinate vibration signals and sound signal fusion using sensor influence analysis. First, machining experiments with computer numerical control (CNC) parameters are designed using a uniform experimental design (UED) method to guarantee the variety of collected data. The vibration, sound, and spindle current signals are collected and labeled according to the machining parameters. To speed up the degree of tool wear, an accelerated experiment is designed, and the corresponding tool wear and surface roughness are measured. An influential sensor selection analysis is proposed to preserve the estimation accuracy and to minimize the number of sensors. After sensor selection analysis, the sensor signals with better estimation capability are selected and combined using the sensor fusion method. The proposed estimation system combined with sensor selection analysis performs well in terms of accuracy and computational effort. Finally, the proposed approach is applied for on-line monitoring of tool wear with an alarm, which demonstrates the effectiveness of our approach. Keywords: deep learning; vibration; sound; fusion; tool wear; surface roughness; convolution neural network</td>
</tr>
<tr id="bib_Huang2021Aug" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Huang2021Aug,
  author = {Huang, Pao-Ming and Lee, Ching-Hung},
  title = {Estimation of Tool Wear and Surface Roughness Development Using Deep Learning and Sensors Fusion},
  journal = {Sensors},
  publisher = {Multidisciplinary Digital Publishing Institute},
  year = {2021},
  volume = {21},
  number = {16},
  pages = {5338},
  url = {https://www.mdpi.com/1424-8220/21/16/5338}
}
</pre></td>
</tr>
<tr id="Wang2018Sep" class="entry">
	<td>Wang, W., Chen, B., Xia, P., Hu, J. and Peng, Y.</td>
	<td>Sensor Fusion for Myoelectric Control Based on Deep Learning With Recurrent Convolutional Neural Networks <p class="infolinks">[<a href="javascript:toggleInfo('Wang2018Sep','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Wang2018Sep','bibtex')">BibTeX</a>]</p></td>
	<td>2018</td>
	<td>Artif. Organs<br/>Vol. 42(9), pp. E272-E282&nbsp;</td>
	<td>article</td>
	<td><a href="https://doi.org/10.1111/aor.13153">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Wang2018Sep" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Electromyogram (EMG) signal decoding is the essential part of myoelectric control. However, traditional machine learning methods lack the capability of learning and expressing the information contained in EMG signals, and the robustness of the myoelectric control system is not sufficient for real life applications. In this article, a novel model based on recurrent convolutional neural networks (RCNNs) is proposed for hand movement classification and tested on the noninvasive EMG dataset. The proposed model uses deep architecture, which has advantages of dealing with complex time-series data, such as EMG signals. Transfer learning is used in the training of multimodal model. The classification performance is compared with support vector machine (SVM) and convolutional neural networks (CNNs) on the same dataset. To improve the adaptability to the effect of arm movements, we fused the EMG signals and acceleration data that are the multimodal input of the model. The parameter transferring of deep neural networks is used to accelerate the training process and avoid over-fitting. The experimental results show that time domain input and 1-dimensional convolution have higher accuracy in the RCNN model. Compared with SVM and CNNs, the proposed model has higher classification accuracy. Sensor fusion can improve the model performance in the condition of arm movements. The RCNN model is a promising decoder of EMG and the sensor fusion can increase the accuracy and robustness of the myoelectric control system.</td>
</tr>
<tr id="bib_Wang2018Sep" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Wang2018Sep,
  author = {Wang, Weiming and Chen, Biao and Xia, Peng and Hu, Jie and Peng, Yinghong},
  title = {Sensor Fusion for Myoelectric Control Based on Deep Learning With Recurrent Convolutional Neural Networks},
  journal = {Artif. Organs},
  publisher = {John Wiley &amp; Sons, Ltd},
  year = {2018},
  volume = {42},
  number = {9},
  pages = {E272--E282},
  url = {https://doi.org/10.1111/aor.13153}
}
</pre></td>
</tr>
<tr id="Xu2017Nov" class="entry">
	<td>Xu, D., Anguelov, D. and Jain, A.</td>
	<td>PointFusion: Deep Sensor Fusion for 3D Bounding Box Estimation <p class="infolinks">[<a href="javascript:toggleInfo('Xu2017Nov','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Xu2017Nov','bibtex')">BibTeX</a>]</p></td>
	<td>2017</td>
	<td>arXiv&nbsp;</td>
	<td>article</td>
	<td><a href="https://arxiv.org/abs/1711.10871v2">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Xu2017Nov" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: We present PointFusion, a generic 3D object detection method that leverages both image and 3D point cloud information. Unlike existing methods that either use multi-stage pipelines or hold sensor and dataset-specific assumptions, PointFusion is conceptually simple and application-agnostic. The image data and the raw point cloud data are independently processed by a CNN and a PointNet architecture, respectively. The resulting outputs are then combined by a novel fusion network, which predicts multiple 3D box hypotheses and their confidences, using the input 3D points as spatial anchors. We evaluate PointFusion on two distinctive datasets: the KITTI dataset that features driving scenes captured with a lidar-camera setup, and the SUN-RGBD dataset that captures indoor environments with RGB-D cameras. Our model is the first one that is able to perform better or on-par with the state-of-the-art on these diverse datasets without any dataset-specific model tuning.</td>
</tr>
<tr id="bib_Xu2017Nov" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Xu2017Nov,
  author = {Xu, Danfei and Anguelov, Dragomir and Jain, Ashesh},
  title = {PointFusion: Deep Sensor Fusion for 3D Bounding Box Estimation},
  journal = {arXiv},
  year = {2017},
  url = {https://arxiv.org/abs/1711.10871v2}
}
</pre></td>
</tr>
<tr id="Liang2020Dec" class="entry">
	<td>Liang, M., Yang, B., Chen, Y., Hu, R. and Urtasun, R.</td>
	<td>Multi-Task Multi-Sensor Fusion for 3D Object Detection <p class="infolinks">[<a href="javascript:toggleInfo('Liang2020Dec','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Liang2020Dec','bibtex')">BibTeX</a>]</p></td>
	<td>2020</td>
	<td>arXiv&nbsp;</td>
	<td>article</td>
	<td><a href="https://arxiv.org/abs/2012.12397v1">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Liang2020Dec" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: In this paper we propose to exploit multiple related tasks for accurate multi-sensor 3D object detection. Towards this goal we present an end-to-end learnable architecture that reasons about 2D and 3D object detection as well as ground estimation and depth completion. Our experiments show that all these tasks are complementary and help the network learn better representations by fusing information at various levels. Importantly, our approach leads the KITTI benchmark on 2D, 3D and BEV object detection, while being real time.</td>
</tr>
<tr id="bib_Liang2020Dec" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Liang2020Dec,
  author = {Liang, Ming and Yang, Bin and Chen, Yun and Hu, Rui and Urtasun, Raquel},
  title = {Multi-Task Multi-Sensor Fusion for 3D Object Detection},
  journal = {arXiv},
  year = {2020},
  url = {https://arxiv.org/abs/2012.12397v1}
}
</pre></td>
</tr>
<tr id="Yeong2021" class="entry">
	<td>Yeong, D.J., Velasco-Hernandez, G., Barry, J. and Walsh, J.</td>
	<td>Sensor and Sensor Fusion Technology in Autonomous Vehicles: A Review <p class="infolinks">[<a href="javascript:toggleInfo('Yeong2021','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Yeong2021','bibtex')">BibTeX</a>]</p></td>
	<td>2021</td>
	<td>Sensors<br/>Vol. 21(6), pp. 2140&nbsp;</td>
	<td>article</td>
	<td><a href="https://www.mdpi.com/1424-8220/21/6/2140">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Yeong2021" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: With the significant advancement of sensor and communication technology and the reliable application of obstacle detection techniques and algorithms, automated driving is becoming a pivotal technology that can revolutionize the future of transportation and mobility. Sensors are fundamental to the perception of vehicle surroundings in an automated driving system, and the use and performance of multiple integrated sensors can directly determine the safety and feasibility of automated driving vehicles. Sensor calibration is the foundation block of any autonomous system and its constituent sensors and must be performed correctly before sensor fusion and obstacle detection processes may be implemented. This paper evaluates the capabilities and the technical performance of sensors which are commonly employed in autonomous vehicles, primarily focusing on a large selection of vision cameras, LiDAR sensors, and radar sensors and the various conditions in which such sensors may operate in practice. We present an overview of the three primary categories of sensor calibration and review existing open-source calibration packages for multi-sensor calibration and their compatibility with numerous commercial sensors. We also summarize the three main approaches to sensor fusion and review current state-of-the-art multi-sensor fusion techniques and algorithms for object detection in autonomous driving applications. The current paper, therefore, provides an end-to-end review of the hardware and software methods required for sensor fusion object detection. We conclude by highlighting some of the challenges in the sensor fusion field and propose possible future research directions for automated driving systems. Keywords: autonomous vehicles; self-driving cars; perception; camera; lidar; radar; sensor fusion; calibration; obstacle detection</td>
</tr>
<tr id="bib_Yeong2021" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Yeong2021,
  author = {Yeong, De Jong and Velasco-Hernandez, Gustavo and Barry, John and Walsh, Joseph},
  title = {Sensor and Sensor Fusion Technology in Autonomous Vehicles: A Review},
  journal = {Sensors},
  publisher = {Multidisciplinary Digital Publishing Institute},
  year = {2021},
  volume = {21},
  number = {6},
  pages = {2140},
  url = {https://www.mdpi.com/1424-8220/21/6/2140}
}
</pre></td>
</tr>
<tr id="Wang2019Dec" class="entry">
	<td>Wang, Z., Wu, Y. and Niu, Q.</td>
	<td>Multi-Sensor Fusion in Automated Driving: A Survey <p class="infolinks">[<a href="javascript:toggleInfo('Wang2019Dec','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Wang2019Dec','bibtex')">BibTeX</a>]</p></td>
	<td>2019</td>
	<td>IEEE Access<br/>Vol. 8, pp. 2847-2868&nbsp;</td>
	<td>article</td>
	<td><a href="https://ieeexplore.ieee.org/abstract/document/8943388/keywords#keywords">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Wang2019Dec" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: With the significant development of practicability in deep learning and the ultra-high-speed information transmission rate of 5G communication technology will overcome the barrier of data transmission on the Internet of Vehicles, automated driving is becoming a pivotal technology affecting the future industry. Sensors are the key to the perception of the outside world in the automated driving system and whose cooperation performance directly determines the safety of automated driving vehicles. In this survey, we mainly discuss the different strategies of multi-sensor fusion in automated driving in recent years. The performance of conventional sensors and the necessity of multi-sensor fusion are analyzed, including radar, LiDAR, camera, ultrasonic, GPS, IMU, and V2X. According to the differences in the latest studies, we divide the fusion strategies into four categories and point out some shortcomings. Sensor fusion is mainly applied for multi-target tracking and environment reconstruction. We discuss the method of establishing a motion model and data association in multi-target tracking. At the end of the paper, we analyzed the deficiencies in the current studies and put forward some suggestions for further improvement in the future. Through this investigation, we hope to analyze the current situation of multi-sensor fusion in the automated driving process and provide more efficient and reliable fusion strategies.</td>
</tr>
<tr id="bib_Wang2019Dec" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Wang2019Dec,
  author = {Wang, Zhangjing and Wu, Yu and Niu, Qingqing},
  title = {Multi-Sensor Fusion in Automated Driving: A Survey},
  journal = {IEEE Access},
  publisher = {IEEE},
  year = {2019},
  volume = {8},
  pages = {2847--2868},
  url = {https://ieeexplore.ieee.org/abstract/document/8943388/keywords#keywords}
}
</pre></td>
</tr>
<tr id="Liang2020Dec" class="entry">
	<td>Liang, M., Yang, B., Wang, S. and Urtasun, R.</td>
	<td>Deep Continuous Fusion for Multi-Sensor 3D Object Detection <p class="infolinks">[<a href="javascript:toggleInfo('Liang2020Dec','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Liang2020Dec','bibtex')">BibTeX</a>]</p></td>
	<td>2020</td>
	<td>arXiv&nbsp;</td>
	<td>article</td>
	<td><a href="https://arxiv.org/abs/2012.10992v1">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Liang2020Dec" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: In this paper, we propose a novel 3D object detector that can exploit both LIDAR as well as cameras to perform very accurate localization. Towards this goal, we design an end-to-end learnable architecture that exploits continuous convolutions to fuse image and LIDAR feature maps at different levels of resolution. Our proposed continuous fusion layer encode both discrete-state image features as well as continuous geometric information. This enables us to design a novel, reliable and efficient end-to-end learnable 3D object detector based on multiple sensors. Our experimental evaluation on both KITTI as well as a large scale 3D object detection benchmark shows significant improvements over the state of the art.</td>
</tr>
<tr id="bib_Liang2020Dec" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Liang2020Dec,
  author = {Liang, Ming and Yang, Bin and Wang, Shenlong and Urtasun, Raquel},
  title = {Deep Continuous Fusion for Multi-Sensor 3D Object Detection},
  journal = {arXiv},
  year = {2020},
  url = {https://arxiv.org/abs/2012.10992v1}
}
</pre></td>
</tr>
<tr id="Bijelic2019Feb" class="entry">
	<td>Bijelic, M., Gruber, T., Mannan, F., Kraus, F., Ritter, W., Dietmayer, K. and Heide, F.</td>
	<td>Seeing Through Fog Without Seeing Fog: Deep Multimodal Sensor Fusion in Unseen Adverse Weather <p class="infolinks">[<a href="javascript:toggleInfo('Bijelic2019Feb','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Bijelic2019Feb','bibtex')">BibTeX</a>]</p></td>
	<td>2019</td>
	<td>arXiv&nbsp;</td>
	<td>article</td>
	<td><a href="https://arxiv.org/abs/1902.08913v3">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Bijelic2019Feb" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: The fusion of multimodal sensor streams, such as camera, lidar, and radar measurements, plays a critical role in object detection for autonomous vehicles, which base their decision making on these inputs. While existing methods exploit redundant information in good environmental conditions, they fail in adverse weather where the sensory streams can be asymmetrically distorted. These rare "edge-case" scenarios are not represented in available datasets, and existing fusion architectures are not designed to handle them. To address this challenge we present a novel multimodal dataset acquired in over 10,000km of driving in northern Europe. Although this dataset is the first large multimodal dataset in adverse weather, with 100k labels for lidar, camera, radar, and gated NIR sensors, it does not facilitate training as extreme weather is rare. To this end, we present a deep fusion network for robust fusion without a large corpus of labeled training data covering all asymmetric distortions. Departing from proposal-level fusion, we propose a single-shot model that adaptively fuses features, driven by measurement entropy. We validate the proposed method, trained on clean data, on our extensive validation dataset. Code and data are available here this https URL.</td>
</tr>
<tr id="bib_Bijelic2019Feb" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Bijelic2019Feb,
  author = {Bijelic, Mario and Gruber, Tobias and Mannan, Fahim and Kraus, Florian and Ritter, Werner and Dietmayer, Klaus and Heide, Felix},
  title = {Seeing Through Fog Without Seeing Fog: Deep Multimodal Sensor Fusion in Unseen Adverse Weather},
  journal = {arXiv},
  year = {2019},
  url = {https://arxiv.org/abs/1902.08913v3}
}
</pre></td>
</tr>
<tr id="Deng2019Oct" class="entry">
	<td>Deng, Z., Zhu, L., Hu, X., Fu, C.-W., Xu, X., Zhang, Q., Qin, J. and Heng, P.-A.</td>
	<td>Deep Multi-Model Fusion for Single-Image Dehazing <p class="infolinks">[<a href="javascript:toggleInfo('Deng2019Oct','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Deng2019Oct','bibtex')">BibTeX</a>]</p></td>
	<td>2019</td>
	<td>2019 IEEE/CVF International Conference on Computer Vision (ICCV)2019 IEEE/CVF International Conference on Computer Vision (ICCV), pp. 2453-2462&nbsp;</td>
	<td>incollection</td>
	<td><a href="https://ieeexplore.ieee.org/document/9009514">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Deng2019Oct" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: This paper presents a deep multi-model fusion network to attentively integrate multiple models to separate layers and boost the performance in single-image dehazing. To do so, we first formulate the attentional feature integration module to maximize the integration of the convolutional neural network (CNN) features at different CNN layers and generate the attentional multi-level integrated features (AMLIF). Then, from the AMLIF, we further predict a haze-free result for an atmospheric scattering model, as well as for four haze-layer separation models, and then fuse the results together to produce the final haze-free image. To evaluate the effectiveness of our method, we compare our network with several state-of-the-art methods on two widely-used dehazing benchmark datasets, as well as on two sets of real-world hazy images. Experimental results demonstrate clear quantitative and qualitative improvements of our method over the state-of-the-arts.</td>
</tr>
<tr id="bib_Deng2019Oct" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@incollection{Deng2019Oct,
  author = {Deng, Zijun and Zhu, Lei and Hu, Xiaowei and Fu, Chi-Wing and Xu, Xuemiao and Zhang, Qing and Qin, Jing and Heng, Pheng-Ann},
  title = {Deep Multi-Model Fusion for Single-Image Dehazing},
  booktitle = {2019 IEEE/CVF International Conference on Computer Vision (ICCV)},
  journal = {2019 IEEE/CVF International Conference on Computer Vision (ICCV)},
  publisher = {IEEE},
  year = {2019},
  pages = {2453--2462},
  url = {https://ieeexplore.ieee.org/document/9009514}
}
</pre></td>
</tr>
<tr id="Qian2021" class="entry">
	<td>Qian, K., Zhu, S., Zhang, X. and Li, L.E.</td>
	<td>Robust Multimodal Vehicle Detection in Foggy Weather Using Complementary Lidar and Radar Signals <p class="infolinks">[<a href="javascript:toggleInfo('Qian2021','bibtex')">BibTeX</a>]</p></td>
	<td>2021</td>
	<td>, pp. 444-453&nbsp;</td>
	<td>misc</td>
	<td><a href="https://openaccess.thecvf.com/content/CVPR2021/html/Qian_Robust_Multimodal_Vehicle_Detection_in_Foggy_Weather_Using_Complementary_Lidar_CVPR_2021_paper.html">URL</a>&nbsp;</td>
</tr>
<tr id="bib_Qian2021" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@misc{Qian2021,
  author = {Qian, Kun and Zhu, Shilin and Zhang, Xinyu and Li, Li Erran},
  title = {Robust Multimodal Vehicle Detection in Foggy Weather Using Complementary Lidar and Radar Signals},
  year = {2021},
  pages = {444--453},
  note = {[Online; accessed 28. Nov. 2022]},
  url = {https://openaccess.thecvf.com/content/CVPR2021/html/Qian_Robust_Multimodal_Vehicle_Detection_in_Foggy_Weather_Using_Complementary_Lidar_CVPR_2021_paper.html}
}
</pre></td>
</tr>
<tr id="xn--Muat-rxb2021Oct" class="entry">
	<td>xn-Muat-rxb, V., Fursa, I., Newman, P., Cuzzolin, F. and Bradley, A.</td>
	<td>Multi-weather city: Adverse weather stacking for autonomous driving <p class="infolinks">[<a href="javascript:toggleInfo('xn--Muat-rxb2021Oct','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('xn--Muat-rxb2021Oct','bibtex')">BibTeX</a>]</p></td>
	<td>2021</td>
	<td>2021 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW)2021 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW), pp. 2906-2915&nbsp;</td>
	<td>incollection</td>
	<td><a href="https://ieeexplore.ieee.org/document/9607439">URL</a>&nbsp;</td>
</tr>
<tr id="abs_xn--Muat-rxb2021Oct" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Autonomous vehicles make use of sensors to perceive the world around them, with heavy reliance on vision-based sensors such as RGB cameras. Unfortunately, since these sensors are affected by adverse weather, perception pipelines require extensive training on visual data under harsh conditions in order to improve the robustness of downstream tasks - data that is difficult and expensive to acquire. Based on GAN and CycleGAN architectures, we propose an overall (modular) architecture for constructing datasets, which allows one to add, swap out and combine components in order to generate images with diverse weather conditions. Starting from a single dataset with ground-truth, we generate 7 versions of the same data in diverse weather, and propose an extension to augment the generated conditions, thus resulting in a total of 14 adverse weather conditions, requiring a single ground truth. We test the quality of the generated conditions both in terms of perceptual quality and suitability for training downstream tasks, using real world, out-of-distribution adverse weather extracted from various datasets. We show improvements in both object detection and instance segmentation across all conditions, in many cases exceeding 10 percentage points increase in AP, and provide the materials and instructions needed to re-construct the multi-weather dataset, based upon the original Cityscapes dataset.</td>
</tr>
<tr id="bib_xn--Muat-rxb2021Oct" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@incollection{xn--Muat-rxb2021Oct,
  author = {xn--Muat-rxb, Valentina and Fursa, Ivan and Newman, Paul and Cuzzolin, Fabio and Bradley, Andrew},
  title = {Multi-weather city: Adverse weather stacking for autonomous driving},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW)},
  journal = {2021 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW)},
  publisher = {IEEE},
  year = {2021},
  pages = {2906--2915},
  url = {https://ieeexplore.ieee.org/document/9607439}
}
</pre></td>
</tr>
<tr id="Lai2022Sep" class="entry">
	<td>Lai, H., Yin, P. and Scherer, S.</td>
	<td>AdaFusion: Visual-LiDAR Fusion With Adaptive Weights for Place Recognition <p class="infolinks">[<a href="javascript:toggleInfo('Lai2022Sep','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Lai2022Sep','bibtex')">BibTeX</a>]</p></td>
	<td>2022</td>
	<td>IEEE Rob. Autom. Lett.<br/>Vol. 7(4), pp. 12038-12045&nbsp;</td>
	<td>article</td>
	<td><a href="https://ieeexplore.ieee.org/abstract/document/9905898?casa_token=f-aZkuzBdY0AAAAA:B5CQVbH-nh5Qd6KsEEMXPWiG5VoQBCa880OdHPxIwpbMumQSPThhMhsvn9WdTQO7tnYicfqsq3Q">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Lai2022Sep" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Recent years have witnessed the increasing application of place recognition in various environments, such as city roads, large buildings, and a mix of indoor and outdoor places. This task, however, still remains challenging due to the limitations of different sensors and the changing appearance of environments. Current works only consider the use of individual sensors or simply combine different sensors, ignoring the fact that the importance of different sensors varies as the environment changes. In this letter, an adaptive weighting visual-LiDAR fusion method, named AdaFusion, is proposed to learn the weights for both image and point cloud features. Features of these two modalities are thus contributed differently according to the current environmental situation. Weights are learned by the multi-scale attention branch of the network, which is then fused with the multi-modality feature extraction branch. Furthermore, to better utilize the potential relationship between images and point clouds, we design a two-stage fusion approach to combine the 2D and 3D attention. Our work is tested on two public datasets. Experiments show that the adaptive weights help improve recognition accuracy and system robustness to varying environments while being efficient in runtime.</td>
</tr>
<tr id="bib_Lai2022Sep" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Lai2022Sep,
  author = {Lai, Haowen and Yin, Peng and Scherer, Sebastian},
  title = {AdaFusion: Visual-LiDAR Fusion With Adaptive Weights for Place Recognition},
  journal = {IEEE Rob. Autom. Lett.},
  publisher = {IEEE},
  year = {2022},
  volume = {7},
  number = {4},
  pages = {12038--12045},
  url = {https://ieeexplore.ieee.org/abstract/document/9905898?casa_token=f-aZkuzBdY0AAAAA:B5CQVbH-nh5Qd6KsEEMXPWiG5VoQBCa880OdHPxIwpbMumQSPThhMhsvn9WdTQO7tnYicfqsq3Q}
}
</pre></td>
</tr>
<tr id="Barnes" class="entry">
	<td>Barnes, D., Gadd, M., Murcutt, P., Newman, P. and Posner, I.</td>
	<td>The Oxford Radar RobotCar Dataset: A Radar Extension to the Oxford RobotCar Dataset <p class="infolinks">[<a href="javascript:toggleInfo('Barnes','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Barnes','bibtex')">BibTeX</a>]</p></td>
	<td>2020</td>
	<td>Published in: 2020 IEEE International Conference on Robotics and Automation (ICRA)2020 IEEE International Conference on Robotics and Automation (ICRA), pp. 2020-31&nbsp;</td>
	<td>incollection</td>
	<td><a href="https://ieeexplore.ieee.org/abstract/document/9196884?casa_token=2BTLePzGRJUAAAAA:-8e1s7lpxTQvbOC02GFlYATkuaRuuV4c_6f9lLSVov180BuKv_sHcU5rhZ5qXwLt6-GoZwC-0uQ">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Barnes" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: In this paper we present The Oxford Radar RobotCar Dataset, a new dataset for researching scene understanding using Millimetre-Wave FMCW scanning radar data. The target application is autonomous vehicles where this modality is robust to environmental conditions such as fog, rain, snow, or lens flare, which typically challenge other sensor modalities such as vision and LIDAR.(/P)(P)The data were gathered in January 2019 over thirty-two traversals of a central Oxford route spanning a total of 280 km of urban driving. It encompasses a variety of weather, traffic, and lighting conditions. This 4.7 TB dataset consists of over 240,000 scans from a Navtech CTS350-X radar and 2.4 million scans from two Velodyne HDL-32E 3D LIDARs; along with six cameras, two 2D LIDARs, and a GPS/INS receiver. In addition we release ground truth optimised radar odometry to provide an additional impetus to research in this domain. The full dataset is available for download at: ori.ox.ac.uk/datasets/radar-robotear-dataset.</td>
</tr>
<tr id="bib_Barnes" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@incollection{Barnes,
  author = {Barnes, Dan and Gadd, Matthew and Murcutt, Paul and Newman, Paul and Posner, Ingmar},
  title = {The Oxford Radar RobotCar Dataset: A Radar Extension to the Oxford RobotCar Dataset},
  booktitle = {2020 IEEE International Conference on Robotics and Automation (ICRA)},
  journal = {Published in: 2020 IEEE International Conference on Robotics and Automation (ICRA)},
  publisher = {IEEE},
  year = {2020},
  pages = {2020--31},
  url = {https://ieeexplore.ieee.org/abstract/document/9196884?casa_token=2BTLePzGRJUAAAAA:-8e1s7lpxTQvbOC02GFlYATkuaRuuV4c_6f9lLSVov180BuKv_sHcU5rhZ5qXwLt6-GoZwC-0uQ}
}
</pre></td>
</tr>
<tr id="Maddern2016Nov" class="entry">
	<td>Maddern, W., Pascoe, G., Linegar, C. and Newman, P.</td>
	<td>1 year, 1000 km: The Oxford RobotCar dataset <p class="infolinks">[<a href="javascript:toggleInfo('Maddern2016Nov','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Maddern2016Nov','bibtex')">BibTeX</a>]</p></td>
	<td>2016</td>
	<td>Int. J. Rob. Res.<br/>Vol. 36(1), pp. 3-15&nbsp;</td>
	<td>article</td>
	<td><a href="https://doi.org/10.1177/0278364916679498">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Maddern2016Nov" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: We present a challenging new dataset for autonomous driving: the Oxford RobotCar Dataset. Over the period of May 2014 to December 2015 we traversed a route through central Oxford twice a week on av...</td>
</tr>
<tr id="bib_Maddern2016Nov" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Maddern2016Nov,
  author = {Maddern, Will and Pascoe, Geoffrey and Linegar, Chris and Newman, Paul},
  title = {1 year, 1000 km: The Oxford RobotCar dataset},
  journal = {Int. J. Rob. Res.},
  publisher = {SAGE Publications Ltd STM},
  year = {2016},
  volume = {36},
  number = {1},
  pages = {3--15},
  url = {https://doi.org/10.1177/0278364916679498}
}
</pre></td>
</tr>
<tr id="Carlevaris-Bianco2015Dec" class="entry">
	<td>Carlevaris-Bianco, N., Ushani, A.K. and Eustice, R.M.</td>
	<td>University of Michigan North Campus long-term vision and lidar dataset <p class="infolinks">[<a href="javascript:toggleInfo('Carlevaris-Bianco2015Dec','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Carlevaris-Bianco2015Dec','bibtex')">BibTeX</a>]</p></td>
	<td>2015</td>
	<td>Int. J. Rob. Res.<br/>Vol. 35(9), pp. 1023-1035&nbsp;</td>
	<td>article</td>
	<td><a href="https://doi.org/10.1177/0278364915614638">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Carlevaris-Bianco2015Dec" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: This paper documents a large scale, long-term autonomy dataset for robotics research collected on the University of Michigan's North Campus. The dataset consists of omnidirectional imagery, 3D lida...</td>
</tr>
<tr id="bib_Carlevaris-Bianco2015Dec" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Carlevaris-Bianco2015Dec,
  author = {Carlevaris-Bianco, Nicholas and Ushani, Arash K. and Eustice, Ryan M.},
  title = {University of Michigan North Campus long-term vision and lidar dataset},
  journal = {Int. J. Rob. Res.},
  publisher = {SAGE Publications Ltd STM},
  year = {2015},
  volume = {35},
  number = {9},
  pages = {1023--1035},
  url = {https://doi.org/10.1177/0278364915614638}
}
</pre></td>
</tr>
<tr id="Huang2021Nov" class="entry">
	<td>Huang, X., Qu, W., Zuo, Y., Fang, Y. and Zhao, X.</td>
	<td>IMFNet: Interpretable Multimodal Fusion for Point Cloud Registration <p class="infolinks">[<a href="javascript:toggleInfo('Huang2021Nov','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Huang2021Nov','bibtex')">BibTeX</a>]</p></td>
	<td>2021</td>
	<td>arXiv&nbsp;</td>
	<td>article</td>
	<td><a href="https://arxiv.org/abs/2111.09624v1">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Huang2021Nov" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: The existing state-of-the-art point descriptor relies on structure information only, which omit the texture information. However, texture information is crucial for our humans to distinguish a scene part. Moreover, the current learning-based point descriptors are all black boxes which are unclear how the original points contribute to the final descriptor. In this paper, we propose a new multimodal fusion method to generate a point cloud registration descriptor by considering both structure and texture information. Specifically, a novel attention-fusion module is designed to extract the weighted texture information for the descriptor extraction. In addition, we propose an interpretable module to explain the original points in contributing to the final descriptor. We use the descriptor element as the loss to backpropagate to the target layer and consider the gradient as the significance of this point to the final descriptor. This paper moves one step further to explainable deep learning in the registration task. Comprehensive experiments on 3DMatch, 3DLoMatch and KITTI demonstrate that the multimodal fusion descriptor achieves state-of-the-art accuracy and improve the descriptor's distinctiveness. We also demonstrate that our interpretable module in explaining the registration descriptor extraction.</td>
</tr>
<tr id="bib_Huang2021Nov" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Huang2021Nov,
  author = {Huang, Xiaoshui and Qu, Wentao and Zuo, Yifan and Fang, Yuming and Zhao, Xiaowei},
  title = {IMFNet: Interpretable Multimodal Fusion for Point Cloud Registration},
  journal = {arXiv},
  year = {2021},
  url = {https://arxiv.org/abs/2111.09624v1}
}
</pre></td>
</tr>
<tr id="Liu2022May" class="entry">
	<td>Liu, Z., Tang, H., Amini, A., Yang, X., Mao, H., Rus, D. and Han, S.</td>
	<td>BEVFusion: Multi-Task Multi-Sensor Fusion with Unified Bird's-Eye View Representation <p class="infolinks">[<a href="javascript:toggleInfo('Liu2022May','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Liu2022May','bibtex')">BibTeX</a>]</p></td>
	<td>2022</td>
	<td>arXiv&nbsp;</td>
	<td>article</td>
	<td><a href="https://arxiv.org/abs/2205.13542v2">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Liu2022May" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Multi-sensor fusion is essential for an accurate and reliable autonomous driving system. Recent approaches are based on point-level fusion: augmenting the LiDAR point cloud with camera features. However, the camera-to-LiDAR projection throws away the semantic density of camera features, hindering the effectiveness of such methods, especially for semantic-oriented tasks (such as 3D scene segmentation). In this paper, we break this deeply-rooted convention with BEVFusion, an efficient and generic multi-task multi-sensor fusion framework. It unifies multi-modal features in the shared bird's-eye view (BEV) representation space, which nicely preserves both geometric and semantic information. To achieve this, we diagnose and lift key efficiency bottlenecks in the view transformation with optimized BEV pooling, reducing latency by more than 40x. BEVFusion is fundamentally task-agnostic and seamlessly supports different 3D perception tasks with almost no architectural changes. It establishes the new state of the art on nuScenes, achieving 1.3% higher mAP and NDS on 3D object detection and 13.6% higher mIoU on BEV map segmentation, with 1.9x lower computation cost. Code to reproduce our results is available at this https URL.</td>
</tr>
<tr id="bib_Liu2022May" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Liu2022May,
  author = {Liu, Zhijian and Tang, Haotian and Amini, Alexander and Yang, Xinyu and Mao, Huizi and Rus, Daniela and Han, Song},
  title = {BEVFusion: Multi-Task Multi-Sensor Fusion with Unified Bird's-Eye View Representation},
  journal = {arXiv},
  year = {2022},
  url = {https://arxiv.org/abs/2205.13542v2}
}
</pre></td>
</tr>
<tr id="Chitta2022May" class="entry">
	<td>Chitta, K., Prakash, A., Jaeger, B., Yu, Z., Renz, K. and Geiger, A.</td>
	<td>TransFuser: Imitation with Transformer-Based Sensor Fusion for Autonomous Driving <p class="infolinks">[<a href="javascript:toggleInfo('Chitta2022May','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Chitta2022May','bibtex')">BibTeX</a>]</p></td>
	<td>2022</td>
	<td>arXiv&nbsp;</td>
	<td>article</td>
	<td><a href="https://arxiv.org/abs/2205.15997v1">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Chitta2022May" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: How should we integrate representations from complementary sensors for autonomous driving? Geometry-based fusion has shown promise for perception (e.g. object detection, motion forecasting). However, in the context of end-to-end driving, we find that imitation learning based on existing sensor fusion methods underperforms in complex driving scenarios with a high density of dynamic agents. Therefore, we propose TransFuser, a mechanism to integrate image and LiDAR representations using self-attention. Our approach uses transformer modules at multiple resolutions to fuse perspective view and bird's eye view feature maps. We experimentally validate its efficacy on a challenging new benchmark with long routes and dense traffic, as well as the official leaderboard of the CARLA urban driving simulator. At the time of submission, TransFuser outperforms all prior work on the CARLA leaderboard in terms of driving score by a large margin. Compared to geometry-based fusion, TransFuser reduces the average collisions per kilometer by 48%.</td>
</tr>
<tr id="bib_Chitta2022May" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Chitta2022May,
  author = {Chitta, Kashyap and Prakash, Aditya and Jaeger, Bernhard and Yu, Zehao and Renz, Katrin and Geiger, Andreas},
  title = {TransFuser: Imitation with Transformer-Based Sensor Fusion for Autonomous Driving},
  journal = {arXiv},
  year = {2022},
  url = {https://arxiv.org/abs/2205.15997v1}
}
</pre></td>
</tr>
<tr id="Matuszka2022Nov" class="entry">
	<td>Matuszka, T., Barton, I., Butykai, ifmmodeAelse., Hajas, P., Kiss, D., Kovifmmodeaelse&aacute;&#64257;cs, D., Kunsifmmodeaelse&aacute;&#64257;gi-Mifmmodeaelse&aacute;&#64257;tifmmodeeelse&eacute;&#64257;, S., Lengyel, P., Nifmmodeeelse&eacute;&#64257;meth, G., Petifmmode\H{o}else&#337;&#64257;, L., Ribli, D., Szeghy, D., Vajna, S. and Varga, B.</td>
	<td>aiMotive Dataset: A Multimodal Dataset for Robust Autonomous Driving with Long-Range Perception <p class="infolinks">[<a href="javascript:toggleInfo('Matuszka2022Nov','bibtex')">BibTeX</a>]</p></td>
	<td>2022</td>
	<td>arXiv&nbsp;</td>
	<td>article</td>
	<td><a href="https://doi.org/10.48550/arXiv.2211.09445">DOI</a> &nbsp;</td>
</tr>
<tr id="bib_Matuszka2022Nov" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Matuszka2022Nov,
  author = {Matuszka, Tamifmmodeaelse&aacute;&#64257;s and Barton, Ivifmmodeaelse&aacute;&#64257;n and Butykai, ifmmodeAelse&Aacute;&#64257;difmmodeaelse&aacute;&#64257;m and Hajas, Pifmmodeeelse&eacute;&#64257;ter and Kiss, Difmmodeaelse&aacute;&#64257;vid and Kovifmmodeaelse&aacute;&#64257;cs, Domonkos and Kunsifmmodeaelse&aacute;&#64257;gi-Mifmmodeaelse&aacute;&#64257;tifmmodeeelse&eacute;&#64257;, Sifmmodeaelse&aacute;&#64257;ndor and Lengyel, Pifmmodeeelse&eacute;&#64257;ter and Nifmmodeeelse&eacute;&#64257;meth, Gifmmodeaelse&aacute;&#64257;bor and Petifmmode\H{o}else&#337;&#64257;, Levente and Ribli, Dezsifmmode\H{o}else&#337;&#64257; and Szeghy, Difmmodeaelse&aacute;&#64257;vid and Vajna, Szabolcs and Varga, Bifmmodeaelse&aacute;&#64257;lint},
  title = {aiMotive Dataset: A Multimodal Dataset for Robust Autonomous Driving with Long-Range Perception},
  journal = {arXiv},
  year = {2022},
  doi = {https://doi.org/10.48550/arXiv.2211.09445}
}
</pre></td>
</tr>
<tr id="Feng2020Feb" class="entry">
	<td>Feng, D., Haase-Schifmmodeuelse&uuml;&#64257;tz, C., Rosenbaum, L., Hertlein, H., Glifmmodeaelse&auml;&#64257;ser, C., Timm, F., Wiesbeck, W. and Dietmayer, K.</td>
	<td>Deep Multi-Modal Object Detection and Semantic Segmentation for Autonomous Driving: Datasets, Methods, and Challenges <p class="infolinks">[<a href="javascript:toggleInfo('Feng2020Feb','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Feng2020Feb','comment')">Comment</a>] [<a href="javascript:toggleInfo('Feng2020Feb','bibtex')">BibTeX</a>]</p></td>
	<td>2020</td>
	<td>IEEE Trans. Intell. Transp. Syst.<br/>Vol. 22(3), pp. 1341-1360&nbsp;</td>
	<td>article</td>
	<td><a href="https://ieeexplore.ieee.org/abstract/document/9000872?casa_token=xxfsDn9JltkAAAAA:UHwwFqAb6b8pfvisl3U9BR4WXfeoYAJdQldUUQBxu_zt9HSzbWeRKQzV1mrkjzEdeHvZH9tSuumG">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Feng2020Feb" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Recent advancements in perception for autonomous driving are driven by deep learning. In order to achieve robust and accurate scene understanding, autonomous vehicles are usually equipped with different sensors (e.g. cameras, LiDARs, Radars), and multiple sensing modalities can be fused to exploit their complementary properties. In this context, many methods have been proposed for deep multi-modal perception problems. However, there is no general guideline for network architecture design, and questions of &ldquo;what to fuse&rdquo;, &ldquo;when to fuse&rdquo;, and &ldquo;how to fuse&rdquo; remain open. This review paper attempts to systematically summarize methodologies and discuss challenges for deep multi-modal object detection and semantic segmentation in autonomous driving. To this end, we first provide an overview of on-board sensors on test vehicles, open datasets, and background information for object detection and semantic segmentation in autonomous driving research. We then summarize the fusion methodologies and discuss challenges and open questions. In the appendix, we provide tables that summarize topics and methods. We also provide an interactive online platform to navigate each reference: https://boschresearch.github.io/multimodalperception/.</td>
</tr>
<tr id="rev_Feng2020Feb" class="comment noshow">
	<td colspan="6"><b>Comment</b>: from Bosch</td>
</tr>
<tr id="bib_Feng2020Feb" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Feng2020Feb,
  author = {Feng, Di and Haase-Schifmmodeuelse&uuml;&#64257;tz, Christian and Rosenbaum, Lars and Hertlein, Heinz and Glifmmodeaelse&auml;&#64257;ser, Claudius and Timm, Fabian and Wiesbeck, Werner and Dietmayer, Klaus},
  title = {Deep Multi-Modal Object Detection and Semantic Segmentation for Autonomous Driving: Datasets, Methods, and Challenges},
  journal = {IEEE Trans. Intell. Transp. Syst.},
  publisher = {IEEE},
  year = {2020},
  volume = {22},
  number = {3},
  pages = {1341--1360},
  url = {https://ieeexplore.ieee.org/abstract/document/9000872?casa_token=xxfsDn9JltkAAAAA:UHwwFqAb6b8pfvisl3U9BR4WXfeoYAJdQldUUQBxu_zt9HSzbWeRKQzV1mrkjzEdeHvZH9tSuumG}
}
</pre></td>
</tr>
<tr id="Gohil" class="entry">
	<td>Gohil, P., Thoduka, S. and Plifmmodeoelse&ouml;&#64257;ger, P.G.</td>
	<td>Sensor Fusion and Multimodal Learning for Robotic Grasp Verification Using Neural Networks <p class="infolinks">[<a href="javascript:toggleInfo('Gohil','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Gohil','bibtex')">BibTeX</a>]</p></td>
	<td>2022</td>
	<td>Published in: 2022 26th International Conference on Pattern Recognition (ICPR)2022 26th International Conference on Pattern Recognition (ICPR), pp. 21-25&nbsp;</td>
	<td>incollection</td>
	<td><a href="https://ieeexplore.ieee.org/document/9955646">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Gohil" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Different sensors on a robot help in understanding different aspects of the environment they are working in; however, each sensor modality is often processed individually and information from other sensors is not utilized jointly. One of the reasons is different sampling rates and different dimensions of input modalities. In this paper, we use multimodal data fusion techniques such as early, late and intermediate fusion for grasp failure identification using four different 3D convolution-based multimodal neural networks (3D-MNN). Our results on a visual-tactile dataset shows that the performance of the classification task is improved while using multimodal data. In addition, a neural network trained with 30:22 train-test split of multimodal data achieved accuracy comparable to a network trained with 78:22 train-test split of unimodal data 1 .</td>
</tr>
<tr id="bib_Gohil" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@incollection{Gohil,
  author = {Gohil, Priteshkumar and Thoduka, Santosh and Plifmmodeoelse&ouml;&#64257;ger, Paul G.},
  title = {Sensor Fusion and Multimodal Learning for Robotic Grasp Verification Using Neural Networks},
  booktitle = {2022 26th International Conference on Pattern Recognition (ICPR)},
  journal = {Published in: 2022 26th International Conference on Pattern Recognition (ICPR)},
  publisher = {IEEE},
  year = {2022},
  pages = {21--25},
  url = {https://ieeexplore.ieee.org/document/9955646}
}
</pre></td>
</tr>
<tr id="Caesar2019Mar" class="entry">
	<td>Caesar, H., Bankiti, V., Lang, A.H., Vora, S., Liong, V.E., Xu, Q., Krishnan, A., Pan, Y., Baldan, G. and Beijbom, O.</td>
	<td>nuScenes: A multimodal dataset for autonomous driving <p class="infolinks">[<a href="javascript:toggleInfo('Caesar2019Mar','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Caesar2019Mar','bibtex')">BibTeX</a>]</p></td>
	<td>2019</td>
	<td>arXiv&nbsp;</td>
	<td>article</td>
	<td><a href="https://arxiv.org/abs/1903.11027v5">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Caesar2019Mar" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Robust detection and tracking of objects is crucial for the deployment of autonomous vehicle technology. Image based benchmark datasets have driven development in computer vision tasks such as object detection, tracking and segmentation of agents in the environment. Most autonomous vehicles, however, carry a combination of cameras and range sensors such as lidar and radar. As machine learning based methods for detection and tracking become more prevalent, there is a need to train and evaluate such methods on datasets containing range sensor data along with images. In this work we present nuTonomy scenes (nuScenes), the first dataset to carry the full autonomous vehicle sensor suite: 6 cameras, 5 radars and 1 lidar, all with full 360 degree field of view. nuScenes comprises 1000 scenes, each 20s long and fully annotated with 3D bounding boxes for 23 classes and 8 attributes. It has 7x as many annotations and 100x as many images as the pioneering KITTI dataset. We define novel 3D detection and tracking metrics. We also provide careful dataset analysis as well as baselines for lidar and image based detection and tracking. Data, development kit and more information are available online.</td>
</tr>
<tr id="bib_Caesar2019Mar" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Caesar2019Mar,
  author = {Caesar, Holger and Bankiti, Varun and Lang, Alex H. and Vora, Sourabh and Liong, Venice Erin and Xu, Qiang and Krishnan, Anush and Pan, Yu and Baldan, Giancarlo and Beijbom, Oscar},
  title = {nuScenes: A multimodal dataset for autonomous driving},
  journal = {arXiv},
  year = {2019},
  url = {https://arxiv.org/abs/1903.11027v5}
}
</pre></td>
</tr>
<tr id="Chang2020Feb" class="entry">
	<td>Chang, S., Zhang, Y., Zhang, F., Zhao, X., Huang, S., Feng, Z. and Wei, Z.</td>
	<td>Spatial Attention Fusion for Obstacle Detection Using MmWave Radar and Vision Sensor <p class="infolinks">[<a href="javascript:toggleInfo('Chang2020Feb','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Chang2020Feb','bibtex')">BibTeX</a>]</p></td>
	<td>2020</td>
	<td>Sensors<br/>Vol. 20(4), pp. 956&nbsp;</td>
	<td>article</td>
	<td><a href="https://www.mdpi.com/1424-8220/20/4/956">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Chang2020Feb" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: For autonomous driving, it is important to detect obstacles in all scales accurately for safety consideration. In this paper, we propose a new spatial attention fusion (SAF) method for obstacle detection using mmWave radar and vision sensor, where the sparsity of radar points are considered in the proposed SAF. The proposed fusion method can be embedded in the feature-extraction stage, which leverages the features of mmWave radar and vision sensor effectively. Based on the SAF, an attention weight matrix is generated to fuse the vision features, which is different from the concatenation fusion and element-wise add fusion. Moreover, the proposed SAF can be trained by an end-to-end manner incorporated with the recent deep learning object detection framework. In addition, we build a generation model, which converts radar points to radar images for neural network training. Numerical results suggest that the newly developed fusion method achieves superior performance in public benchmarking. In addition, the source code will be released in the GitHub. Keywords: autonomous driving; obstacle detection; mmWave radar; vision; spatial attention fusion</td>
</tr>
<tr id="bib_Chang2020Feb" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Chang2020Feb,
  author = {Chang, Shuo and Zhang, Yifan and Zhang, Fan and Zhao, Xiaotong and Huang, Sai and Feng, Zhiyong and Wei, Zhiqing},
  title = {Spatial Attention Fusion for Obstacle Detection Using MmWave Radar and Vision Sensor},
  journal = {Sensors},
  publisher = {Multidisciplinary Digital Publishing Institute},
  year = {2020},
  volume = {20},
  number = {4},
  pages = {956},
  url = {https://www.mdpi.com/1424-8220/20/4/956}
}
</pre></td>
</tr>
<tr id="Cheng2021Oct" class="entry">
	<td>Cheng, Y., Xu, H. and Liu, Y.</td>
	<td>Robust Small Object Detection on the Water Surface through Fusion of Camera and Millimeter Wave Radar <p class="infolinks">[<a href="javascript:toggleInfo('Cheng2021Oct','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Cheng2021Oct','bibtex')">BibTeX</a>]</p></td>
	<td>2021</td>
	<td>2021 IEEE/CVF International Conference on Computer Vision (ICCV)2021 IEEE/CVF International Conference on Computer Vision (ICCV), pp. 15243-15252&nbsp;</td>
	<td>incollection</td>
	<td><a href="https://ieeexplore.ieee.org/document/9710582">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Cheng2021Oct" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: In recent years, unmanned surface vehicles (USVs) have been experiencing growth in various applications. With the expansion of USVs' application scenes from the typical marine areas to inland waters, new challenges arise for the object detection task, which is an essential part of the perception system of USVs. In our work, we focus on a relatively unexplored task for USVs in inland waters: small object detection on water surfaces, which is of vital importance for safe autonomous navigation and USVs' certain missions such as floating waste cleaning. Considering the limitations of vision-based object detection, we propose a novel radar-vision fusion based method for robust small object detection on water surfaces. By using a novel representation format of millimeter wave radar point clouds and applying a deep-level multi-scale fusion of RGB images and radar data, the proposed method can efficiently utilize the characteristics of radar data and improve the accuracy and robustness for small object detection on water surfaces. We test the method on the real-world floating bottle dataset that we collected and released. The result shows that, our method improves the average detection accuracy significantly compared to the vision-based methods and achieves state-of-the-art performance. Besides, the proposed method performs robustly when single sensor degrades.</td>
</tr>
<tr id="bib_Cheng2021Oct" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@incollection{Cheng2021Oct,
  author = {Cheng, Yuwei and Xu, Hu and Liu, Yimin},
  title = {Robust Small Object Detection on the Water Surface through Fusion of Camera and Millimeter Wave Radar},
  booktitle = {2021 IEEE/CVF International Conference on Computer Vision (ICCV)},
  journal = {2021 IEEE/CVF International Conference on Computer Vision (ICCV)},
  publisher = {IEEE},
  year = {2021},
  pages = {15243--15252},
  url = {https://ieeexplore.ieee.org/document/9710582}
}
</pre></td>
</tr>
<tr id="Shuai2021May" class="entry">
	<td>Shuai, X., Shen, Y., Tang, Y., Shi, S., Ji, L. and Xing, G.</td>
	<td>milliEye: A Lightweight mmWave Radar and Camera Fusion System for Robust Object Detection <p class="infolinks">[<a href="javascript:toggleInfo('Shuai2021May','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Shuai2021May','bibtex')">BibTeX</a>]</p></td>
	<td>2021</td>
	<td>IoTDI '21: Proceedings of the International Conference on Internet-of-Things Design and Implementation, pp. 145-157&nbsp;</td>
	<td>incollection</td>
	<td><a href="https://dl.acm.org/doi/abs/10.1145/3450268.3453532">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Shuai2021May" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: A wide range of advanced deep learning algorithms have recently been proposed for image classification and object detection. However, the effectiveness of these methods can be significantly restricted in many real-world scenarios where the visibility or illumination is poor. Compared to RGB cameras, millimeter-wave (mmWave) radars are immune to the above environmental variability and can assist cameras under adverse conditions. To this end, we propose milliEye, a lightweight mmWave radar and camera fusion system for robust object detection on the edge platforms. milliEye has several key advantages over existing sensor fusion approaches. First, while milliEye fuses two sensing modalities in a learning-based fashion, it requires only a small amount of labeled image/radar data of a new scene as it can fully utilize large public image datasets for extensive training. This salient feature enables milliEye to adapt to highly complex real-world environments. Second, based on a novel architecture that decouples the image-based object detector from other modules, milliEye is compatible with different off-the-shelf image-based object detectors. As a result, it can take advantage of the rapid progress of object detection algorithms. Moreover, thanks to the highly compute-efficient fusion approach, milliEye is lightweight and thus suitable for edge-based real-time applications. To evaluate the performance of milliEye, we collect a new radar and camera fusion dataset for object detection, which contains both ordinary-light and low-light illumination conditions. The results show that milliEye can provide substantial performance boosts over state-of-the-art image-based object detectors, including Tiny YOLOv3 and SSD, especially in low-light scenes, while incurring low compute overhead on edge platforms.</td>
</tr>
<tr id="bib_Shuai2021May" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@incollection{Shuai2021May,
  author = {Shuai, Xian and Shen, Yulin and Tang, Yi and Shi, Shuyao and Ji, Luping and Xing, Guoliang},
  title = {milliEye: A Lightweight mmWave Radar and Camera Fusion System for Robust Object Detection},
  booktitle = {IoTDI '21: Proceedings of the International Conference on Internet-of-Things Design and Implementation},
  publisher = {Association for Computing Machinery},
  year = {2021},
  pages = {145--157},
  url = {https://dl.acm.org/doi/abs/10.1145/3450268.3453532}
}
</pre></td>
</tr>
<tr id="Geyer2020Apr" class="entry">
	<td>Geyer, J., Kassahun, Y., Mahmudi, M., Ricou, X., Durgesh, R., Chung, A.S., Hauswald, L., Pham, V.H., Mifmmodeuelse&uuml;&#64257;hlegg, M., Dorn, S., Fernandez, T., Jifmmodeaelse&auml;&#64257;nicke, M., Mirashi, S., Savani, C., Sturm, M., Vorobiov, O., Oelker, M., Garreis, S. and Schuberth, P.</td>
	<td>A2D2: Audi Autonomous Driving Dataset <p class="infolinks">[<a href="javascript:toggleInfo('Geyer2020Apr','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Geyer2020Apr','bibtex')">BibTeX</a>]</p></td>
	<td>2020</td>
	<td>arXiv&nbsp;</td>
	<td>article</td>
	<td><a href="https://arxiv.org/abs/2004.06320v1">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Geyer2020Apr" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Research in machine learning, mobile robotics, and autonomous driving is accelerated by the availability of high quality annotated data. To this end, we release the Audi Autonomous Driving Dataset (A2D2). Our dataset consists of simultaneously recorded images and 3D point clouds, together with 3D bounding boxes, semantic segmentation, instance segmentation, and data extracted from the automotive bus. Our sensor suite consists of six cameras and five LiDAR units, providing full 360 degree coverage. The recorded data is time synchronized and mutually registered. Annotations are for non-sequential frames: 41,277 frames with semantic segmentation image and point cloud labels, of which 12,497 frames also have 3D bounding box annotations for objects within the field of view of the front camera. In addition, we provide 392,556 sequential frames of unannotated sensor data for recordings in three cities in the south of Germany. These sequences contain several loops. Faces and vehicle number plates are blurred due to GDPR legislation and to preserve anonymity. A2D2 is made available under the CC BY-ND 4.0 license, permitting commercial use subject to the terms of the license. Data and further information are available at http://www.a2d2.audi.</td>
</tr>
<tr id="bib_Geyer2020Apr" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Geyer2020Apr,
  author = {Geyer, Jakob and Kassahun, Yohannes and Mahmudi, Mentar and Ricou, Xavier and Durgesh, Rupesh and Chung, Andrew S. and Hauswald, Lorenz and Pham, Viet Hoang and Mifmmodeuelse&uuml;&#64257;hlegg, Maximilian and Dorn, Sebastian and Fernandez, Tiffany and Jifmmodeaelse&auml;&#64257;nicke, Martin and Mirashi, Sudesh and Savani, Chiragkumar and Sturm, Martin and Vorobiov, Oleksandr and Oelker, Martin and Garreis, Sebastian and Schuberth, Peter},
  title = {A2D2: Audi Autonomous Driving Dataset},
  journal = {arXiv},
  year = {2020},
  url = {https://arxiv.org/abs/2004.06320v1}
}
</pre></td>
</tr>
<tr id="Sun2020" class="entry">
	<td>Sun, P., Kretzschmar, H., Dotiwalla, X., Chouard, A., Patnaik, V., Tsui, P., Guo, J., Zhou, Y., Chai, Y., Caine, B., Vasudevan, V., Han, W., Ngiam, J., Zhao, H., Timofeev, A., Ettinger, S., Krivokon, M., Gao, A., Joshi, A., Zhang, Y., Shlens, J., Chen, Z. and Anguelov, D.</td>
	<td>Scalability in Perception for Autonomous Driving: Waymo Open Dataset <p class="infolinks">[<a href="javascript:toggleInfo('Sun2020','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Sun2020','bibtex')">BibTeX</a>]</p></td>
	<td>2020</td>
	<td>, pp. 2446-2454&nbsp;</td>
	<td>misc</td>
	<td><a href="https://openaccess.thecvf.com/content_CVPR_2020/html/Sun_Scalability_in_Perception_for_Autonomous_Driving_Waymo_Open_Dataset_CVPR_2020_paper.html">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Sun2020" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: The research community has increasing interest in autonomous driving research, despite the resource intensity of obtaining representative real world data. Existing self-driving datasets are limited in the scale and variation of the environments they capture, even though generalization within and between operating regions is crucial to the over-all viability of the technology. In an effort to help align the research community's contributions with real-world self-driving problems, we introduce a new large scale, high quality, diverse dataset. Our new dataset consists of 1150 scenes that each span 20 seconds, consisting of well synchronized and calibrated high quality LiDAR and camera data captured across a range of urban and suburban geographies. It is 15x more diverse than the largest camera+LiDAR dataset available based on our proposed diversity metric. We exhaustively annotated this data with 2D (camera image) and 3D (LiDAR) bounding boxes, with consistent identifiers across frames. Finally, we provide strong baselines for 2D as well as 3D detection and tracking tasks. We further study the effects of dataset size and generalization across geographies on 3D detection methods. Find data, code and more up-to-date information at http://www.waymo.com/open.</td>
</tr>
<tr id="bib_Sun2020" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@misc{Sun2020,
  author = {Sun, Pei and Kretzschmar, Henrik and Dotiwalla, Xerxes and Chouard, Aurelien and Patnaik, Vijaysai and Tsui, Paul and Guo, James and Zhou, Yin and Chai, Yuning and Caine, Benjamin and Vasudevan, Vijay and Han, Wei and Ngiam, Jiquan and Zhao, Hang and Timofeev, Aleksei and Ettinger, Scott and Krivokon, Maxim and Gao, Amy and Joshi, Aditya and Zhang, Yu and Shlens, Jonathon and Chen, Zhifeng and Anguelov, Dragomir},
  title = {Scalability in Perception for Autonomous Driving: Waymo Open Dataset},
  year = {2020},
  pages = {2446--2454},
  note = {[Online; accessed 14. Dec. 2022]},
  url = {https://openaccess.thecvf.com/content_CVPR_2020/html/Sun_Scalability_in_Perception_for_Autonomous_Driving_Waymo_Open_Dataset_CVPR_2020_paper.html}
}
</pre></td>
</tr>
<tr id="Nabati" class="entry">
	<td>Nabati, R. and Qi, H.</td>
	<td>RRPN: Radar Region Proposal Network for Object Detection in Autonomous Vehicles <p class="infolinks">[<a href="javascript:toggleInfo('Nabati','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Nabati','bibtex')">BibTeX</a>]</p></td>
	<td>2019</td>
	<td>Published in: 2019 IEEE International Conference on Image Processing (ICIP)2019 IEEE International Conference on Image Processing (ICIP), pp. 22-25&nbsp;</td>
	<td>incollection</td>
	<td><a href="https://ieeexplore.ieee.org/abstract/document/8803392?casa_token=kx0KF3J5AzUAAAAA:VWZllClS9535awmq5zCreagvZviF_VfuCE2wtAajpuP40M7N_AnCYx2wjPkGr6qR1jkUZwxlyJwo">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Nabati" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Region proposal algorithms play an important role in most state-of-the-art two-stage object detection networks by hypothesizing object locations in the image. Nonetheless, region proposal algorithms are known to be the bottleneck in most two-stage object detection networks, increasing the processing time for each image and resulting in slow networks not suitable for real-time applications such as autonomous driving vehicles. In this paper we introduce RRPN, a Radar-based real-time region proposal algorithm for object detection in autonomous driving vehicles. RRPN generates object proposals by mapping Radar detections to the image coordinate system and generating pre-defined anchor boxes for each mapped Radar detection point. These anchor boxes are then transformed and scaled based on the object's distance from the vehicle, to provide more accurate proposals for the detected objects. We evaluate our method on the newly released NuScenes dataset [1] using the Fast R-CNN object detection network [2]. Compared to the Selective Search object proposal algorithm [3], our model operates more than 100ifmmode&times;elsetexttimes&#64257; faster while at the same time achieves higher detection precision and recall. Code has been made publicly available at https://github.com/mrnabati/RRPN.</td>
</tr>
<tr id="bib_Nabati" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@incollection{Nabati,
  author = {Nabati, Ramin and Qi, Hairong},
  title = {RRPN: Radar Region Proposal Network for Object Detection in Autonomous Vehicles},
  booktitle = {2019 IEEE International Conference on Image Processing (ICIP)},
  journal = {Published in: 2019 IEEE International Conference on Image Processing (ICIP)},
  publisher = {IEEE},
  year = {2019},
  pages = {22--25},
  url = {https://ieeexplore.ieee.org/abstract/document/8803392?casa_token=kx0KF3J5AzUAAAAA:VWZllClS9535awmq5zCreagvZviF_VfuCE2wtAajpuP40M7N_AnCYx2wjPkGr6qR1jkUZwxlyJwo}
}
</pre></td>
</tr>
<tr id="Chadwick" class="entry">
	<td>Chadwick, S., Maddern, W. and Newman, P.</td>
	<td>Distant Vehicle Detection Using Radar and Vision <p class="infolinks">[<a href="javascript:toggleInfo('Chadwick','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Chadwick','bibtex')">BibTeX</a>]</p></td>
	<td>2019</td>
	<td>Published in: 2019 International Conference on Robotics and Automation (ICRA)2019 International Conference on Robotics and Automation (ICRA), pp. 20-24&nbsp;</td>
	<td>incollection</td>
	<td><a href="https://ieeexplore.ieee.org/abstract/document/8794312?casa_token=O186EQMAZg0AAAAA:EylpF-Vb0ej42VXy5olYx3D7tL1YZLV9T_OJ1k8RzKJv6zT11BjV6ANxGzeSPrRBJduM5y-5aR6v">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Chadwick" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: For autonomous vehicles to be able to operate successfully they need to be aware of other vehicles with sufficient time to make safe, stable plans. Given the possible closing speeds between two vehicles, this necessitates the ability to accurately detect distant vehicles. Many current image-based object detectors using convolutional neural networks exhibit excellent performance on existing datasets such as KITTI. However, the performance of these networks falls when detecting small (distant) objects. We demonstrate that incorporating radar data can boost performance in these difficult situations. We also introduce an efficient automated method for training data generation using cameras of different focal lengths.</td>
</tr>
<tr id="bib_Chadwick" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@incollection{Chadwick,
  author = {Chadwick, Simon and Maddern, Will and Newman, Paul},
  title = {Distant Vehicle Detection Using Radar and Vision},
  booktitle = {2019 International Conference on Robotics and Automation (ICRA)},
  journal = {Published in: 2019 International Conference on Robotics and Automation (ICRA)},
  publisher = {IEEE},
  year = {2019},
  pages = {20--24},
  url = {https://ieeexplore.ieee.org/abstract/document/8794312?casa_token=O186EQMAZg0AAAAA:EylpF-Vb0ej42VXy5olYx3D7tL1YZLV9T_OJ1k8RzKJv6zT11BjV6ANxGzeSPrRBJduM5y-5aR6v}
}
</pre></td>
</tr>
<tr id="Zhou2022May" class="entry">
	<td>Zhou, Y., Liu, L., Zhao, H., Lifmmodeoelse&oacute;&#64257;pez-Benifmmode\imathelse\i&#64257;tez, M., Yu, L. and Yue, Y.</td>
	<td>Towards Deep Radar Perception for Autonomous Driving: Datasets, Methods, and Challenges <p class="infolinks">[<a href="javascript:toggleInfo('Zhou2022May','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Zhou2022May','bibtex')">BibTeX</a>]</p></td>
	<td>2022</td>
	<td>Sensors<br/>Vol. 22(11), pp. 4208&nbsp;</td>
	<td>article</td>
	<td><a href="https://www.mdpi.com/1424-8220/22/11/4208">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Zhou2022May" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: With recent developments, the performance of automotive radar has improved significantly. The next generation of 4D radar can achieve imaging capability in the form of high-resolution point clouds. In this context, we believe that the era of deep learning for radar perception has arrived. However, studies on radar deep learning are spread across different tasks, and a holistic overview is lacking. This review paper attempts to provide a big picture of the deep radar perception stack, including signal processing, datasets, labelling, data augmentation, and downstream tasks such as depth and velocity estimation, object detection, and sensor fusion. For these tasks, we focus on explaining how the network structure is adapted to radar domain knowledge. In particular, we summarise three overlooked challenges in deep radar perception, including multi-path effects, uncertainty problems, and adverse weather effects, and present some attempts to solve them. Keywords: automotive radars; radar signal processing; object detection; multi-sensor fusion; deep learning; autonomous driving</td>
</tr>
<tr id="bib_Zhou2022May" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Zhou2022May,
  author = {Zhou, Yi and Liu, Lulu and Zhao, Haocheng and Lifmmodeoelse&oacute;&#64257;pez-Benifmmode\imathelse\i&#64257;tez, Miguel and Yu, Limin and Yue, Yutao},
  title = {Towards Deep Radar Perception for Autonomous Driving: Datasets, Methods, and Challenges},
  journal = {Sensors},
  publisher = {Multidisciplinary Digital Publishing Institute},
  year = {2022},
  volume = {22},
  number = {11},
  pages = {4208},
  url = {https://www.mdpi.com/1424-8220/22/11/4208}
}
</pre></td>
</tr>
<tr id="Huang2022Feb" class="entry">
	<td>Huang, K., Shi, B., Li, X., Li, X., Huang, S. and Li, Y.</td>
	<td>Multi-modal Sensor Fusion for Auto Driving Perception: A Survey <p class="infolinks">[<a href="javascript:toggleInfo('Huang2022Feb','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Huang2022Feb','bibtex')">BibTeX</a>]</p></td>
	<td>2022</td>
	<td>arXiv&nbsp;</td>
	<td>article</td>
	<td><a href="https://arxiv.org/abs/2202.02703v2">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Huang2022Feb" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Multi-modal fusion is a fundamental task for the perception of an autonomous driving system, which has recently intrigued many researchers. However, achieving a rather good performance is not an easy task due to the noisy raw data, underutilized information, and the misalignment of multi-modal sensors. In this paper, we provide a literature review of the existing multi-modal-based methods for perception tasks in autonomous driving. Generally, we make a detailed analysis including over 50 papers leveraging perception sensors including LiDAR and camera trying to solve object detection and semantic segmentation tasks. Different from traditional fusion methodology for categorizing fusion models, we propose an innovative way that divides them into two major classes, four minor classes by a more reasonable taxonomy in the view of the fusion stage. Moreover, we dive deep into the current fusion methods, focusing on the remaining problems and open-up discussions on the potential research opportunities. In conclusion, what we expect to do in this paper is to present a new taxonomy of multi-modal fusion methods for the autonomous driving perception tasks and provoke thoughts of the fusion-based techniques in the future.</td>
</tr>
<tr id="bib_Huang2022Feb" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Huang2022Feb,
  author = {Huang, Keli and Shi, Botian and Li, Xiang and Li, Xin and Huang, Siyuan and Li, Yikang},
  title = {Multi-modal Sensor Fusion for Auto Driving Perception: A Survey},
  journal = {arXiv},
  year = {2022},
  url = {https://arxiv.org/abs/2202.02703v2}
}
</pre></td>
</tr>
<tr id="Hwang2022Oct" class="entry">
	<td>Hwang, J.-J., Kretzschmar, H., Manela, J., Rafferty, S., Armstrong-Crews, N., Chen, T. and Anguelov, D.</td>
	<td>CramNet: Camera-Radar Fusion with Ray-Constrained Cross-Attention for Robust 3D Object Detection <p class="infolinks">[<a href="javascript:toggleInfo('Hwang2022Oct','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Hwang2022Oct','bibtex')">BibTeX</a>]</p></td>
	<td>2022</td>
	<td>arXiv&nbsp;</td>
	<td>article</td>
	<td><a href="https://arxiv.org/abs/2210.09267v2">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Hwang2022Oct" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Robust 3D object detection is critical for safe autonomous driving. Camera and radar sensors are synergistic as they capture complementary information and work well under different environmental conditions. Fusing camera and radar data is challenging, however, as each of the sensors lacks information along a perpendicular axis, that is, depth is unknown to camera and elevation is unknown to radar. We propose the camera-radar matching network CramNet, an efficient approach to fuse the sensor readings from camera and radar in a joint 3D space. To leverage radar range measurements for better camera depth predictions, we propose a novel ray-constrained cross-attention mechanism that resolves the ambiguity in the geometric correspondences between camera features and radar features. Our method supports training with sensor modality dropout, which leads to robust 3D object detection, even when a camera or radar sensor suddenly malfunctions on a vehicle. We demonstrate the effectiveness of our fusion approach through extensive experiments on the RADIATE dataset, one of the few large-scale datasets that provide radar radio frequency imagery. A camera-only variant of our method achieves competitive performance in monocular 3D object detection on the Waymo Open Dataset.</td>
</tr>
<tr id="bib_Hwang2022Oct" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Hwang2022Oct,
  author = {Hwang, Jyh-Jing and Kretzschmar, Henrik and Manela, Joshua and Rafferty, Sean and Armstrong-Crews, Nicholas and Chen, Tiffany and Anguelov, Dragomir},
  title = {CramNet: Camera-Radar Fusion with Ray-Constrained Cross-Attention for Robust 3D Object Detection},
  journal = {arXiv},
  year = {2022},
  url = {https://arxiv.org/abs/2210.09267v2}
}
</pre></td>
</tr>
<tr id="Paek2022Jun" class="entry">
	<td>Paek, D.-H., Kong, S.-H. and Wijaya, K.T.</td>
	<td>K-Radar: 4D Radar Object Detection for Autonomous Driving in Various Weather Conditions <p class="infolinks">[<a href="javascript:toggleInfo('Paek2022Jun','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Paek2022Jun','bibtex')">BibTeX</a>]</p></td>
	<td>2022</td>
	<td>arXiv&nbsp;</td>
	<td>article</td>
	<td><a href="https://arxiv.org/abs/2206.08171v2">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Paek2022Jun" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Unlike RGB cameras that use visible light bands (384&dollar;elsetextdollar&#64257;ifmmodebackslashelse&bsol;&#64257;sim769 THz) and Lidar that use infrared bands (361ifmmodebackslashelse&bsol;&#64257;sim331 THz), Radars use relatively longer wavelength radio bands (77ifmmodebackslashelse&bsol;&#64257;sim&dollar;elsetextdollar&#64257;81 GHz), resulting in robust measurements in adverse weathers. Unfortunately, existing Radar datasets only contain a relatively small number of samples compared to the existing camera and Lidar datasets. This may hinder the development of sophisticated data-driven deep learning techniques for Radar-based perception. Moreover, most of the existing Radar datasets only provide 3D Radar tensor (3DRT) data that contain power measurements along the Doppler, range, and azimuth dimensions. As there is no elevation information, it is challenging to estimate the 3D bounding box of an object from 3DRT. In this work, we introduce KAIST-Radar (K-Radar), a novel large-scale object detection dataset and benchmark that contains 35K frames of 4D Radar tensor (4DRT) data with power measurements along the Doppler, range, azimuth, and elevation dimensions, together with carefully annotated 3D bounding box labels of objects on the roads. K-Radar includes challenging driving conditions such as adverse weathers (fog, rain, and snow) on various road structures (urban, suburban roads, alleyways, and highways). In addition to the 4DRT, we provide auxiliary measurements from carefully calibrated high-resolution Lidars, surround stereo cameras, and RTK-GPS. We also provide 4DRT-based object detection baseline neural networks (baseline NNs) and show that the height information is crucial for 3D object detection. And by comparing the baseline NN with a similarly-structured Lidar-based neural network, we demonstrate that 4D Radar is a more robust sensor for adverse weather conditions. All codes are available at this https URL.</td>
</tr>
<tr id="bib_Paek2022Jun" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Paek2022Jun,
  author = {Paek, Dong-Hee and Kong, Seung-Hyun and Wijaya, Kevin Tirta},
  title = {K-Radar: 4D Radar Object Detection for Autonomous Driving in Various Weather Conditions},
  journal = {arXiv},
  year = {2022},
  url = {https://arxiv.org/abs/2206.08171v2}
}
</pre></td>
</tr>
<tr id="Zhang2021Dec" class="entry">
	<td>Zhang, Y., Carballo, A., Yang, H. and Takeda, K.</td>
	<td>Autonomous Driving in Adverse Weather Conditions: A Survey <p class="infolinks">[<a href="javascript:toggleInfo('Zhang2021Dec','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Zhang2021Dec','bibtex')">BibTeX</a>]</p></td>
	<td>2021</td>
	<td>arXiv&nbsp;</td>
	<td>article</td>
	<td><a href="https://arxiv.org/abs/2112.08936v1">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Zhang2021Dec" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Automated Driving Systems (ADS) open up a new domain for the automotive industry and offer new possibilities for future transportation with higher efficiency and comfortable experiences. However, autonomous driving under adverse weather conditions has been the problem that keeps autonomous vehicles (AVs) from going to level 4 or higher autonomy for a long time. This paper assesses the influences and challenges that weather brings to ADS sensors in an analytic and statistical way, and surveys the solutions against inclement weather conditions. State-of-the-art techniques on perception enhancement with regard to each kind of weather are thoroughly reported. External auxiliary solutions like V2X technology, weather conditions coverage in currently available datasets, simulators, and experimental facilities with weather chambers are distinctly sorted out. By pointing out all kinds of major weather problems the autonomous driving field is currently facing, and reviewing both hardware and computer science solutions in recent years, this survey contributes a holistic overview on the obstacles and directions of ADS development in terms of adverse weather driving conditions.</td>
</tr>
<tr id="bib_Zhang2021Dec" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Zhang2021Dec,
  author = {Zhang, Yuxiao and Carballo, Alexander and Yang, Hanting and Takeda, Kazuya},
  title = {Autonomous Driving in Adverse Weather Conditions: A Survey},
  journal = {arXiv},
  year = {2021},
  url = {https://arxiv.org/abs/2112.08936v1}
}
</pre></td>
</tr>
<tr id="Yan2019Sep" class="entry">
	<td>Yan, Z., Sun, L., Krajnik, T. and Ruichek, Y.</td>
	<td>EU Long-term Dataset with Multiple Sensors for Autonomous Driving <p class="infolinks">[<a href="javascript:toggleInfo('Yan2019Sep','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Yan2019Sep','bibtex')">BibTeX</a>]</p></td>
	<td>2019</td>
	<td>arXiv&nbsp;</td>
	<td>article</td>
	<td><a href="https://arxiv.org/abs/1909.03330v3">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Yan2019Sep" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: The field of autonomous driving has grown tremendously over the past few years, along with the rapid progress in sensor technology. One of the major purposes of using sensors is to provide environment perception for vehicle understanding, learning and reasoning, and ultimately interacting with the environment. In this paper, we first introduce a multisensor platform allowing vehicle to perceive its surroundings and locate itself in a more efficient and accurate way. The platform integrates eleven heterogeneous sensors including various cameras and lidars, a radar, an IMU (Inertial Measurement Unit), and a GPS-RTK (Global Positioning System / Real-Time Kinematic), while exploits a ROS (Robot Operating System) based software to process the sensory data. Then, we present a new dataset (this https URL) for autonomous driving captured many new research challenges (e.g. highly dynamic environment), and especially for long-term autonomy (e.g. creating and maintaining maps), collected with our instrumented vehicle, publicly available to the community.</td>
</tr>
<tr id="bib_Yan2019Sep" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Yan2019Sep,
  author = {Yan, Zhi and Sun, Li and Krajnik, Tomas and Ruichek, Yassine},
  title = {EU Long-term Dataset with Multiple Sensors for Autonomous Driving},
  journal = {arXiv},
  year = {2019},
  url = {https://arxiv.org/abs/1909.03330v3}
}
</pre></td>
</tr>
<tr id="Nabati2020Nov" class="entry">
	<td>Nabati, R. and Qi, H.</td>
	<td>CenterFusion: Center-based Radar and Camera Fusion for 3D Object Detection <p class="infolinks">[<a href="javascript:toggleInfo('Nabati2020Nov','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Nabati2020Nov','bibtex')">BibTeX</a>]</p></td>
	<td>2020</td>
	<td>arXiv&nbsp;</td>
	<td>article</td>
	<td><a href="https://arxiv.org/abs/2011.04841v1">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Nabati2020Nov" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: The perception system in autonomous vehicles is responsible for detecting and tracking the surrounding objects. This is usually done by taking advantage of several sensing modalities to increase robustness and accuracy, which makes sensor fusion a crucial part of the perception system. In this paper, we focus on the problem of radar and camera sensor fusion and propose a middle-fusion approach to exploit both radar and camera data for 3D object detection. Our approach, called CenterFusion, first uses a center point detection network to detect objects by identifying their center points on the image. It then solves the key data association problem using a novel frustum-based method to associate the radar detections to their corresponding object's center point. The associated radar detections are used to generate radar-based feature maps to complement the image features, and regress to object properties such as depth, rotation and velocity. We evaluate CenterFusion on the challenging nuScenes dataset, where it improves the overall nuScenes Detection Score (NDS) of the state-of-the-art camera-based algorithm by more than 12%. We further show that CenterFusion significantly improves the velocity estimation accuracy without using any additional temporal information. The code is available at this https URL .</td>
</tr>
<tr id="bib_Nabati2020Nov" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Nabati2020Nov,
  author = {Nabati, Ramin and Qi, Hairong},
  title = {CenterFusion: Center-based Radar and Camera Fusion for 3D Object Detection},
  journal = {arXiv},
  year = {2020},
  url = {https://arxiv.org/abs/2011.04841v1}
}
</pre></td>
</tr>
<tr id="Kim2021Jan" class="entry">
	<td>Kim, Y., Choi, J.W. and Kum, D.</td>
	<td>GRIF Net: Gated Region of Interest Fusion Network for Robust 3D Object Detection from Radar Point Cloud and Monocular Image <p class="infolinks">[<a href="javascript:toggleInfo('Kim2021Jan','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Kim2021Jan','bibtex')">BibTeX</a>]</p></td>
	<td>2021</td>
	<td>2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 10857-10864&nbsp;</td>
	<td>incollection</td>
	<td><a href="https://ieeexplore.ieee.org/document/9341177">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Kim2021Jan" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Robust and accurate scene representation is essential for advanced driver assistance systems (ADAS) such as automated driving. The radar and camera are two widely used sensors for commercial vehicles due to their low-cost, high-reliability, and low-maintenance. Despite their strengths, radar and camera have very limited performance when used individually. In this paper, we propose a low-level sensor fusion 3D object detector that combines two Region of Interest (RoI) from radar and camera feature maps by a Gated RoI Fusion (GRIF) to perform robust vehicle detection. To take advantage of sensors and utilize a sparse radar point cloud, we design a GRIF that employs the explicit gating mechanism to adaptively select the appropriate data when one of the sensors is abnormal. Our experimental evaluations on nuScenes show that our fusion method GRIF not only has significant performance improvement over single radar and image method but achieves comparable performance to the LiDAR detection method. We also observe that the proposed GRIF achieve higher recall than mean or concatenation fusion operation when points are sparse.</td>
</tr>
<tr id="bib_Kim2021Jan" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@incollection{Kim2021Jan,
  author = {Kim, Youngseok and Choi, Jun Won and Kum, Dongsuk},
  title = {GRIF Net: Gated Region of Interest Fusion Network for Robust 3D Object Detection from Radar Point Cloud and Monocular Image},
  booktitle = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  journal = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  publisher = {IEEE},
  year = {2021},
  pages = {10857--10864},
  url = {https://ieeexplore.ieee.org/document/9341177}
}
</pre></td>
</tr>
<tr id="Danapal2022Oct" class="entry">
	<td>Danapal, G., Mayr, C., Kariminezhad, A., Vriesmann, D. and Zimmer, A.</td>
	<td>Attention Empowered Feature-level Radar-Camera Fusion for Object Detection <p class="infolinks">[<a href="javascript:toggleInfo('Danapal2022Oct','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Danapal2022Oct','bibtex')">BibTeX</a>]</p></td>
	<td>2022</td>
	<td>2022 Sensor Data Fusion: Trends, Solutions, Applications (SDF)2022 Sensor Data Fusion: Trends, Solutions, Applications (SDF), pp. 1-6&nbsp;</td>
	<td>incollection</td>
	<td><a href="https://ieeexplore.ieee.org/document/9931956">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Danapal2022Oct" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Safe autonomous driving can not be realizable unless with robust environment perception. However, robustness can only be guaranteed if the system functions reliably in all weather conditions with any darkness level, while capturing all corner cases. Perception has heavily relied on cameras for object detection purposes. Due to operating at visible-light frequencies, there exists a plethora of corner cases for a sole camera-based perception system. In this paper, radar data is constructively fused with the RGB images for improving perception performance. Radar data that is in the form of point cloud is pre-processed by domain conversion from a bird-eye-view perspective into an image coordinate system. These alongside with RGB images from the camera are given as inputs to our proposed fusion network, which extracts the features of each sensor independently. These features are then fused to perform a joint detection. The robustness in adverse conditions like fog is validated via synthetically foggified images for different levels of fog densities. A channel attention module is integrated into the fusion network, which helps to prevent the drop in performance up to a fog density of 25. The network is trained and tested on NuScenes [1] dataset. Our proposed fusion network is capable of outperforming the other state-of-the-art radar-camera fusion networks by at least 8%.</td>
</tr>
<tr id="bib_Danapal2022Oct" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@incollection{Danapal2022Oct,
  author = {Danapal, Gokulesh and Mayr, Christian and Kariminezhad, Ali and Vriesmann, Daniel and Zimmer, Alessandro},
  title = {Attention Empowered Feature-level Radar-Camera Fusion for Object Detection},
  booktitle = {2022 Sensor Data Fusion: Trends, Solutions, Applications (SDF)},
  journal = {2022 Sensor Data Fusion: Trends, Solutions, Applications (SDF)},
  publisher = {IEEE},
  year = {2022},
  pages = {1--6},
  url = {https://ieeexplore.ieee.org/document/9931956}
}
</pre></td>
</tr>
<tr id="Li2020Dec" class="entry">
	<td>Li, L.-q. and Xie, Y.-l.</td>
	<td>A Feature Pyramid Fusion Detection Algorithm Based on Radar and Camera Sensor <p class="infolinks">[<a href="javascript:toggleInfo('Li2020Dec','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Li2020Dec','bibtex')">BibTeX</a>]</p></td>
	<td>2020</td>
	<td>2020 15th IEEE International Conference on Signal Processing (ICSP)<br/>Vol. 12020 15th IEEE International Conference on Signal Processing (ICSP), pp. 366-370&nbsp;</td>
	<td>incollection</td>
	<td><a href="https://ieeexplore.ieee.org/document/9320985">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Li2020Dec" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Considering the development of object detection based on deep learning framework in recent years, it has brought a new scope for multi-source fusion in the field of autonomous driving. In this paper, we propose a new architecture with a feature pyramid attention module to fuse the projected radar data and camera data. In the proposed algorithm, the detection model of YOLOv3 is employed by us and the feature pyramid module is extended with the input interface of the radar projection image and attention module. Additionally, in order to reduce the interference information from different scales of radar projected block, a new generation mechanism of radar projection images is introduced. Finally, the radar projection image is fused in feature pyramid layers with an attention module. The result shows that the proposed fusion algorithm outperforms better than image-only network for the nuScenes dataset. The code for this research will be made available to the public at: https://github.com/yuanliangxie/nuscenes_data_process.</td>
</tr>
<tr id="bib_Li2020Dec" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@incollection{Li2020Dec,
  author = {Li, Liang-qun and Xie, Yuan-liang},
  title = {A Feature Pyramid Fusion Detection Algorithm Based on Radar and Camera Sensor},
  booktitle = {2020 15th IEEE International Conference on Signal Processing (ICSP)},
  journal = {2020 15th IEEE International Conference on Signal Processing (ICSP)},
  publisher = {IEEE},
  year = {2020},
  volume = {1},
  pages = {366--370},
  url = {https://ieeexplore.ieee.org/document/9320985}
}
</pre></td>
</tr>
<tr id="Nabati2020Sep" class="entry">
	<td>Nabati, R. and Qi, H.</td>
	<td>Radar-Camera Sensor Fusion for Joint Object Detection and Distance Estimation in Autonomous Vehicles <p class="infolinks">[<a href="javascript:toggleInfo('Nabati2020Sep','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Nabati2020Sep','bibtex')">BibTeX</a>]</p></td>
	<td>2020</td>
	<td>arXiv&nbsp;</td>
	<td>article</td>
	<td><a href="https://arxiv.org/abs/2009.08428v1">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Nabati2020Sep" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: In this paper we present a novel radar-camera sensor fusion framework for accurate object detection and distance estimation in autonomous driving scenarios. The proposed architecture uses a middle-fusion approach to fuse the radar point clouds and RGB images. Our radar object proposal network uses radar point clouds to generate 3D proposals from a set of 3D prior boxes. These proposals are mapped to the image and fed into a Radar Proposal Refinement (RPR) network for objectness score prediction and box refinement. The RPR network utilizes both radar information and image feature maps to generate accurate object proposals and distance estimations. The radar-based proposals are combined with image-based proposals generated by a modified Region Proposal Network (RPN). The RPN has a distance regression layer for estimating distance for every generated proposal. The radar-based and image-based proposals are merged and used in the next stage for object classification. Experiments on the challenging nuScenes dataset show our method outperforms other existing radar-camera fusion methods in the 2D object detection task while at the same time accurately estimates objects' distances.</td>
</tr>
<tr id="bib_Nabati2020Sep" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Nabati2020Sep,
  author = {Nabati, Ramin and Qi, Hairong},
  title = {Radar-Camera Sensor Fusion for Joint Object Detection and Distance Estimation in Autonomous Vehicles},
  journal = {arXiv},
  year = {2020},
  url = {https://arxiv.org/abs/2009.08428v1}
}
</pre></td>
</tr>
<tr id="Sengupta2022Jan" class="entry">
	<td>Sengupta, A., Yoshizawa, A. and Cao, S.</td>
	<td>Automatic Radar-Camera Dataset Generation for Sensor-Fusion Applications <p class="infolinks">[<a href="javascript:toggleInfo('Sengupta2022Jan','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Sengupta2022Jan','bibtex')">BibTeX</a>]</p></td>
	<td>2022</td>
	<td>IEEE Rob. Autom. Lett.<br/>Vol. 7(2), pp. 2875-2882&nbsp;</td>
	<td>article</td>
	<td><a href="https://ieeexplore.ieee.org/document/9690006">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Sengupta2022Jan" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Withheterogeneous sensors offering complementary advantages in perception, there has been a significant growth in sensor-fusion based research and development in object perception and tracking using classical or deep neural networks based approaches. However, supervised learning requires massive labeled data-sets, that require expensive manual labor to generate. This paper presents a novel approach that leverages YOLOv3 based highly accurate object detection from camera to automatically label point cloud data obtained from a co-calibrated radar sensor to generate labeled radar-image and radar-only data-sets to aid learning algorithms for different applications. To achieve this we first co-calibrate the vision and radar sensors and obtain a radar-to-camera transformation matrix. The collected radar returns are segregated by different targets using a density based clustering scheme and the cluster centroids are projected onto the camera image using the transformation matrix. The Hungarian Algorithm is then used to associate the radar cluster centroids with the YOLOv3 generated bounding box centroids, and are labeled with the predicted class. The proposed approach is efficient, easy to implement and aims to encourage rapid development of multi-sensor data-sets, which are extremely limited currently, compared to the optical counterparts. The calibration process, software pipeline and the data-set generation is described in detail. Furthermore preliminary results from two sample applications for object detection using the data-sets are also presented.</td>
</tr>
<tr id="bib_Sengupta2022Jan" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Sengupta2022Jan,
  author = {Sengupta, Arindam and Yoshizawa, Atsushi and Cao, Siyang},
  title = {Automatic Radar-Camera Dataset Generation for Sensor-Fusion Applications},
  journal = {IEEE Rob. Autom. Lett.},
  publisher = {IEEE},
  year = {2022},
  volume = {7},
  number = {2},
  pages = {2875--2882},
  url = {https://ieeexplore.ieee.org/document/9690006}
}
</pre></td>
</tr>
<tr id="Ghiasi2022Dec" class="entry">
	<td>Ghiasi, A., Kazemi, H., Borgnia, E., Reich, S., Shu, M., Goldblum, M., Wilson, A.G. and Goldstein, T.</td>
	<td>What do Vision Transformers Learn? A Visual Exploration <p class="infolinks">[<a href="javascript:toggleInfo('Ghiasi2022Dec','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Ghiasi2022Dec','bibtex')">BibTeX</a>]</p></td>
	<td>2022</td>
	<td>arXiv&nbsp;</td>
	<td>article</td>
	<td><a href="https://arxiv.org/abs/2212.06727v1">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Ghiasi2022Dec" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Vision transformers (ViTs) are quickly becoming the de-facto architecture for computer vision, yet we understand very little about why they work and what they learn. While existing studies visually analyze the mechanisms of convolutional neural networks, an analogous exploration of ViTs remains challenging. In this paper, we first address the obstacles to performing visualizations on ViTs. Assisted by these solutions, we observe that neurons in ViTs trained with language model supervision (e.g., CLIP) are activated by semantic concepts rather than visual features. We also explore the underlying differences between ViTs and CNNs, and we find that transformers detect image background features, just like their convolutional counterparts, but their predictions depend far less on high-frequency information. On the other hand, both architecture types behave similarly in the way features progress from abstract patterns in early layers to concrete objects in late layers. In addition, we show that ViTs maintain spatial information in all layers except the final layer. In contrast to previous works, we show that the last layer most likely discards the spatial information and behaves as a learned global pooling operation. Finally, we conduct large-scale visualizations on a wide range of ViT variants, including DeiT, CoaT, ConViT, PiT, Swin, and Twin, to validate the effectiveness of our method.</td>
</tr>
<tr id="bib_Ghiasi2022Dec" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Ghiasi2022Dec,
  author = {Ghiasi, Amin and Kazemi, Hamid and Borgnia, Eitan and Reich, Steven and Shu, Manli and Goldblum, Micah and Wilson, Andrew Gordon and Goldstein, Tom},
  title = {What do Vision Transformers Learn? A Visual Exploration},
  journal = {arXiv},
  year = {2022},
  url = {https://arxiv.org/abs/2212.06727v1}
}
</pre></td>
</tr>
<tr id="Mafukidze2022Oct" class="entry">
	<td>Mafukidze, H.D., Mishra, A.K., Pidanic, J. and Francois, S.W.P.</td>
	<td>Scattering Centers to Point Clouds: A Review of mmWave Radars for Non-Radar-Engineers <p class="infolinks">[<a href="javascript:toggleInfo('Mafukidze2022Oct','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Mafukidze2022Oct','bibtex')">BibTeX</a>]</p></td>
	<td>2022</td>
	<td>IEEE Access<br/>Vol. 10, pp. 110992-111021&nbsp;</td>
	<td>article</td>
	<td><a href="https://ieeexplore.ieee.org/document/9908570">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Mafukidze2022Oct" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Recently, mmWave radars have been gaining popularity, thanks to their low cost, ease of use and high-resolution sensing. In this paper, we provide a review of the mmWave radar data processing frameworks, starting from mathematical foundations to applications. Specifically, we focus on the mmWave radar point cloud as a robust data structure representing compressed signatures for target recognition and classification. We first focus on the generation of the radar point clouds, and the signal processing algorithms designed for their unique characteristics. Then, we illustrate how the radar point clouds are prepared for feature extraction and classification using machine learning and deep learning approaches. Finally, we summarize the state-of-the-art applications, open datasets, developments and future research directions in this field.</td>
</tr>
<tr id="bib_Mafukidze2022Oct" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Mafukidze2022Oct,
  author = {Mafukidze, Harry D. and Mishra, Amit K. and Pidanic, Jan and Francois, Schonken W. P.},
  title = {Scattering Centers to Point Clouds: A Review of mmWave Radars for Non-Radar-Engineers},
  journal = {IEEE Access},
  publisher = {IEEE},
  year = {2022},
  volume = {10},
  pages = {110992--111021},
  url = {https://ieeexplore.ieee.org/document/9908570}
}
</pre></td>
</tr>
<tr id="Zang2019Mar" class="entry">
	<td>Zang, S., Ding, M., Smith, D., Tyler, P., Rakotoarivelo, T. and Kaafar, M.A.</td>
	<td>The Impact of Adverse Weather Conditions on Autonomous Vehicles: How Rain, Snow, Fog, and Hail Affect the Performance of a Self-Driving Car <p class="infolinks">[<a href="javascript:toggleInfo('Zang2019Mar','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Zang2019Mar','bibtex')">BibTeX</a>]</p></td>
	<td>2019</td>
	<td>IEEE Veh. Technol. Mag.<br/>Vol. 14(2), pp. 103-111&nbsp;</td>
	<td>article</td>
	<td><a href="https://ieeexplore.ieee.org/document/8666747">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Zang2019Mar" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Recently, the development of autonomous vehicles and intelligent driver assistance systems has drawn a significant amount of attention from the general public. One of the most critical issues in the development of autonomous vehicles and driver assistance systems is their poor performance under adverse weather conditions, such as rain, snow, fog, and hail. However, no current study provides a systematic and unified review of the effect that weather has on the various types of sensors used in autonomous vehicles. In this article, we first present a literature review about the impact of adverse weather conditions on state-ofthe-art sensors, such as lidar, GPS, camera, and radar. Then, we characterize the effect of rainfall on millimeter-wave (mmwave) radar, which considers both the rain attenuation and the backscatter effects. Our simulation results show that the detection range of mm-wave radar can be reduced by up to 45% under severe rainfall conditions. Moreover, the rain backscatter effect is significantly different for targets with different radar cross-section (RCS) areas.</td>
</tr>
<tr id="bib_Zang2019Mar" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Zang2019Mar,
  author = {Zang, Shizhe and Ding, Ming and Smith, David and Tyler, Paul and Rakotoarivelo, Thierry and Kaafar, Mohamed Ali},
  title = {The Impact of Adverse Weather Conditions on Autonomous Vehicles: How Rain, Snow, Fog, and Hail Affect the Performance of a Self-Driving Car},
  journal = {IEEE Veh. Technol. Mag.},
  publisher = {IEEE},
  year = {2019},
  volume = {14},
  number = {2},
  pages = {103--111},
  url = {https://ieeexplore.ieee.org/document/8666747}
}
</pre></td>
</tr>
<tr id="Park2020Aug" class="entry">
	<td>Park, J.-I., Park, J. and Kim, K.-S.</td>
	<td>Fast and Accurate Desnowing Algorithm for LiDAR Point Clouds <p class="infolinks">[<a href="javascript:toggleInfo('Park2020Aug','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Park2020Aug','bibtex')">BibTeX</a>]</p></td>
	<td>2020</td>
	<td>IEEE Access<br/>Vol. 8, pp. 160202-160212&nbsp;</td>
	<td>article</td>
	<td><a href="https://ieeexplore.ieee.org/document/9180326">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Park2020Aug" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: LiDAR sensors have the advantage of being able to generate high-resolution imaging quickly during both day and night; however, their performance is severely limited in adverse weather conditions such as snow, rain, and dense fog. Consequently, many researchers are actively working to overcome these limitations by applying sensor fusion with radar and optical cameras to LiDAR. While studies on the denoising of point clouds acquired by LiDAR in adverse weather have been conducted recently, the results are still insufficient for application to autonomous vehicles because of speed and accuracy performance limitations. Therefore, we propose a new intensity-based filter that differs from the existing distance-based filter, which limits the speed. The proposed method showed overwhelming performance advantages in terms of both speed and accuracy by removing only snow particles while leaving important environmental features. The intensity criteria for snow removal were derived based on an analysis of the properties of laser light and snow particles.</td>
</tr>
<tr id="bib_Park2020Aug" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Park2020Aug,
  author = {Park, Ji-Il and Park, Jihyuk and Kim, Kyung-Soo},
  title = {Fast and Accurate Desnowing Algorithm for LiDAR Point Clouds},
  journal = {IEEE Access},
  publisher = {IEEE},
  year = {2020},
  volume = {8},
  pages = {160202--160212},
  url = {https://ieeexplore.ieee.org/document/9180326}
}
</pre></td>
</tr>
<tr id="Lin2021Mar" class="entry">
	<td>Lin, S.-L. and Wu, B.-H.</td>
	<td>Application of Kalman Filter to Improve 3D LiDAR Signals of Autonomous Vehicles in Adverse Weather <p class="infolinks">[<a href="javascript:toggleInfo('Lin2021Mar','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Lin2021Mar','bibtex')">BibTeX</a>]</p></td>
	<td>2021</td>
	<td>Appl. Sci.<br/>Vol. 11(7), pp. 3018&nbsp;</td>
	<td>article</td>
	<td><a href="https://www.mdpi.com/2076-3417/11/7/3018">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Lin2021Mar" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: A worldwide increase in the number of vehicles on the road has led to an increase in the frequency of serious traffic accidents, causing loss of life and property. Autonomous vehicles could be part of the solution, but their safe operation is dependent on the onboard LiDAR (light detection and ranging) systems used for the detection of the environment outside the vehicle. Unfortunately, problems with the application of LiDAR in autonomous vehicles remain, for example, the weakening of the echo detection capability in adverse weather conditions. The signal is also affected, even drowned out, by sensory noise outside the vehicles, and the problem can become so severe that the autonomous vehicle cannot move. Clearly, the accuracy of the stereo images sensed by the LiDAR must be improved. In this study, we developed a method to improve the acquisition of LiDAR data in adverse weather by using a combination of a Kalman filter and nearby point cloud denoising. The overall LiDAR framework was tested in experiments in a space 2 m in length and width and 0.6 m high. Normal weather and three kinds of adverse weather conditions (rain, thick smoke, and rain and thick smoke) were simulated. The results show that this system can be used to recover normal weather data from data measured by LiDAR even in adverse weather conditions. The results showed an effective improvement of 10% to 30% in the LiDAR stereo images. This method can be developed and widely applied in the future. Keywords: autopilot; LiDAR of autonomous vehicles; Kalman filter</td>
</tr>
<tr id="bib_Lin2021Mar" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Lin2021Mar,
  author = {Lin, Shih-Lin and Wu, Bing-Han},
  title = {Application of Kalman Filter to Improve 3D LiDAR Signals of Autonomous Vehicles in Adverse Weather},
  journal = {Appl. Sci.},
  publisher = {Multidisciplinary Digital Publishing Institute},
  year = {2021},
  volume = {11},
  number = {7},
  pages = {3018},
  url = {https://www.mdpi.com/2076-3417/11/7/3018}
}
</pre></td>
</tr>
<tr id="Wang2022Mar" class="entry">
	<td>Wang, W., You, X., Chen, L., Tian, J., Tang, F. and Zhang, L.</td>
	<td>A Scalable and Accurate De-Snowing Algorithm for LiDAR Point Clouds in Winter <p class="infolinks">[<a href="javascript:toggleInfo('Wang2022Mar','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Wang2022Mar','bibtex')">BibTeX</a>]</p></td>
	<td>2022</td>
	<td>Remote Sens.<br/>Vol. 14(6), pp. 1468&nbsp;</td>
	<td>article</td>
	<td><a href="https://www.mdpi.com/2072-4292/14/6/1468">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Wang2022Mar" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Accurate and efficient environmental awareness is a fundamental capability of autonomous driving technology and the real-time data collected by sensors offer autonomous vehicles an intuitive impression of their environment. Unfortunately, the ambient noise caused by varying weather conditions immediately affects the ability of autonomous vehicles to accurately understand their environment and its expected impact. In recent years, researchers have improved the environmental perception capabilities of simultaneous localization and mapping (SLAM), object detection and tracking, semantic segmentation and panoptic segmentation, but relatively few studies have focused on enhancing environmental perception capabilities in adverse weather conditions, such as rain, snow and fog. To enhance the environmental perception of autonomous vehicles in adverse weather, we developed a dynamic filtering method called Dynamic Distance&ndash;Intensity Outlier Removal (DDIOR), which integrates the distance and intensity of points based on the systematic and accurate analysis of LiDAR point cloud data characteristics in snowy weather. Experiments on the publicly available WADS dataset (Winter Adverse Driving dataSet) showed that our method can efficiently remove snow noise while fully preserving the detailed features of the environment. Keywords: autonomous driving; de-snowing algorithm; LiDAR point clouds; data processing</td>
</tr>
<tr id="bib_Wang2022Mar" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Wang2022Mar,
  author = {Wang, Weiqi and You, Xiong and Chen, Lingyu and Tian, Jiangpeng and Tang, Fen and Zhang, Lantian},
  title = {A Scalable and Accurate De-Snowing Algorithm for LiDAR Point Clouds in Winter},
  journal = {Remote Sens.},
  publisher = {Multidisciplinary Digital Publishing Institute},
  year = {2022},
  volume = {14},
  number = {6},
  pages = {1468},
  url = {https://www.mdpi.com/2072-4292/14/6/1468}
}
</pre></td>
</tr>
<tr id="Fursa2021Mar" class="entry">
	<td>Fursa, I., Fandi, E., Musat, V., Culley, J., Gil, E., Teeti, I., Bilous, L., Sluis, I.V., Rast, A. and Bradley, A.</td>
	<td>Worsening Perception: Real-time Degradation of Autonomous Vehicle Perception Performance for Simulation of Adverse Weather Conditions <p class="infolinks">[<a href="javascript:toggleInfo('Fursa2021Mar','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Fursa2021Mar','bibtex')">BibTeX</a>]</p></td>
	<td>2021</td>
	<td>arXiv&nbsp;</td>
	<td>article</td>
	<td><a href="https://arxiv.org/abs/2103.02760v4">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Fursa2021Mar" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Autonomous vehicles rely heavily upon their perception subsystems to see the environment in which they operate. Unfortunately, the effect of variable weather conditions presents a significant challenge to object detection algorithms, and thus it is imperative to test the vehicle extensively in all conditions which it may experience. However, development of robust autonomous vehicle subsystems requires repeatable, controlled testing - while real weather is unpredictable and cannot be scheduled. Real-world testing in adverse conditions is an expensive and time-consuming task, often requiring access to specialist facilities. Simulation is commonly relied upon as a substitute, with increasingly visually realistic representations of the real-world being developed. In the context of the complete autonomous vehicle control pipeline, subsystems downstream of perception need to be tested with accurate recreations of the perception system output, rather than focusing on subjective visual realism of the input - whether in simulation or the real world. This study develops the untapped potential of a lightweight weather augmentation method in an autonomous racing vehicle - focusing not on visual accuracy, but rather the effect upon perception subsystem performance in real time. With minimal adjustment, the prototype developed in this study can replicate the effects of water droplets on the camera lens, and fading light conditions. This approach introduces a latency of less than 8 ms using compute hardware well suited to being carried in the vehicle - rendering it ideal for real-time implementation that can be run during experiments in simulation, and augmented reality testing in the real world.</td>
</tr>
<tr id="bib_Fursa2021Mar" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Fursa2021Mar,
  author = {Fursa, Ivan and Fandi, Elias and Musat, Valentina and Culley, Jacob and Gil, Enric and Teeti, Izzeddin and Bilous, Louise and Sluis, Isaac Vander and Rast, Alexander and Bradley, Andrew},
  title = {Worsening Perception: Real-time Degradation of Autonomous Vehicle Perception Performance for Simulation of Adverse Weather Conditions},
  journal = {arXiv},
  year = {2021},
  url = {https://arxiv.org/abs/2103.02760v4}
}
</pre></td>
</tr>
<tr id="Wu2020Jun" class="entry">
	<td>Wu, J., Xu, H., Tian, Y., Pi, R. and Yue, R.</td>
	<td>Vehicle Detection under Adverse Weather from Roadside LiDAR Data <p class="infolinks">[<a href="javascript:toggleInfo('Wu2020Jun','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Wu2020Jun','bibtex')">BibTeX</a>]</p></td>
	<td>2020</td>
	<td>Sensors<br/>Vol. 20(12), pp. 3433&nbsp;</td>
	<td>article</td>
	<td><a href="https://www.mdpi.com/1424-8220/20/12/3433">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Wu2020Jun" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Roadside light detection and ranging (LiDAR) is an emerging traffic data collection device and has recently been deployed in different transportation areas. The current data processing algorithms for roadside LiDAR are usually developed assuming normal weather conditions. Adverse weather conditions, such as windy and snowy conditions, could be challenges for data processing. This paper examines the performance of the state-of-the-art data processing algorithms developed for roadside LiDAR under adverse weather and then composed an improved background filtering and object clustering method in order to process the roadside LiDAR data, which was proven to perform better under windy and snowy weather. The testing results showed that the accuracy of the background filtering and point clustering was greatly improved compared to the state-of-the-art methods. With this new approach, vehicles can be identified with relatively high accuracy under windy and snowy weather. Keywords: vehicle detection; adverse weather; roadside LiDAR; data processing</td>
</tr>
<tr id="bib_Wu2020Jun" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Wu2020Jun,
  author = {Wu, Jianqing and Xu, Hao and Tian, Yuan and Pi, Rendong and Yue, Rui},
  title = {Vehicle Detection under Adverse Weather from Roadside LiDAR Data},
  journal = {Sensors},
  publisher = {Multidisciplinary Digital Publishing Institute},
  year = {2020},
  volume = {20},
  number = {12},
  pages = {3433},
  url = {https://www.mdpi.com/1424-8220/20/12/3433}
}
</pre></td>
</tr>
<tr id="Charron2018May" class="entry">
	<td>Charron, N., Phillips, S. and Waslander, S.L.</td>
	<td>De-noising of Lidar Point Clouds Corrupted by Snowfall <p class="infolinks">[<a href="javascript:toggleInfo('Charron2018May','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Charron2018May','bibtex')">BibTeX</a>]</p></td>
	<td>2018</td>
	<td>2018 15th Conference on Computer and Robot Vision (CRV)2018 15th Conference on Computer and Robot Vision (CRV), pp. 254-261&nbsp;</td>
	<td>incollection</td>
	<td><a href="https://ieeexplore.ieee.org/document/8575761">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Charron2018May" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: A common problem in autonomous driving is designing a system that can operate in adverse weather conditions. Falling rain and snow tends to corrupt sensor measurements, particularly for lidar sensors. Surprisingly, very little research has been published on methods to de-noise point clouds which are collected by lidar in rainy or snowy weather conditions. In this paper, we present a method for removing snow noise by processing point clouds using a 3D outlier detection algorithm. Our method, the dynamic radius outlier removal filter, accounts for the variation in point cloud density with increasing distance from the sensor, with the goal of removing the noise caused by snow while retaining detail in environmental features (which is necessary for autonomous localization and navigation). The proposed method outperforms other noise-removal methods, including methods which operate on depth image representations of the lidar scans. We show on point clouds obtained while driving in falling snow that we can simultaneously obtain > 90% precision and recall, indicating that the proposed method is effective at removing snow, without removing environmental features.</td>
</tr>
<tr id="bib_Charron2018May" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@incollection{Charron2018May,
  author = {Charron, Nicholas and Phillips, Stephen and Waslander, Steven L.},
  title = {De-noising of Lidar Point Clouds Corrupted by Snowfall},
  booktitle = {2018 15th Conference on Computer and Robot Vision (CRV)},
  journal = {2018 15th Conference on Computer and Robot Vision (CRV)},
  publisher = {IEEE},
  year = {2018},
  pages = {254--261},
  url = {https://ieeexplore.ieee.org/document/8575761}
}
</pre></td>
</tr>
<tr id="Gao2020Nov" class="entry">
	<td>Gao, X., Xing, G., Roy, S. and Liu, H.</td>
	<td>RAMP-CNN: A Novel Neural Network for Enhanced Automotive Radar Object Recognition <p class="infolinks">[<a href="javascript:toggleInfo('Gao2020Nov','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Gao2020Nov','bibtex')">BibTeX</a>]</p></td>
	<td>2020</td>
	<td>arXiv&nbsp;</td>
	<td>article</td>
	<td><a href="https://arxiv.org/abs/2011.08981v2">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Gao2020Nov" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Millimeter-wave radars are being increasingly integrated into commercial vehicles to support new advanced driver-assistance systems by enabling robust and high-performance object detection, localization, as well as recognition - a key component of new environmental perception. In this paper, we propose a novel radar multiple-perspectives convolutional neural network (RAMP-CNN) that extracts the location and class of objects based on further processing of the range-velocity-angle (RVA) heatmap sequences. To bypass the complexity of 4D convolutional neural networks (NN), we propose to combine several lower-dimension NN models within our RAMP-CNN model that nonetheless approaches the performance upper-bound with lower complexity. The extensive experiments show that the proposed RAMP-CNN model achieves better average recall and average precision than prior works in all testing scenarios. Besides, the RAMP-CNN model is validated to work robustly under nighttime, which enables low-cost radars as a potential substitute for pure optical sensing under severe conditions.</td>
</tr>
<tr id="bib_Gao2020Nov" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Gao2020Nov,
  author = {Gao, Xiangyu and Xing, Guanbin and Roy, Sumit and Liu, Hui},
  title = {RAMP-CNN: A Novel Neural Network for Enhanced Automotive Radar Object Recognition},
  journal = {arXiv},
  year = {2020},
  url = {https://arxiv.org/abs/2011.08981v2}
}
</pre></td>
</tr>
<tr id="radiant-radar-image-association-network-for-3d-object-detection" class="entry">
	<td>Long, Y., Kumar, A., Morris, D., Liu, X., Castro, M.P.G. and Chakravarty, P.</td>
	<td>RADIANT: RADar Image Association NeTwork for 3D Object Detection <p class="infolinks">[<a href="javascript:toggleInfo('radiant-radar-image-association-network-for-3d-object-detection','bibtex')">BibTeX</a>]</p></td>
	<td>2023</td>
	<td>In Proceeding of Thirty-Seventh AAAI Conference on Artificial Intelligence (AAAI)&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="http://cvlab.cse.msu.edu/pdfs/Long_Kumar_Morris_Liu_Castro_Chakravarty_AAAI2023.pdf">URL</a>&nbsp;</td>
</tr>
<tr id="bib_radiant-radar-image-association-network-for-3d-object-detection" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{radiant-radar-image-association-network-for-3d-object-detection,
  author = {Yunfei Long and Abhinav Kumar and Daniel Morris and Xiaoming Liu and Marcos Paul Gerardo Castro and Punarjay Chakravarty},
  title = {RADIANT: RADar Image Association NeTwork for 3D Object Detection},
  booktitle = {In Proceeding of Thirty-Seventh AAAI Conference on Artificial Intelligence (AAAI)},
  year = {2023},
  url = {http://cvlab.cse.msu.edu/pdfs/Long_Kumar_Morris_Liu_Castro_Chakravarty_AAAI2023.pdf}
}
</pre></td>
</tr>
<tr id="Sahba2020Nov" class="entry">
	<td>Sahba, R., Sahba, A. and Sahba, F.</td>
	<td>Using a Combination of LiDAR, RADAR, and Image Data for 3D Object Detection in Autonomous Vehicles <p class="infolinks">[<a href="javascript:toggleInfo('Sahba2020Nov','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Sahba2020Nov','bibtex')">BibTeX</a>]</p></td>
	<td>2020</td>
	<td>2020 11th IEEE Annual Information Technology, Electronics and Mobile Communication Conference (IEMCON)2020 11th IEEE Annual Information Technology, Electronics and Mobile Communication Conference (IEMCON), pp. 0427-0431&nbsp;</td>
	<td>incollection</td>
	<td><a href="https://ieeexplore.ieee.org/document/9284930">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Sahba2020Nov" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: One of the topics that is highly regarded and researched in the field of artificial intelligence and machine learning is object detection. Its use is especially important in autonomous vehicles. The various methods used to detect objects are based on different types of data, including image, radar, and lidar. Using a point clouds is one of the new methods for 3D object detection proposed in some recent work. One of the recently presented efficient methods is PointPillars network. It is an encoder that can learn from data available in a point cloud and then organize it as a representation in vertical columns (pillars). This representation can be used for 3D object detection. in this work, we try to develop a high performance model for 3D object detection based on PointPillars network exploiting a combination of lidar, radar, and image data to be used for autonomous vehicles perception. We use lidar, radar, and image data in nuScenes dataset to predict 3D boxes for three classes of objects that are car, pedestrian, and bus. To measure and compare results, we use nuScenes detection score (NDS) that is a combined metric for detection task. Results show that increasing the number of lidar sweeps, and combining them with radar and image data, significantly improve the performance of the 3D object detector. We suggest a method to combine different types of input data (lidar, radar, image) using a weighting system that can be used as the input for the encoder.</td>
</tr>
<tr id="bib_Sahba2020Nov" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@incollection{Sahba2020Nov,
  author = {Sahba, Ramin and Sahba, Amin and Sahba, Farshid},
  title = {Using a Combination of LiDAR, RADAR, and Image Data for 3D Object Detection in Autonomous Vehicles},
  booktitle = {2020 11th IEEE Annual Information Technology, Electronics and Mobile Communication Conference (IEMCON)},
  journal = {2020 11th IEEE Annual Information Technology, Electronics and Mobile Communication Conference (IEMCON)},
  publisher = {IEEE},
  year = {2020},
  pages = {0427--0431},
  url = {https://ieeexplore.ieee.org/document/9284930}
}
</pre></td>
</tr>
<tr id="Wang2022Dec" class="entry">
	<td>Wang, L., Zhang, X., Li, J., Xv, B., Fu, R., Chen, H., Yang, L., Jin, D. and Zhao, L.</td>
	<td>Multi-Modal and Multi-Scale Fusion 3D Object Detection of 4D Radar and LiDAR for Autonomous Driving <p class="infolinks">[<a href="javascript:toggleInfo('Wang2022Dec','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Wang2022Dec','bibtex')">BibTeX</a>]</p></td>
	<td>2022</td>
	<td>IEEE Trans. Veh. Technol., pp. 1-15&nbsp;</td>
	<td>article</td>
	<td><a href="https://ieeexplore.ieee.org/abstract/document/9991894">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Wang2022Dec" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Multi-modal fusion plays a critical role in 3D object detection, overcoming the inherent limitations of single-sensor perception in autonomous driving. Most fusion methods require data from high-resolution cameras and LiDAR sensors, which are less robust and the detection accuracy drops drastically with the increase of range as the point cloud density decreases. Alternatively, the fusion of Radar and LiDAR alleviates these issues but is still a developing field, especially for 4D Radar with a more robust and broader detection range. Nevertheless, different data characteristics and noise distributions between two sensors hinder performance improvement when directly integrating them. Therefore, we are the first to propose a novel fusion method termed</td>
</tr>
<tr id="bib_Wang2022Dec" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Wang2022Dec,
  author = {Wang, Li and Zhang, Xinyu and Li, Jun and Xv, Baowei and Fu, Rong and Chen, Haifeng and Yang, Lei and Jin, Dafeng and Zhao, Lijun},
  title = {Multi-Modal and Multi-Scale Fusion 3D Object Detection of 4D Radar and LiDAR for Autonomous Driving},
  journal = {IEEE Trans. Veh. Technol.},
  publisher = {IEEE},
  year = {2022},
  pages = {1--15},
  url = {https://ieeexplore.ieee.org/abstract/document/9991894}
}
</pre></td>
</tr>
<tr id="Wang2021Feb" class="entry">
	<td>Wang, Y., Jiang, Z., Li, Y., Hwang, J.-N., Xing, G. and Liu, H.</td>
	<td>RODNet: A Real-Time Radar Object Detection Network Cross-Supervised by Camera-Radar Fused Object 3D Localization <p class="infolinks">[<a href="javascript:toggleInfo('Wang2021Feb','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Wang2021Feb','bibtex')">BibTeX</a>]</p></td>
	<td>2021</td>
	<td>IEEE J. Sel. Top. Signal Process.<br/>Vol. 15(4), pp. 954-967&nbsp;</td>
	<td>article</td>
	<td><a href="https://ieeexplore.ieee.org/abstract/document/9353210">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Wang2021Feb" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Various autonomous or assisted driving strategies have been facilitated through the accurate and reliable perception of the environment around a vehicle. Among the commonly used sensors, radar has usually been considered as a robust and cost-effective solution even in adverse driving scenarios, e.g., weak/strong lighting or bad weather. Instead of considering fusing the unreliable information from all available sensors, perception from pure radar data becomes a valuable alternative that is worth exploring. In this paper, we propose a deep radar object detection network, named RODNet, which is cross-supervised by a camera-radar fused algorithm without laborious annotation efforts, to effectively detect objects from the radio frequency (RF) images in real-time. First, the raw signals captured by millimeter-wave radars are transformed to RF images in range-azimuth coordinates. Second, our proposed RODNet takes a snippet of RF images as the input to predict the likelihood of objects in the radar field of view (FoV). Two customized modules are also added to handle multi-chirp information and object relative motion. The proposed RODNet is cross-supervised by a novel 3D localization of detected objects using a camera-radar fusion (CRF) strategy in the training stage. Due to no existing public dataset available for our task, we create a new dataset, named CRUW, 1 1</td>
</tr>
<tr id="bib_Wang2021Feb" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Wang2021Feb,
  author = {Wang, Yizhou and Jiang, Zhongyu and Li, Yudong and Hwang, Jenq-Neng and Xing, Guanbin and Liu, Hui},
  title = {RODNet: A Real-Time Radar Object Detection Network Cross-Supervised by Camera-Radar Fused Object 3D Localization},
  journal = {IEEE J. Sel. Top. Signal Process.},
  publisher = {IEEE},
  year = {2021},
  volume = {15},
  number = {4},
  pages = {954--967},
  url = {https://ieeexplore.ieee.org/abstract/document/9353210}
}
</pre></td>
</tr>
<tr id="Valanarasu2021Nov" class="entry">
	<td>Valanarasu, J.M.J., Yasarla, R. and Patel, V.M.</td>
	<td>TransWeather: Transformer-based Restoration of Images Degraded by Adverse Weather Conditions <p class="infolinks">[<a href="javascript:toggleInfo('Valanarasu2021Nov','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Valanarasu2021Nov','bibtex')">BibTeX</a>]</p></td>
	<td>2021</td>
	<td>arXiv&nbsp;</td>
	<td>article</td>
	<td><a href="https://arxiv.org/abs/2111.14813v2">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Valanarasu2021Nov" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Removing adverse weather conditions like rain, fog, and snow from images is an important problem in many applications. Most methods proposed in the literature have been designed to deal with just removing one type of degradation. Recently, a CNN-based method using neural architecture search (All-in-One) was proposed to remove all the weather conditions at once. However, it has a large number of parameters as it uses multiple encoders to cater to each weather removal task and still has scope for improvement in its performance. In this work, we focus on developing an efficient solution for the all adverse weather removal problem. To this end, we propose TransWeather, a transformer-based end-to-end model with just a single encoder and a decoder that can restore an image degraded by any weather condition. Specifically, we utilize a novel transformer encoder using intra-patch transformer blocks to enhance attention inside the patches to effectively remove smaller weather degradations. We also introduce a transformer decoder with learnable weather type embeddings to adjust to the weather degradation at hand. TransWeather achieves improvements across multiple test datasets over both All-in-One network as well as methods fine-tuned for specific tasks. TransWeather is also validated on real world test images and found to be more effective than previous methods. Implementation code can be accessed at this https URL .</td>
</tr>
<tr id="bib_Valanarasu2021Nov" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Valanarasu2021Nov,
  author = {Valanarasu, Jeya Maria Jose and Yasarla, Rajeev and Patel, Vishal M.},
  title = {TransWeather: Transformer-based Restoration of Images Degraded by Adverse Weather Conditions},
  journal = {arXiv},
  year = {2021},
  url = {https://arxiv.org/abs/2111.14813v2}
}
</pre></td>
</tr>
<tr id="Yoneda2019Dec" class="entry">
	<td>Yoneda, K., Suganuma, N., Yanase, R. and Aldibaja, M.</td>
	<td>Automated driving recognition technologies for adverse weather conditions <p class="infolinks">[<a href="javascript:toggleInfo('Yoneda2019Dec','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Yoneda2019Dec','bibtex')">BibTeX</a>]</p></td>
	<td>2019</td>
	<td>IATSS Research<br/>Vol. 43(4), pp. 253-262&nbsp;</td>
	<td>article</td>
	<td><a href="https://www.sciencedirect.com/science/article/pii/S0386111219301463">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Yoneda2019Dec" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: During automated driving in urban areas, decisions must be made while recognizing the surrounding environment using sensors such as camera, Light Detection and Ranging (LiDAR), millimeter-wave radar (MWR), and the global navigation satellite system (GNSS). The ability to drive under various environmental conditions is an important issue for automated driving on any road. In order to introduce the automated vehicles into the markets, the ability to evaluate various traffic conditions and navigate safely presents serious challenges. Another important challenge is the development of a robust recognition system can account for adverse weather conditions. Sun glare, rain, fog, and snow are adverse weather conditions that can occur in the driving environment. This paper summarizes research focused on automated driving technologies and discuss challenges to identifying adverse weather and other situations that make driving difficult, thus complicating the introduction of automated vehicles to the market.</td>
</tr>
<tr id="bib_Yoneda2019Dec" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Yoneda2019Dec,
  author = {Yoneda, Keisuke and Suganuma, Naoki and Yanase, Ryo and Aldibaja, Mohammad},
  title = {Automated driving recognition technologies for adverse weather conditions},
  journal = {IATSS Research},
  publisher = {Elsevier},
  year = {2019},
  volume = {43},
  number = {4},
  pages = {253--262},
  url = {https://www.sciencedirect.com/science/article/pii/S0386111219301463}
}
</pre></td>
</tr>
<tr id="Burnett2022Mar" class="entry">
	<td>Burnett, K., Yoon, D.J., Wu, Y., Li, A.Z., Zhang, H., Lu, S., Qian, J., Tseng, W.-K., Lambert, A., Leung, K.Y.K., Schoellig, A.P. and Barfoot, T.D.</td>
	<td>Boreas: A Multi-Season Autonomous Driving Dataset <p class="infolinks">[<a href="javascript:toggleInfo('Burnett2022Mar','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Burnett2022Mar','bibtex')">BibTeX</a>]</p></td>
	<td>2022</td>
	<td>arXiv&nbsp;</td>
	<td>article</td>
	<td><a href="https://arxiv.org/abs/2203.10168v1">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Burnett2022Mar" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: The Boreas dataset was collected by driving a repeated route over the course of one year, resulting in stark seasonal variations and adverse weather conditions such as rain and falling snow. In total, the Boreas dataset contains over 350km of driving data featuring a 128-channel Velodyne Alpha-Prime lidar, a 360 degree Navtech CIR304-H scanning radar, a 5MP FLIR Blackfly S camera, and centimetre-accurate post-processed ground truth poses. At launch, our dataset will support live leaderboards for odometry, metric localization, and 3D object detection. The dataset and development kit are available at: this https URL</td>
</tr>
<tr id="bib_Burnett2022Mar" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Burnett2022Mar,
  author = {Burnett, Keenan and Yoon, David J. and Wu, Yuchen and Li, Andrew Zou and Zhang, Haowei and Lu, Shichen and Qian, Jingxing and Tseng, Wei-Kang and Lambert, Andrew and Leung, Keith Y. K. and Schoellig, Angela P. and Barfoot, Timothy D.},
  title = {Boreas: A Multi-Season Autonomous Driving Dataset},
  journal = {arXiv},
  year = {2022},
  url = {https://arxiv.org/abs/2203.10168v1}
}
</pre></td>
</tr>
<tr id="Bai2021Jun" class="entry">
	<td>Bai, J., Zheng, L., Li, S., Tan, B., Chen, S. and Huang, L.</td>
	<td>Radar Transformer: An Object Classification Network Based on 4D MMW Imaging Radar <p class="infolinks">[<a href="javascript:toggleInfo('Bai2021Jun','bibtex')">BibTeX</a>]</p></td>
	<td>2021</td>
	<td>Sensors<br/>Vol. 21(11), pp. 3854&nbsp;</td>
	<td>article</td>
	<td><a href="https://doi.org/10.3390/s21113854">DOI</a> &nbsp;</td>
</tr>
<tr id="bib_Bai2021Jun" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Bai2021Jun,
  author = {Bai, Jie and Zheng, Lianqing and Li, Sen and Tan, Bin and Chen, Sihan and Huang, Libo},
  title = {Radar Transformer: An Object Classification Network Based on 4D MMW Imaging Radar},
  journal = {Sensors},
  publisher = {Multidisciplinary Digital Publishing Institute},
  year = {2021},
  volume = {21},
  number = {11},
  pages = {3854},
  doi = {https://doi.org/10.3390/s21113854}
}
</pre></td>
</tr>
<tr id="Deziel2021Feb" class="entry">
	<td>Difmmodeeelse&eacute;&#64257;ziel, J.-L., Merriaux, P., Tremblay, F., Lessard, D., Plourde, D., Stanguennec, J., Goulet, P. and Olivier, P.</td>
	<td>PixSet : An Opportunity for 3D Computer Vision to Go Beyond Point Clouds With a Full-Waveform LiDAR Dataset <p class="infolinks">[<a href="javascript:toggleInfo('Deziel2021Feb','bibtex')">BibTeX</a>]</p></td>
	<td>2021</td>
	<td>arXiv&nbsp;</td>
	<td>article</td>
	<td><a href="https://doi.org/10.48550/arXiv.2102.12010">DOI</a> &nbsp;</td>
</tr>
<tr id="bib_Deziel2021Feb" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Deziel2021Feb,
  author = {Difmmodeeelse&eacute;&#64257;ziel, Jean-Luc and Merriaux, Pierre and Tremblay, Francis and Lessard, Dave and Plourde, Dominique and Stanguennec, Julien and Goulet, Pierre and Olivier, Pierre},
  title = {PixSet : An Opportunity for 3D Computer Vision to Go Beyond Point Clouds With a Full-Waveform LiDAR Dataset},
  journal = {arXiv},
  year = {2021},
  doi = {https://doi.org/10.48550/arXiv.2102.12010}
}
</pre></td>
</tr>
<tr id="Li2022Apr" class="entry">
	<td>Li, P., Wang, P., Berntorp, K. and Liu, H.</td>
	<td>Exploiting Temporal Relations on Radar Perception for Autonomous Driving <p class="infolinks">[<a href="javascript:toggleInfo('Li2022Apr','bibtex')">BibTeX</a>]</p></td>
	<td>2022</td>
	<td>arXiv&nbsp;</td>
	<td>article</td>
	<td><a href="https://doi.org/10.48550/arXiv.2204.01184">DOI</a> &nbsp;</td>
</tr>
<tr id="bib_Li2022Apr" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Li2022Apr,
  author = {Li, Peizhao and Wang, Pu and Berntorp, Karl and Liu, Hongfu},
  title = {Exploiting Temporal Relations on Radar Perception for Autonomous Driving},
  journal = {arXiv},
  year = {2022},
  doi = {https://doi.org/10.48550/arXiv.2204.01184}
}
</pre></td>
</tr>
<tr id="Popov2022Sep" class="entry">
	<td>Popov, A., Gebhardt, P., Chen, K., Oldja, R., Lee, H., Murray, S., Bhargava, R. and Smolyanskiy, N.</td>
	<td>NVRadarNet: Real-Time Radar Obstacle and Free Space Detection for Autonomous Driving <p class="infolinks">[<a href="javascript:toggleInfo('Popov2022Sep','bibtex')">BibTeX</a>]</p></td>
	<td>2022</td>
	<td>arXiv&nbsp;</td>
	<td>article</td>
	<td><a href="https://doi.org/10.48550/arXiv.2209.14499">DOI</a> &nbsp;</td>
</tr>
<tr id="bib_Popov2022Sep" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Popov2022Sep,
  author = {Popov, Alexander and Gebhardt, Patrik and Chen, Ke and Oldja, Ryan and Lee, Heeseok and Murray, Shane and Bhargava, Ruchi and Smolyanskiy, Nikolai},
  title = {NVRadarNet: Real-Time Radar Obstacle and Free Space Detection for Autonomous Driving},
  journal = {arXiv},
  year = {2022},
  doi = {https://doi.org/10.48550/arXiv.2209.14499}
}
</pre></td>
</tr>
<tr id="Le2021Aug" class="entry">
	<td>Le, N., Rathour, V.S., Yamazaki, K., Luu, K. and Savvides, M.</td>
	<td>Deep Reinforcement Learning in Computer Vision: A Comprehensive Survey <p class="infolinks">[<a href="javascript:toggleInfo('Le2021Aug','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Le2021Aug','bibtex')">BibTeX</a>]</p></td>
	<td>2021</td>
	<td>arXiv&nbsp;</td>
	<td>article</td>
	<td><a href="https://arxiv.org/abs/2108.11510v1">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Le2021Aug" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Deep reinforcement learning augments the reinforcement learning framework and utilizes the powerful representation of deep neural networks. Recent works have demonstrated the remarkable successes of deep reinforcement learning in various domains including finance, medicine, healthcare, video games, robotics, and computer vision. In this work, we provide a detailed review of recent and state-of-the-art research advances of deep reinforcement learning in computer vision. We start with comprehending the theories of deep learning, reinforcement learning, and deep reinforcement learning. We then propose a categorization of deep reinforcement learning methodologies and discuss their advantages and limitations. In particular, we divide deep reinforcement learning into seven main categories according to their applications in computer vision, i.e. (i)landmark localization (ii) object detection; (iii) object tracking; (iv) registration on both 2D image and 3D image volumetric data (v) image segmentation; (vi) videos analysis; and (vii) other applications. Each of these categories is further analyzed with reinforcement learning techniques, network design, and performance. Moreover, we provide a comprehensive analysis of the existing publicly available datasets and examine source code availability. Finally, we present some open issues and discuss future research directions on deep reinforcement learning in computer vision</td>
</tr>
<tr id="bib_Le2021Aug" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Le2021Aug,
  author = {Le, Ngan and Rathour, Vidhiwar Singh and Yamazaki, Kashu and Luu, Khoa and Savvides, Marios},
  title = {Deep Reinforcement Learning in Computer Vision: A Comprehensive Survey},
  journal = {arXiv},
  year = {2021},
  url = {https://arxiv.org/abs/2108.11510v1}
}
</pre></td>
</tr>
<tr id="Zheng2022Apr" class="entry">
	<td>Zheng, L., Ma, Z., Zhu, X., Tan, B., Li, S., Long, K., Sun, W., Chen, S., Zhang, L., Wan, M., Huang, L. and Bai, J.</td>
	<td>TJ4DRadSet: A 4D Radar Dataset for Autonomous Driving <p class="infolinks">[<a href="javascript:toggleInfo('Zheng2022Apr','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Zheng2022Apr','bibtex')">BibTeX</a>]</p></td>
	<td>2022</td>
	<td>arXiv&nbsp;</td>
	<td>article</td>
	<td><a href="https://arxiv.org/abs/2204.13483v1">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Zheng2022Apr" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: The new generation of 4D high-resolution imaging radar provides not only a huge amount of point cloud but also additional elevation measurement, which has a great potential of 3D sensing in autonomous driving. In this paper, we introduce an autonomous driving dataset named TJ4DRadSet, including multi-modal sensors that are 4D radar, lidar, camera and GNSS, with about 40K frames in total. 7757 frames within 44 consecutive sequences in various driving scenarios are well annotated with 3D bounding boxes and track id. We provide a 4D radar-based 3D object detection baseline for our dataset to demonstrate the effectiveness of deep learning methods for 4D radar point clouds.</td>
</tr>
<tr id="bib_Zheng2022Apr" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Zheng2022Apr,
  author = {Zheng, Lianqing and Ma, Zhixiong and Zhu, Xichan and Tan, Bin and Li, Sen and Long, Kai and Sun, Weiqi and Chen, Sihan and Zhang, Lu and Wan, Mengyue and Huang, Libo and Bai, Jie},
  title = {TJ4DRadSet: A 4D Radar Dataset for Autonomous Driving},
  journal = {arXiv},
  year = {2022},
  url = {https://arxiv.org/abs/2204.13483v1}
}
</pre></td>
</tr>
<tr id="Sun2021May" class="entry">
	<td>Sun, S. and Zhang, Y.D.</td>
	<td>4D Automotive Radar Sensing for Autonomous Vehicles: A Sparsity-Oriented Approach <p class="infolinks">[<a href="javascript:toggleInfo('Sun2021May','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Sun2021May','bibtex')">BibTeX</a>]</p></td>
	<td>2021</td>
	<td>IEEE Journal of Selected Topics in Signal Processing<br/>Vol. 15(4), pp. 879-891&nbsp;</td>
	<td>article</td>
	<td><a href="https://doi.org/10.1109/JSTSP.2021.3079626">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Sun2021May" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: We propose a high-resolution imaging radar system to enable high-fidelity four-dimensional (4D) sensing for autonomous driving, i.e., range, Doppler, azimuth, and elevation, through a joint sparsity design in frequency spectrum and array configurations. To accommodate a high number of automotive radars operating at the same frequency band while avoiding mutual interference, random sparse step-frequency waveform (RSSFW) is proposed to synthesize a large effective bandwidth to achieve high range resolution profiles. To mitigate high range sidelobes in RSSFW radars, optimal weights are designed to minimize the peak sidelobe level such that targets with a relatively small radar cross section are detectable without introducing high probability of false alarm. We extend the RSSFW concept to multi-input multi-output (MIMO) radar by applying phase codes along slow time to synthesize a two-dimensional (2D) sparse array with hundreds of virtual array elements to enable high-resolution direction finding in both azimuth and elevation. The 2D sparse array acts as a sub-Nyquist sampler of the corresponding uniform rectangular array (URA) with half-wavelength interelement spacing, and the corresponding URA response is recovered by completing a low-rank block Hankel matrix. Consequently, the high sidelobes in the azimuth and elevation spectra are greatly suppressed so that weak targets can be reliably detected. The proposed imaging radar provides point clouds with a resolution comparable to LiDAR but with a much lower cost. Numerical simulations are conducted to demonstrate the performance of the proposed 4D imaging radar system with joint sparsity in frequency spectrum and antenna arrays.</td>
</tr>
<tr id="bib_Sun2021May" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Sun2021May,
  author = {Sun, Shunqiao and Zhang, Yimin D.},
  title = {4D Automotive Radar Sensing for Autonomous Vehicles: A Sparsity-Oriented Approach},
  journal = {IEEE Journal of Selected Topics in Signal Processing},
  publisher = {IEEE},
  year = {2021},
  volume = {15},
  number = {4},
  pages = {879--891},
  url = {https://doi.org/10.1109/JSTSP.2021.3079626}
}
</pre></td>
</tr>
<tr id="Chu2023" class="entry">
	<td>Chu, S.-Y. and Lee, M.-S.</td>
	<td>MT-DETR: Robust End-to-End Multimodal Detection With Confidence Fusion <p class="infolinks">[<a href="javascript:toggleInfo('Chu2023','bibtex')">BibTeX</a>]</p></td>
	<td>2023</td>
	<td>, pp. 5252-5261&nbsp;</td>
	<td>misc</td>
	<td><a href="https://openaccess.thecvf.com/content/WACV2023/html/Chu_MT-DETR_Robust_End-to-End_Multimodal_Detection_With_Confidence_Fusion_WACV_2023_paper.html">URL</a>&nbsp;</td>
</tr>
<tr id="bib_Chu2023" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@misc{Chu2023,
  author = {Chu, Shih-Yun and Lee, Ming-Sui},
  title = {MT-DETR: Robust End-to-End Multimodal Detection With Confidence Fusion},
  year = {2023},
  pages = {5252--5261},
  note = {[Online; accessed 2. Jan. 2023]},
  url = {https://openaccess.thecvf.com/content/WACV2023/html/Chu_MT-DETR_Robust_End-to-End_Multimodal_Detection_With_Confidence_Fusion_WACV_2023_paper.html}
}
</pre></td>
</tr>
<tr id="Wang2022Oct" class="entry">
	<td>Wang, L., Zhang, X., Xv, B., Zhang, J., Fu, R., Wang, X., Zhu, L., Ren, H., Lu, P., Li, J. and Liu, H.</td>
	<td>InterFusion: Interaction-based 4D Radar and LiDAR Fusion for 3D Object Detection <p class="infolinks">[<a href="javascript:toggleInfo('Wang2022Oct','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Wang2022Oct','bibtex')">BibTeX</a>]</p></td>
	<td>2022</td>
	<td>2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 12247-12253&nbsp;</td>
	<td>incollection</td>
	<td><a href="https://doi.org/10.1109/IROS47612.2022.9982123">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Wang2022Oct" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Many recent works detect 3D objects by several sensor modalities for autonomous driving, where high-resolution cameras and high-line LiDARs are mostly used but relatively expensive. To achieve a balance between overall cost and detection accuracy, many multi-modal fusion techniques have been suggested. In recent years, the fusion of LiDAR and Radar has gained ever-increasing attention, especially 4D Radar, which can adapt to bad weather conditions due to its penetrability. Although features have been fused from multiple sensing modalities, most methods cannot learn interactions from different modalities, which does not make for their best use. Inspired by the self-attention mechanism, we present InterFusion, an interaction-based fusion framework, to fuse 16-line LiDAR with 4D Radar. It aggregates features from two modalities and identifies cross-modal relations between Radar and LiDAR features. In experimental evaluations on the Astyx HiRes 2019 dataset, our method outperformed the baseline by 4.20% mAP in 3D and 10.76% BEV mAP for the car class at the moderate level.</td>
</tr>
<tr id="bib_Wang2022Oct" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@incollection{Wang2022Oct,
  author = {Wang, Li and Zhang, Xinyu and Xv, Baowei and Zhang, Jinzhao and Fu, Rong and Wang, Xiaoyu and Zhu, Lei and Ren, Haibing and Lu, Pingping and Li, Jun and Liu, Huaping},
  title = {InterFusion: Interaction-based 4D Radar and LiDAR Fusion for 3D Object Detection},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  journal = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  publisher = {IEEE},
  year = {2022},
  pages = {12247--12253},
  url = {https://doi.org/10.1109/IROS47612.2022.9982123}
}
</pre></td>
</tr>
<tr id="Barreto-Cubero2021Dec" class="entry">
	<td>Barreto-Cubero, A.J., Gifmmodeoelse&oacute;&#64257;mez-Espinosa, A., Escobedo Cabello, J.A., Cuan-Urquizo, E. and Cruz-Ramifmmode\imathelse\i&#64257;rez, S.R.</td>
	<td>Sensor Data Fusion for a Mobile Robot Using Neural Networks <p class="infolinks">[<a href="javascript:toggleInfo('Barreto-Cubero2021Dec','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Barreto-Cubero2021Dec','bibtex')">BibTeX</a>]</p></td>
	<td>2021</td>
	<td>Sensors<br/>Vol. 22(1), pp. 305&nbsp;</td>
	<td>article</td>
	<td><a href="https://doi.org/10.3390/s22010305">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Barreto-Cubero2021Dec" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Mobile robots must be capable to obtain an accurate map of their surroundings to move within it. To detect different materials that might be undetectable to one sensor but not others it is necessary to construct at least a two-sensor fusion scheme. With this, it is possible to generate a 2D occupancy map in which glass obstacles are identified. An artificial neural network is used to fuse data from a tri-sensor (RealSense Stereo camera, 2D  360 circ LiDAR, and Ultrasonic Sensors) setup capable of detecting glass and other materials typically found in indoor environments that may or may not be visible to traditional 2D LiDAR sensors, hence the expression improved LiDAR. A preprocessing scheme is implemented to filter all the outliers, project a 3D pointcloud to a 2D plane and adjust distance data. With a Neural Network as a data fusion algorithm, we integrate all the information into a single, more accurate distance-to-obstacle reading to finally generate a 2D Occupancy Grid Map (OGM) that considers all sensors information. The Robotis Turtlebot3 Waffle Pi robot is used as the experimental platform to conduct experiments given the different fusion strategies. Test results show that with such a fusion algorithm, it is possible to detect glass and other obstacles with an estimated root-mean-square error (RMSE) of 3 cm with multiple fusion strategies. Keywords: sensor data fusion; mobile robot; artificial neural network; improved LiDAR; occupancy grid map</td>
</tr>
<tr id="bib_Barreto-Cubero2021Dec" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Barreto-Cubero2021Dec,
  author = {Barreto-Cubero, Andres J. and Gifmmodeoelse&oacute;&#64257;mez-Espinosa, Alfonso and Escobedo Cabello, Jesifmmodeuelse&uacute;&#64257;s Arturo and Cuan-Urquizo, Enrique and Cruz-Ramifmmode\imathelse\i&#64257;rez, Sergio R.},
  title = {Sensor Data Fusion for a Mobile Robot Using Neural Networks},
  journal = {Sensors},
  publisher = {Multidisciplinary Digital Publishing Institute},
  year = {2021},
  volume = {22},
  number = {1},
  pages = {305},
  url = {https://doi.org/10.3390/s22010305}
}
</pre></td>
</tr>
<tr id="Rizzoli2022Jul" class="entry">
	<td>Rizzoli, G., Barbato, F. and Zanuttigh, P.</td>
	<td>Multimodal Semantic Segmentation in Autonomous Driving: A Review of Current Approaches and Future Perspectives <p class="infolinks">[<a href="javascript:toggleInfo('Rizzoli2022Jul','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Rizzoli2022Jul','bibtex')">BibTeX</a>]</p></td>
	<td>2022</td>
	<td>Technologies<br/>Vol. 10(4), pp. 90&nbsp;</td>
	<td>article</td>
	<td><a href="https://doi.org/10.3390/technologies10040090">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Rizzoli2022Jul" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: The perception of the surrounding environment is a key requirement for autonomous driving systems, yet the computation of an accurate semantic representation of the scene starting from RGB information alone is very challenging. In particular, the lack of geometric information and the strong dependence on weather and illumination conditions introduce critical challenges for approaches tackling this task. For this reason, most autonomous cars exploit a variety of sensors, including color, depth or thermal cameras, LiDARs, and RADARs. How to efficiently combine all these sources of information to compute an accurate semantic description of the scene is still an unsolved task, leading to an active research field. In this survey, we start by presenting the most commonly employed acquisition setups and datasets. Then we review several different deep learning architectures for multimodal semantic segmentation. We will discuss the various techniques to combine color, depth, LiDAR, and other modalities of data at different stages of the learning architectures, and we will show how smart fusion strategies allow us to improve performances with respect to the exploitation of a single source of information. Keywords: semantic segmentation; autonomous driving; multimodal; LiDAR; depth; modality fusion; deep learning</td>
</tr>
<tr id="bib_Rizzoli2022Jul" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Rizzoli2022Jul,
  author = {Rizzoli, Giulia and Barbato, Francesco and Zanuttigh, Pietro},
  title = {Multimodal Semantic Segmentation in Autonomous Driving: A Review of Current Approaches and Future Perspectives},
  journal = {Technologies},
  publisher = {Multidisciplinary Digital Publishing Institute},
  year = {2022},
  volume = {10},
  number = {4},
  pages = {90},
  url = {https://doi.org/10.3390/technologies10040090}
}
</pre></td>
</tr>
<tr id="Meyer2019Oct" class="entry">
	<td>Meyer, M. and Kuschk, G.</td>
	<td>Automotive Radar Dataset for Deep Learning Based 3D Object Detection <p class="infolinks">[<a href="javascript:toggleInfo('Meyer2019Oct','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Meyer2019Oct','comment')">Comment</a>] [<a href="javascript:toggleInfo('Meyer2019Oct','bibtex')">BibTeX</a>]</p></td>
	<td>2019</td>
	<td>2019 16th European Radar Conference (EuRAD)2019 16th European Radar Conference (EuRAD), pp. 129-132&nbsp;</td>
	<td>incollection</td>
	<td><a href="https://ieeexplore.ieee.org/document/8904734">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Meyer2019Oct" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: We present a radar-centric automotive dataset based on radar, lidar and camera data for the purpose of 3D object detection. Our main focus is to provide high resolution radar data to the research community, facilitating and stimulating research on algorithms using radar sensor data. To this end, semi-automatically generated and manually refined 3D ground truth data for object detection is provided. We describe the complete process of generating such a dataset, highlight some main features of the corresponding high-resolution radar and demonstrate its usage for level 3-5 autonomous driving applications by showing results of a deep learning based 3D object detection algorithm on this dataset. Our dataset will be available online at: www.astyx.net.</td>
</tr>
<tr id="rev_Meyer2019Oct" class="comment noshow">
	<td colspan="6"><b>Comment</b>: Astyx dataset</td>
</tr>
<tr id="bib_Meyer2019Oct" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@incollection{Meyer2019Oct,
  author = {Meyer, Michael and Kuschk, Georg},
  title = {Automotive Radar Dataset for Deep Learning Based 3D Object Detection},
  booktitle = {2019 16th European Radar Conference (EuRAD)},
  journal = {2019 16th European Radar Conference (EuRAD)},
  publisher = {IEEE},
  year = {2019},
  pages = {129--132},
  url = {https://ieeexplore.ieee.org/document/8904734}
}
</pre></td>
</tr>
<tr id="Drews2022Sep" class="entry">
	<td>Drews, F., Feng, D., Faion, F., Rosenbaum, L., Ulrich, M. and Glifmmodeaelse&auml;&#64257;ser, C.</td>
	<td>DeepFusion: A Robust and Modular 3D Object Detector for Lidars, Cameras and Radars <p class="infolinks">[<a href="javascript:toggleInfo('Drews2022Sep','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Drews2022Sep','bibtex')">BibTeX</a>]</p></td>
	<td>2022</td>
	<td>ArXiv e-prints&nbsp;</td>
	<td>article</td>
	<td><a href="https://doi.org/10.48550/arXiv.2209.12729">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Drews2022Sep" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: We propose DeepFusion, a modular multi-modal architecture to fuse lidars, cameras and radars in different combinations for 3D object detection. Specialized feature extractors take advantage of each modality and can be exchanged easily, making the approach simple and flexible. Extracted features are transformed into bird's-eye-view as a common representation for fusion. Spatial and semantic alignment is performed prior to fusing modalities in the feature space. Finally, a detection head exploits rich multi-modal features for improved 3D detection performance. Experimental results for lidar-camera, lidar-camera-radar and camera-radar fusion show the flexibility and effectiveness of our fusion approach. In the process, we study the largely unexplored task of faraway car detection up to 225textasciitildemeters, showing the benefits of our lidar-camera fusion. Furthermore, we investigate the required density of lidar points for 3D object detection and illustrate implications at the example of robustness against adverse weather conditions. Moreover, ablation studies on our camera-radar fusion highlight the importance of accurate depth estimation.</td>
</tr>
<tr id="bib_Drews2022Sep" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Drews2022Sep,
  author = {Drews, Florian and Feng, Di and Faion, Florian and Rosenbaum, Lars and Ulrich, Michael and Glifmmodeaelse&auml;&#64257;ser, Claudius},
  title = {DeepFusion: A Robust and Modular 3D Object Detector for Lidars, Cameras and Radars},
  journal = {ArXiv e-prints},
  year = {2022},
  url = {https://doi.org/10.48550/arXiv.2209.12729}
}
</pre></td>
</tr>
<tr id="Pfeuffer2018Jul" class="entry">
	<td>Pfeuffer, A. and Dietmayer, K.</td>
	<td>Optimal Sensor Data Fusion Architecture for Object Detection in Adverse Weather Conditions <p class="infolinks">[<a href="javascript:toggleInfo('Pfeuffer2018Jul','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Pfeuffer2018Jul','bibtex')">BibTeX</a>]</p></td>
	<td>2018</td>
	<td>ArXiv e-prints&nbsp;</td>
	<td>article</td>
	<td><a href="https://doi.org/10.48550/arXiv.1807.02323">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Pfeuffer2018Jul" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: A good and robust sensor data fusion in diverse weather conditions is a quite challenging task. There are several fusion architectures in the literature, e.g. the sensor data can be fused right at the beginning (Early Fusion), or they can be first processed separately and then concatenated later (Late Fusion). In this work, different fusion architectures are compared and evaluated by means of object detection tasks, in which the goal is to recognize and localize predefined objects in a stream of data. Usually, state-of-the-art object detectors based on neural networks are highly optimized for good weather conditions, since the well-known benchmarks only consist of sensor data recorded in optimal weather conditions. Therefore, the performance of these approaches decreases enormously or even fails in adverse weather conditions. In this work, different sensor fusion architectures are compared for good and adverse weather conditions for finding the optimal fusion architecture for diverse weather situations. A new training strategy is also introduced such that the performance of the object detector is greatly enhanced in adverse weather scenarios or if a sensor fails. Furthermore, the paper responds to the question if the detection accuracy can be increased further by providing the neural network with a-priori knowledge such as the spatial calibration of the sensors.</td>
</tr>
<tr id="bib_Pfeuffer2018Jul" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Pfeuffer2018Jul,
  author = {Pfeuffer, Andreas and Dietmayer, Klaus},
  title = {Optimal Sensor Data Fusion Architecture for Object Detection in Adverse Weather Conditions},
  journal = {ArXiv e-prints},
  year = {2018},
  url = {https://doi.org/10.48550/arXiv.1807.02323}
}
</pre></td>
</tr>
<tr id="Mees2017Jul" class="entry">
	<td>Mees, O., Eitel, A. and Burgard, W.</td>
	<td>Choosing Smartly: Adaptive Multimodal Fusion for Object Detection in Changing Environments <p class="infolinks">[<a href="javascript:toggleInfo('Mees2017Jul','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Mees2017Jul','bibtex')">BibTeX</a>]</p></td>
	<td>2017</td>
	<td>ArXiv e-prints&nbsp;</td>
	<td>article</td>
	<td><a href="https://doi.org/10.1109/IROS.2016.7759048">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Mees2017Jul" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Object detection is an essential task for autonomous robots operating in dynamic and changing environments. A robot should be able to detect objects in the presence of sensor noise that can be induced by changing lighting conditions for cameras and false depth readings for range sensors, especially RGB-D cameras. To tackle these challenges, we propose a novel adaptive fusion approach for object detection that learns weighting the predictions of different sensor modalities in an online manner. Our approach is based on a mixture of convolutional neural network (CNN) experts and incorporates multiple modalities including appearance, depth and motion. We test our method in extensive robot experiments, in which we detect people in a combined indoor and outdoor scenario from RGB-D data, and we demonstrate that our method can adapt to harsh lighting changes and severe camera motion blur. Furthermore, we present a new RGB-D dataset for people detection in mixed in- and outdoor environments, recorded with a mobile robot. Code, pretrained models and dataset are available at this http URL</td>
</tr>
<tr id="bib_Mees2017Jul" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Mees2017Jul,
  author = {Mees, Oier and Eitel, Andreas and Burgard, Wolfram},
  title = {Choosing Smartly: Adaptive Multimodal Fusion for Object Detection in Changing Environments},
  journal = {ArXiv e-prints},
  year = {2017},
  url = {https://doi.org/10.1109/IROS.2016.7759048}
}
</pre></td>
</tr>
<tr id="Casas2021Jan" class="entry">
	<td>Casas, S., Luo, W. and Urtasun, R.</td>
	<td>IntentNet: Learning to Predict Intention from Raw Sensor Data <p class="infolinks">[<a href="javascript:toggleInfo('Casas2021Jan','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Casas2021Jan','comment')">Comment</a>] [<a href="javascript:toggleInfo('Casas2021Jan','bibtex')">BibTeX</a>]</p></td>
	<td>2021</td>
	<td>ArXiv e-prints&nbsp;</td>
	<td>article</td>
	<td><a href="https://doi.org/10.48550/arXiv.2101.07907">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Casas2021Jan" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: In order to plan a safe maneuver, self-driving vehicles need to understand the intent of other traffic participants. We define intent as a combination of discrete high-level behaviors as well as continuous trajectories describing future motion. In this paper, we develop a one-stage detector and forecaster that exploits both 3D point clouds produced by a LiDAR sensor as well as dynamic maps of the environment. Our multi-task model achieves better accuracy than the respective separate modules while saving computation, which is critical to reducing reaction time in self-driving applications.</td>
</tr>
<tr id="rev_Casas2021Jan" class="comment noshow">
	<td colspan="6"><b>Comment</b>: incorporate temporal cues as mentioned in Bosch survey paper</td>
</tr>
<tr id="bib_Casas2021Jan" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Casas2021Jan,
  author = {Casas, Sergio and Luo, Wenjie and Urtasun, Raquel},
  title = {IntentNet: Learning to Predict Intention from Raw Sensor Data},
  journal = {ArXiv e-prints},
  year = {2021},
  url = {https://doi.org/10.48550/arXiv.2101.07907}
}
</pre></td>
</tr>
<tr id="Luo2020Dec" class="entry">
	<td>Luo, W., Yang, B. and Urtasun, R.</td>
	<td>Fast and Furious: Real Time End-to-End 3D Detection, Tracking and Motion Forecasting with a Single Convolutional Net <p class="infolinks">[<a href="javascript:toggleInfo('Luo2020Dec','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Luo2020Dec','comment')">Comment</a>] [<a href="javascript:toggleInfo('Luo2020Dec','bibtex')">BibTeX</a>]</p></td>
	<td>2020</td>
	<td>ArXiv e-prints&nbsp;</td>
	<td>article</td>
	<td><a href="https://doi.org/10.48550/arXiv.2012.12395">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Luo2020Dec" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: In this paper we propose a novel deep neural network that is able to jointly reason about 3D detection, tracking and motion forecasting given data captured by a 3D sensor. By jointly reasoning about these tasks, our holistic approach is more robust to occlusion as well as sparse data at range. Our approach performs 3D convolutions across space and time over a bird's eye view representation of the 3D world, which is very efficient in terms of both memory and computation. Our experiments on a new very large scale dataset captured in several north american cities, show that we can outperform the state-of-the-art by a large margin. Importantly, by sharing computation we can perform all tasks in as little as 30 ms.</td>
</tr>
<tr id="rev_Luo2020Dec" class="comment noshow">
	<td colspan="6"><b>Comment</b>: incorporate temporal cues as mentioned in Bosch survey paper</td>
</tr>
<tr id="bib_Luo2020Dec" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Luo2020Dec,
  author = {Luo, Wenjie and Yang, Bin and Urtasun, Raquel},
  title = {Fast and Furious: Real Time End-to-End 3D Detection, Tracking and Motion Forecasting with a Single Convolutional Net},
  journal = {ArXiv e-prints},
  year = {2020},
  url = {https://doi.org/10.48550/arXiv.2012.12395}
}
</pre></td>
</tr>
<tr id="Valada2017May" class="entry">
	<td>Valada, A., Vertens, J., Dhall, A. and Burgard, W.</td>
	<td>AdapNet: Adaptive semantic segmentation in adverse environmental conditions <p class="infolinks">[<a href="javascript:toggleInfo('Valada2017May','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Valada2017May','bibtex')">BibTeX</a>]</p></td>
	<td>2017</td>
	<td>2017 IEEE International Conference on Robotics and Automation (ICRA)2017 IEEE International Conference on Robotics and Automation (ICRA), pp. 4644-4651&nbsp;</td>
	<td>incollection</td>
	<td><a href="https://doi.org/10.1109/ICRA.2017.7989540">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Valada2017May" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Robust scene understanding of outdoor environments using passive optical sensors is a onerous and essential task for autonomous navigation. The problem is heavily characterized by changing environmental conditions throughout the day and across seasons. Robots should be equipped with models that are impervious to these factors in order to be operable and more importantly to ensure safety in the real-world. In this paper, we propose a novel semantic segmentation architecture and the convoluted mixture of deep experts (CMoDE) fusion technique that enables a multi-stream deep neural network to learn features from complementary modalities and spectra, each of which are specialized in a subset of the input space. Our model adaptively weighs class-specific features of expert networks based on the scene condition and further learns fused representations to yield robust segmentation. We present results from experimentation on three publicly available datasets that contain diverse conditions including rain, summer, winter, dusk, fall, night and sunset, and show that our approach exceeds the state-of-the-art. In addition, we evaluate the performance of autonomously traversing several kilometres of a forested environment using only the segmentation for perception.</td>
</tr>
<tr id="bib_Valada2017May" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@incollection{Valada2017May,
  author = {Valada, Abhinav and Vertens, Johan and Dhall, Ankit and Burgard, Wolfram},
  title = {AdapNet: Adaptive semantic segmentation in adverse environmental conditions},
  booktitle = {2017 IEEE International Conference on Robotics and Automation (ICRA)},
  journal = {2017 IEEE International Conference on Robotics and Automation (ICRA)},
  publisher = {IEEE},
  year = {2017},
  pages = {4644--4651},
  url = {https://doi.org/10.1109/ICRA.2017.7989540}
}
</pre></td>
</tr>
<tr id="Eigen2013Dec" class="entry">
	<td>Eigen, D., Ranzato, M. and Sutskever, I.</td>
	<td>Learning Factored Representations in a Deep Mixture of Experts <p class="infolinks">[<a href="javascript:toggleInfo('Eigen2013Dec','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Eigen2013Dec','bibtex')">BibTeX</a>]</p></td>
	<td>2013</td>
	<td>ArXiv e-prints&nbsp;</td>
	<td>article</td>
	<td><a href="https://doi.org/10.48550/arXiv.1312.4314">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Eigen2013Dec" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Mixtures of Experts combine the outputs of several "expert" networks, each of which specializes in a different part of the input space. This is achieved by training a "gating" network that maps each input to a distribution over the experts. Such models show promise for building larger networks that are still cheap to compute at test time, and more parallelizable at training time. In this this work, we extend the Mixture of Experts to a stacked model, the Deep Mixture of Experts, with multiple sets of gating and experts. This exponentially increases the number of effective experts by associating each input with a combination of experts at each layer, yet maintains a modest model size. On a randomly translated version of the MNIST dataset, we find that the Deep Mixture of Experts automatically learns to develop location-dependent ("where") experts at the first layer, and class-specific ("what") experts at the second layer. In addition, we see that the different combinations are in use when the model is applied to a dataset of speech monophones. These demonstrate effective use of all expert combinations.</td>
</tr>
<tr id="bib_Eigen2013Dec" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Eigen2013Dec,
  author = {Eigen, David and Ranzato, Marc'Aurelio and Sutskever, Ilya},
  title = {Learning Factored Representations in a Deep Mixture of Experts},
  journal = {ArXiv e-prints},
  year = {2013},
  url = {https://doi.org/10.48550/arXiv.1312.4314}
}
</pre></td>
</tr>
<tr id="Ngiam2019Aug" class="entry">
	<td>Ngiam, J., Caine, B., Han, W., Yang, B., Chai, Y., Sun, P., Zhou, Y., Yi, X., Alsharif, O., Nguyen, P., Chen, Z., Shlens, J. and Vasudevan, V.</td>
	<td>StarNet: Targeted Computation for Object Detection in Point Clouds <p class="infolinks">[<a href="javascript:toggleInfo('Ngiam2019Aug','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Ngiam2019Aug','comment')">Comment</a>] [<a href="javascript:toggleInfo('Ngiam2019Aug','bibtex')">BibTeX</a>]</p></td>
	<td>2019</td>
	<td>ArXiv e-prints&nbsp;</td>
	<td>article</td>
	<td><a href="https://doi.org/10.48550/arXiv.1908.11069">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Ngiam2019Aug" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Detecting objects from LiDAR point clouds is an important component of self-driving car technology as LiDAR provides high resolution spatial information. Previous work on point-cloud 3D object detection has re-purposed convolutional approaches from traditional camera imagery. In this work, we present an object detection system called StarNet designed specifically to take advantage of the sparse and 3D nature of point cloud data. StarNet is entirely point-based, uses no global information, has data dependent anchors, and uses sampling instead of learned region proposals. We demonstrate how this design leads to competitive or superior performance on the large Waymo Open Dataset and the KITTI detection dataset, as compared to convolutional baselines. In particular, we show how our detector can outperform a competitive baseline on Pedestrian detection on the Waymo Open Dataset by more than 7 absolute mAP while being more computationally efficient. We show how our redesign---namely using only local information and using sampling instead of learned proposals---leads to a significantly more flexible and adaptable system: we demonstrate how we can vary the computational cost of a single trained StarNet without retraining, and how we can target proposals towards areas of interest with priors and heuristics. Finally, we show how our design allows for incorporating temporal context by using detections from previous frames to target computation of the detector, which leads to further improvements in performance without additional computational cost.</td>
</tr>
<tr id="rev_Ngiam2019Aug" class="comment noshow">
	<td colspan="6"><b>Comment</b>: In fact, a recent work [164] states that the most performance gain for object detection in the KITTI dataset is due to data augmentation, rather than advances in network architectures<p>-- from Bosch survey paper</td>
</tr>
<tr id="bib_Ngiam2019Aug" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Ngiam2019Aug,
  author = {Ngiam, Jiquan and Caine, Benjamin and Han, Wei and Yang, Brandon and Chai, Yuning and Sun, Pei and Zhou, Yin and Yi, Xi and Alsharif, Ouais and Nguyen, Patrick and Chen, Zhifeng and Shlens, Jonathon and Vasudevan, Vijay},
  title = {StarNet: Targeted Computation for Object Detection in Point Clouds},
  journal = {ArXiv e-prints},
  year = {2019},
  url = {https://doi.org/10.48550/arXiv.1908.11069}
}
</pre></td>
</tr>
<tr id="Ramos2016Dec" class="entry">
	<td>Ramos, S., Gehrig, S., Pinggera, P., Franke, U. and Rother, C.</td>
	<td>Detecting Unexpected Obstacles for Self-Driving Cars: Fusing Deep Learning and Geometric Modeling <p class="infolinks">[<a href="javascript:toggleInfo('Ramos2016Dec','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Ramos2016Dec','bibtex')">BibTeX</a>]</p></td>
	<td>2016</td>
	<td>ArXiv e-prints&nbsp;</td>
	<td>article</td>
	<td><a href="https://doi.org/10.48550/arXiv.1612.06573">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Ramos2016Dec" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: The detection of small road hazards, such as lost cargo, is a vital capability for self-driving cars. We tackle this challenging and rarely addressed problem with a vision system that leverages appearance, contextual as well as geometric cues. To utilize the appearance and contextual cues, we propose a new deep learning-based obstacle detection framework. Here a variant of a fully convolutional network is used to predict a pixel-wise semantic labeling of (i) free-space, (ii) on-road unexpected obstacles, and (iii) background. The geometric cues are exploited using a state-of-the-art detection approach that predicts obstacles from stereo input images via model-based statistical hypothesis tests. We present a principled Bayesian framework to fuse the semantic and stereo-based detection results. The mid-level Stixel representation is used to describe obstacles in a flexible, compact and robust manner. We evaluate our new obstacle detection system on the Lost and Found dataset, which includes very challenging scenes with obstacles of only 5 cm height. Overall, we report a major improvement over the state-of-the-art, with relative performance gains of up to 50%. In particular, we achieve a detection rate of over 90% for distances of up to 50 m. Our system operates at 22 Hz on our self-driving platform.</td>
</tr>
<tr id="bib_Ramos2016Dec" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Ramos2016Dec,
  author = {Ramos, Sebastian and Gehrig, Stefan and Pinggera, Peter and Franke, Uwe and Rother, Carsten},
  title = {Detecting Unexpected Obstacles for Self-Driving Cars: Fusing Deep Learning and Geometric Modeling},
  journal = {ArXiv e-prints},
  year = {2016},
  url = {https://doi.org/10.48550/arXiv.1612.06573}
}
</pre></td>
</tr>
<tr id="Chen2015May" class="entry">
	<td>Chen, C., Seff, A., Kornhauser, A. and Xiao, J.</td>
	<td>DeepDriving: Learning Affordance for Direct Perception in Autonomous Driving <p class="infolinks">[<a href="javascript:toggleInfo('Chen2015May','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Chen2015May','bibtex')">BibTeX</a>]</p></td>
	<td>2015</td>
	<td>ArXiv e-prints&nbsp;</td>
	<td>article</td>
	<td><a href="https://doi.org/10.48550/arXiv.1505.00256">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Chen2015May" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Today, there are two major paradigms for vision-based autonomous driving systems: mediated perception approaches that parse an entire scene to make a driving decision, and behavior reflex approaches that directly map an input image to a driving action by a regressor. In this paper, we propose a third paradigm: a direct perception approach to estimate the affordance for driving. We propose to map an input image to a small number of key perception indicators that directly relate to the affordance of a road/traffic state for driving. Our representation provides a set of compact yet complete descriptions of the scene to enable a simple controller to drive autonomously. Falling in between the two extremes of mediated perception and behavior reflex, we argue that our direct perception representation provides the right level of abstraction. To demonstrate this, we train a deep Convolutional Neural Network using recording from 12 hours of human driving in a video game and show that our model can work well to drive a car in a very diverse set of virtual environments. We also train a model for car distance estimation on the KITTI dataset. Results show that our direct perception approach can generalize well to real driving images. Source code and data are available on our project website.</td>
</tr>
<tr id="bib_Chen2015May" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Chen2015May,
  author = {Chen, Chenyi and Seff, Ari and Kornhauser, Alain and Xiao, Jianxiong},
  title = {DeepDriving: Learning Affordance for Direct Perception in Autonomous Driving},
  journal = {ArXiv e-prints},
  year = {2015},
  url = {https://doi.org/10.48550/arXiv.1505.00256}
}
</pre></td>
</tr>
<tr id="Sauer2018Jun" class="entry">
	<td>Sauer, A., Savinov, N. and Geiger, A.</td>
	<td>Conditional Affordance Learning for Driving in Urban Environments <p class="infolinks">[<a href="javascript:toggleInfo('Sauer2018Jun','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Sauer2018Jun','bibtex')">BibTeX</a>]</p></td>
	<td>2018</td>
	<td>ArXiv e-prints&nbsp;</td>
	<td>article</td>
	<td><a href="https://doi.org/10.48550/arXiv.1806.06498">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Sauer2018Jun" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Most existing approaches to autonomous driving fall into one of two categories: modular pipelines, that build an extensive model of the environment, and imitation learning approaches, that map images directly to control outputs. A recently proposed third paradigm, direct perception, aims to combine the advantages of both by using a neural network to learn appropriate low-dimensional intermediate representations. However, existing direct perception approaches are restricted to simple highway situations, lacking the ability to navigate intersections, stop at traffic lights or respect speed limits. In this work, we propose a direct perception approach which maps video input to intermediate representations suitable for autonomous navigation in complex urban environments given high-level directional inputs. Compared to state-of-the-art reinforcement and conditional imitation learning approaches, we achieve an improvement of up to 68 % in goal-directed navigation on the challenging CARLA simulation benchmark. In addition, our approach is the first to handle traffic lights and speed signs by using image-level labels only, as well as smooth car-following, resulting in a significant reduction of traffic accidents in simulation.</td>
</tr>
<tr id="bib_Sauer2018Jun" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Sauer2018Jun,
  author = {Sauer, Axel and Savinov, Nikolay and Geiger, Andreas},
  title = {Conditional Affordance Learning for Driving in Urban Environments},
  journal = {ArXiv e-prints},
  year = {2018},
  url = {https://doi.org/10.48550/arXiv.1806.06498}
}
</pre></td>
</tr>
<tr id="Tan2022Nov" class="entry">
	<td>Tan, B., Ma, Z., Zhu, X., Li, S., Zheng, L., Chen, S., Huang, L. and Bai, J.</td>
	<td>3D Object Detection for Multi-frame 4D Automotive Millimeter-wave Radar Point Cloud <p class="infolinks">[<a href="javascript:toggleInfo('Tan2022Nov','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Tan2022Nov','comment')">Comment</a>] [<a href="javascript:toggleInfo('Tan2022Nov','bibtex')">BibTeX</a>]</p></td>
	<td>2022</td>
	<td>IEEE Sensors Journal, pp. 1&nbsp;</td>
	<td>article</td>
	<td><a href="https://doi.org/10.1109/JSEN.2022.3219643">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Tan2022Nov" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Object detection is a crucial task in autonomous driving. Currently, object-detection methods for autonomous driving systems are primarily based on information from cameras and lidar, which may experience interference from complex lighting or poor weather. At present, the four-dimensional (4D) (x, y, z, v) millimeter-wave radar can provide a denser point cloud to achieve 3D object-detection tasks that are difficult to complete with traditional millimeter-wave radar. Existing 3D object point-cloud detection algorithms are mostly based on 3D lidar, these methods are not necessarily applicable to millimeter-wave radars, which have sparser data and more noise and include velocity information. This study proposes a 3D object-detection framework based on a multi-frame 4D millimeter-wave radar point cloud. First, the ego vehicle velocity information is estimated by the millimeter-wave radar, and the relative velocity information of the millimeter-wave radar point cloud is compensated to the absolute velocity. Second, by matching between millimeter-wave radar frames, the multi-frame millimeter-wave radar point cloud is matched to the last frame. Finally, the object is detected by the proposed multi-frame millimeter-wave radar point cloud detection network. Experiments are performed using our newly recorded TJ4DRadSet dataset in a complex traffic environment. The results showed that the proposed object-detection framework outperformed the comparison methods based on the 3D mean average precision. The experimental results and methods can be used as the baseline for other multi-frame 4D millimeter-wave radar detection algorithms.</td>
</tr>
<tr id="rev_Tan2022Nov" class="comment noshow">
	<td colspan="6"><b>Comment</b>: Using TJ4D dataset</td>
</tr>
<tr id="bib_Tan2022Nov" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Tan2022Nov,
  author = {Tan, Bin and Ma, Zhixiong and Zhu, Xichan and Li, Sen and Zheng, Lianqing and Chen, Sihan and Huang, Libo and Bai, Jie},
  title = {3D Object Detection for Multi-frame 4D Automotive Millimeter-wave Radar Point Cloud},
  journal = {IEEE Sensors Journal},
  publisher = {IEEE},
  year = {2022},
  pages = {1},
  url = {https://doi.org/10.1109/JSEN.2022.3219643}
}
</pre></td>
</tr>
<tr id="Kowol2020Oct" class="entry">
	<td>Kowol, K., Rottmann, M., Bracke, S. and Gottschalk, H.</td>
	<td>YOdar: Uncertainty-based Sensor Fusion for Vehicle Detection with Camera and Radar Sensors <p class="infolinks">[<a href="javascript:toggleInfo('Kowol2020Oct','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Kowol2020Oct','bibtex')">BibTeX</a>]</p></td>
	<td>2020</td>
	<td>ArXiv e-prints&nbsp;</td>
	<td>article</td>
	<td><a href="https://doi.org/10.48550/arXiv.2010.03320">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Kowol2020Oct" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: In this work, we present an uncertainty-based method for sensor fusion with camera and radar data. The outputs of two neural networks, one processing camera and the other one radar data, are combined in an uncertainty aware manner. To this end, we gather the outputs and corresponding meta information for both networks. For each predicted object, the gathered information is post-processed by a gradient boosting method to produce a joint prediction of both networks. In our experiments we combine the YOLOv3 object detection network with a customized 1D radar segmentation network and evaluate our method on the nuScenes dataset. In particular we focus on night scenes, where the capability of object detection networks based on camera data is potentially handicapped. Our experiments show, that this approach of uncertainty aware fusion, which is also of very modular nature, significantly gains performance compared to single sensor baselines and is in range of specifically tailored deep learning based fusion approaches.</td>
</tr>
<tr id="bib_Kowol2020Oct" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Kowol2020Oct,
  author = {Kowol, Kamil and Rottmann, Matthias and Bracke, Stefan and Gottschalk, Hanno},
  title = {YOdar: Uncertainty-based Sensor Fusion for Vehicle Detection with Camera and Radar Sensors},
  journal = {ArXiv e-prints},
  year = {2020},
  url = {https://doi.org/10.48550/arXiv.2010.03320}
}
</pre></td>
</tr>
<tr id="Lee2021" class="entry">
	<td>Lee, W.-Y., Jovanov, L. and Philips, W.</td>
	<td>Semantic-Guided Radar-Vision Fusion for Depth Estimation and Object Detection <p class="infolinks">[<a href="javascript:toggleInfo('Lee2021','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Lee2021','bibtex')">BibTeX</a>]</p></td>
	<td>2021</td>
	<td>&nbsp;</td>
	<td>misc</td>
	<td><a href="https://www.semanticscholar.org/paper/Semantic-Guided-Radar-Vision-Fusion-for-Depth-and-Lee-Jovanov/432fd37984cb21c3e4265de9096f50cc6dbc6ea3">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Lee2021" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Quantitative and qualitative results on the public nuScenes dataset show that depth estimation results are on par with the state-of-the-art method, and object detection results compare favourably to the baseline and other models. In the last decade, radar is gaining its importance in perception modules of cars and infrastructure, due to its robustness against various weather and light conditions. Although radar has numerous advantages, the properties of its output signal also make the development of fusion scheme a challenging task. Most of the prior work does not exploit full potential of fusion due to the abstraction, sparsity and low quality of radar data. In this paper, we propose a novel fusion scheme to overcome this limitation by introducing semantic understanding to assist the fusion process. The sparse radar point-cloud and vision data is transformed to robust and reliable depth maps and fused in a multi-scale detection network for further exploiting the complementary information. In our experiments, we evaluate the proposed fusion scheme on both depth estimation and 2D object detection problems. Object detection results compare favourably to the state-of-the-art and demonstrate the effectiveness of the proposed scheme. Depth map estimation results are on par with the state-of-the-art on depth from RGB estimation. The ablation studies also show the effectiveness of the proposed components. scheme to perform object detection. Quantitative and qualitative results on the public nuScenes dataset confirm that our depth estimation results are on par with the state-of-the-art method, and object detection results compare favourably to the baseline and other models. Our ablation studies also clarify the effectiveness of the proposed components.</td>
</tr>
<tr id="bib_Lee2021" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@misc{Lee2021,
  author = {Lee, Wei-Yu and Jovanov, L. and Philips, W.},
  title = {Semantic-Guided Radar-Vision Fusion for Depth Estimation and Object Detection},
  year = {2021},
  note = {[Online; accessed 12. Jan. 2023]},
  url = {https://www.semanticscholar.org/paper/Semantic-Guided-Radar-Vision-Fusion-for-Depth-and-Lee-Jovanov/432fd37984cb21c3e4265de9096f50cc6dbc6ea3}
}
</pre></td>
</tr>
<tr id="Zhang2023Feb" class="entry">
	<td>Zhang, Y., Carballo, A., Yang, H. and Takeda, K.</td>
	<td>Perception and sensing for autonomous vehicles under adverse weather conditions: A survey <p class="infolinks">[<a href="javascript:toggleInfo('Zhang2023Feb','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Zhang2023Feb','comment')">Comment</a>] [<a href="javascript:toggleInfo('Zhang2023Feb','bibtex')">BibTeX</a>]</p></td>
	<td>2023</td>
	<td>ISPRS Journal of Photogrammetry and Remote Sensing<br/>Vol. 196, pp. 146-177&nbsp;</td>
	<td>article</td>
	<td><a href="https://doi.org/10.1016/j.isprsjprs.2022.12.021">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Zhang2023Feb" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Automated Driving Systems (ADS) open up a new domain for the automotive industry and offer new possibilities for future transportation with higher efficiency and comfortable experiences. However, perception and sensing for autonomous driving under adverse weather conditions have been the problem that keeps autonomous vehicles (AVs) from going to higher autonomy for a long time. This paper assesses the influences and challenges that weather brings to ADS sensors in a systematic way, and surveys the solutions against inclement weather conditions. State-of-the-art algorithms and deep learning methods on perception enhancement with regard to each kind of weather, weather status classification, and remote sensing are thoroughly reported. Sensor fusion solutions, weather conditions coverage in currently available datasets, simulators, and experimental facilities are categorized. Additionally, potential ADS sensor candidates and developing research directions such as V2X (Vehicle to Everything) technologies are discussed. By looking into all kinds of major weather problems, and reviewing both sensor and computer science solutions in recent years, this survey points out the main moving trends of adverse weather problems in perception and sensing, i.e., advanced sensor fusion and more sophisticated machine learning techniques; and also the limitations brought by emerging 1550 nm LiDARs. In general, this work contributes a holistic overview of the obstacles and directions of perception and sensing research development in terms of adverse weather conditions.</td>
</tr>
<tr id="rev_Zhang2023Feb" class="comment noshow">
	<td colspan="6"><b>Comment</b>: same as "Autonomous Driving in Adverse Weather Conditions: A Survey"</td>
</tr>
<tr id="bib_Zhang2023Feb" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Zhang2023Feb,
  author = {Zhang, Yuxiao and Carballo, Alexander and Yang, Hanting and Takeda, Kazuya},
  title = {Perception and sensing for autonomous vehicles under adverse weather conditions: A survey},
  journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
  publisher = {Elsevier},
  year = {2023},
  volume = {196},
  pages = {146--177},
  url = {https://doi.org/10.1016/j.isprsjprs.2022.12.021}
}
</pre></td>
</tr>
<tr id="Christian2022Dec" class="entry">
	<td>Christian, A.B., Wu, Y.-H., Lin, C.-Y., Van, L.-D. and Tseng, Y.-C.</td>
	<td>Radar and Camera Fusion for Object Forecasting in Driving Scenarios <p class="infolinks">[<a href="javascript:toggleInfo('Christian2022Dec','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Christian2022Dec','bibtex')">BibTeX</a>]</p></td>
	<td>2022</td>
	<td>2022 IEEE 15th International Symposium on Embedded Multicore/Many-core Systems-on-Chip (MCSoC)2022 IEEE 15th International Symposium on Embedded Multicore/Many-core Systems-on-Chip (MCSoC), pp. 105-111&nbsp;</td>
	<td>incollection</td>
	<td><a href="https://doi.org/10.1109/MCSoC57363.2022.00026">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Christian2022Dec" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: In this paper, we propose a sensor fusion architecture that combines data collected by the camera and radars and utilizes radar velocity for road users' trajectory prediction in real-world driving scenarios. This architecture is multi-stage, following the detect-track-predict paradigm. In the detection stage, camera images and radar point clouds are used to detect objects in the vehicle's surroundings by adopting two object detection models. The detected objects are tracked by an online tracking method. We also design a radar association method to extract radar velocity for an object. In the prediction stage, we build a recurrent neural network to process an object's temporal sequence of positions and velocities and predict future trajectories. Experiments on the real-world autonomous driving nuScenes dataset show that the radar velocity mainly affects the center of the bounding box representing the position of an object and thus improves the prediction performance.</td>
</tr>
<tr id="bib_Christian2022Dec" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@incollection{Christian2022Dec,
  author = {Christian, Albert Budi and Wu, Yu-Hsuan and Lin, Chih-Yu and Van, Lan-Da and Tseng, Yu-Chee},
  title = {Radar and Camera Fusion for Object Forecasting in Driving Scenarios},
  booktitle = {2022 IEEE 15th International Symposium on Embedded Multicore/Many-core Systems-on-Chip (MCSoC)},
  journal = {2022 IEEE 15th International Symposium on Embedded Multicore/Many-core Systems-on-Chip (MCSoC)},
  publisher = {IEEE},
  year = {2022},
  pages = {105--111},
  url = {https://doi.org/10.1109/MCSoC57363.2022.00026}
}
</pre></td>
</tr>
<tr id="Yadav2020Oct" class="entry">
	<td>Yadav, R., Vierling, A. and Berns, K.</td>
	<td>Radar + RGB Fusion For Robust Object Detection In Autonomous Vehicle <p class="infolinks">[<a href="javascript:toggleInfo('Yadav2020Oct','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Yadav2020Oct','bibtex')">BibTeX</a>]</p></td>
	<td>2020</td>
	<td>2020 IEEE International Conference on Image Processing (ICIP)2020 IEEE International Conference on Image Processing (ICIP), pp. 1986-1990&nbsp;</td>
	<td>incollection</td>
	<td><a href="https://doi.org/10.1109/ICIP40778.2020.9191046">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Yadav2020Oct" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: This paper presents two variations of architecture referred to as RANet and BIRANet. The proposed architecture aims to use radar signal data along with RGB camera images to form a robust detection network that works efficiently, even in variable lighting and weather conditions such as rain, dust, fog, and others. First, radar information is fused in the feature extractor network. Second, radar points are used to generate guided anchors. Third, a method is proposed to improve region proposal network [1] targets. BIRANet yields 72.3/75.3% average AP/AR on the NuScenes [2] dataset, which is better than the performance of our base network Faster-RCNN with Feature pyramid network(FFPN) [3]. RANet gives 69/71.9% average AP/AR on the same dataset, which is reasonably acceptable performance. Also, both BIRANet and RANet are evaluated to be robust towards the noise.</td>
</tr>
<tr id="bib_Yadav2020Oct" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@incollection{Yadav2020Oct,
  author = {Yadav, Ritu and Vierling, Axel and Berns, Karsten},
  title = {Radar + RGB Fusion For Robust Object Detection In Autonomous Vehicle},
  booktitle = {2020 IEEE International Conference on Image Processing (ICIP)},
  journal = {2020 IEEE International Conference on Image Processing (ICIP)},
  publisher = {IEEE},
  year = {2020},
  pages = {1986--1990},
  url = {https://doi.org/10.1109/ICIP40778.2020.9191046}
}
</pre></td>
</tr>
<tr id="Kurup2023Jan" class="entry">
	<td>Kurup, A.M. and Bos, J.P.</td>
	<td>Winter adverse driving dataset for autonomy in inclement winter weather <p class="infolinks">[<a href="javascript:toggleInfo('Kurup2023Jan','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Kurup2023Jan','bibtex')">BibTeX</a>]</p></td>
	<td>2023</td>
	<td>Optical Engineering<br/>Vol. 62(3)Optical Engineering, Vol. 62, Issue 3, pp. 031207&nbsp;</td>
	<td>incollection</td>
	<td><a href="https://doi.org/10.1117/1.OE.62.3.031207">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Kurup2023Jan" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: The availability of public datasets with annotated light detection and ranging (LiDAR) point clouds has advanced autonomous driving tasks, such as semantic and panoptic segmentation. However, there is a lack of datasets focused on inclement weather. Snow and rain degrade visibility and introduce noise in LiDAR point clouds. In this article, summarize a 3-year winter weather data collection effort and introduce the winter adverse driving dataset. It is the first multimodal dataset featuring moderate to severe winter weather---else&mdash;&#64257;weather that would cause an experienced driver to alter their driving behavior. Our dataset features exclusively events with heavy snowfall and occasional white-out conditions. Data are collected using high-resolution LiDAR, visible as well as near infrared (IR) cameras, a long wave IR camera, forward-facing radio detection and ranging, and Global Navigation Satellite Systems/Inertial Measurement Unit units. Our dataset is unique in the range of sensors and the severity of the conditions observed. It is also one of the only data sets to focus on rural and semi-rural environments. Over 36 TB of adverse winter data have been collected over 3 years. We also provide dense point-wise labels to sequential LiDAR scans collected in severe winter weather. We have labeled and will make available around 1000 sequential LiDAR scenes, amounting to over 7 GB or 3.6 billion labeled points. This is the first point-wise semantically labeled dataset to include falling snow.</td>
</tr>
<tr id="bib_Kurup2023Jan" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@incollection{Kurup2023Jan,
  author = {Kurup, Akhil M. and Bos, Jeremy P.},
  title = {Winter adverse driving dataset for autonomy in inclement winter weather},
  booktitle = {Optical Engineering, Vol. 62, Issue 3},
  journal = {Optical Engineering},
  publisher = {SPIE},
  year = {2023},
  volume = {62},
  number = {3},
  pages = {031207},
  url = {https://doi.org/10.1117/1.OE.62.3.031207}
}
</pre></td>
</tr>
<tr id="Bijelic2019" class="entry">
	<td>Bijelic, M., Gruber, T., Ritter, W. and Dietmayer, K.</td>
	<td>Automotive Sensor Performance in Adverse Weather <p class="infolinks">[<a href="javascript:toggleInfo('Bijelic2019','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Bijelic2019','bibtex')">BibTeX</a>]</p></td>
	<td>2019</td>
	<td>&nbsp;</td>
	<td>misc</td>
	<td><a href="https://www.semanticscholar.org/paper/Automotive-Sensor-Performance-in-Adverse-Weather-Bijelic-Gruber/1a5c09b014a7a5c28826e6d8f39f9e55bf57770f">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Bijelic2019" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: To assess both enhancement and robust sensor technologies for autonomous driving in various difficult weather situations, novel evaluation metrics as well as dataset baselines are necessary. Vision in all seasons is one of the key components enabling the perception for autonomous driving in various difficult weather situations aside sunny California. Towards achieving this ultimate goal, different kind of problems have to be faced. Adverse weather noise is complex, it can have multiple appearances and disturbs each sensor technology differently. This can be circumvented by enhanced and robust sensor technologies, where the performance is increased and robust downstream algorithms can interpret the perceived sensor signals. To assess both enhancement directions novel evaluation metrics as well as dataset baselines are necessary.</td>
</tr>
<tr id="bib_Bijelic2019" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@misc{Bijelic2019,
  author = {Bijelic, Mario and Gruber, Tobias and Ritter, W. and Dietmayer, K.},
  title = {Automotive Sensor Performance in Adverse Weather},
  year = {2019},
  note = {[Online; accessed 1. Feb. 2023]},
  url = {https://www.semanticscholar.org/paper/Automotive-Sensor-Performance-in-Adverse-Weather-Bijelic-Gruber/1a5c09b014a7a5c28826e6d8f39f9e55bf57770f}
}
</pre></td>
</tr>
<tr id="Zou2023Feb" class="entry">
	<td>Zou, J., Zheng, H. and Wang, F.</td>
	<td>Real-Time Target Detection System for Intelligent Vehicles Based on Multi-Source Data Fusion <p class="infolinks">[<a href="javascript:toggleInfo('Zou2023Feb','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Zou2023Feb','bibtex')">BibTeX</a>]</p></td>
	<td>2023</td>
	<td>Sensors<br/>Vol. 23(4), pp. 1823&nbsp;</td>
	<td>article</td>
	<td><a href="https://doi.org/10.3390/s23041823">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Zou2023Feb" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: To improve the identification accuracy of target detection for intelligent vehicles, a real-time target detection system based on the multi-source fusion method is proposed. Based on the ROS melodic software development environment and the NVIDIA Xavier hardware development platform, this system integrates sensing devices such as millimeter-wave radar and camera, and it can realize functions such as real-time target detection and tracking. At first, the image data can be processed by the You Only Look Once v5 network, which can increase the speed and accuracy of identification; secondly, the millimeter-wave radar data are processed to provide a more accurate distance and velocity of the targets. Meanwhile, in order to improve the accuracy of the system, the sensor fusion method is used. The radar point cloud is projected onto the image, then through space-time synchronization, region of interest (ROI) identification, and data association, the target-tracking information is presented. At last, field tests of the system are conducted, the results of which indicate that the system has a more accurate recognition effect and scene adaptation ability in complex scenes. Keywords: machine vision; millimeter-wave radar; multi-source data fusion; YOLOv5 algorithm; target detection</td>
</tr>
<tr id="bib_Zou2023Feb" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Zou2023Feb,
  author = {Zou, Junyi and Zheng, Hongyi and Wang, Feng},
  title = {Real-Time Target Detection System for Intelligent Vehicles Based on Multi-Source Data Fusion},
  journal = {Sensors},
  publisher = {Multidisciplinary Digital Publishing Institute},
  year = {2023},
  volume = {23},
  number = {4},
  pages = {1823},
  url = {https://doi.org/10.3390/s23041823}
}
</pre></td>
</tr>
<tr id="Senel2023Feb" class="entry">
	<td>Senel, N., Elger, G., Kefferpifmmodeuelse&uuml;&#64257;tz, K. and Doycheva, K.</td>
	<td>Multi-Sensor Data Fusion for Real-Time Multi-Object Tracking <p class="infolinks">[<a href="javascript:toggleInfo('Senel2023Feb','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Senel2023Feb','bibtex')">BibTeX</a>]</p></td>
	<td>2023</td>
	<td>Processes<br/>Vol. 11(2), pp. 501&nbsp;</td>
	<td>article</td>
	<td><a href="https://doi.org/10.3390/pr11020501">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Senel2023Feb" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Sensor data fusion is essential for environmental perception within smart traffic applications. By using multiple sensors cooperatively, the accuracy and probability of the perception are increased, which is crucial for critical traffic scenarios or under bad weather conditions. In this paper, a modular real-time capable multi-sensor fusion framework is presented and tested to fuse data on the object list level from distributed automotive sensors (cameras, radar, and LiDAR). The modular multi-sensor fusion architecture receives an object list (untracked objects) from each sensor. The fusion framework combines classical data fusion algorithms, as it contains a coordinate transformation module, an object association module (Hungarian algorithm), an object tracking module (unscented Kalman filter), and a movement compensation module. Due to the modular design, the fusion framework is adaptable and does not rely on the number of sensors or their types. Moreover, the method continues to operate because of this adaptable design in case of an individual sensor failure. This is an essential feature for safety-critical applications. The architecture targets environmental perception in challenging time-critical applications. The developed fusion framework is tested using simulation and public domain experimental data. Using the developed framework, sensor fusion is obtained well below 10 milliseconds of computing time using an AMD Ryzen 7 5800H mobile processor and the Python programming language. Furthermore, the object-level multi-sensor approach enables the detection of changes in the extrinsic calibration of the sensors and potential sensor failures. A concept was developed to use the multi-sensor framework to identify sensor malfunctions. This feature will become extremely important in ensuring the functional safety of the sensors for autonomous driving.</td>
</tr>
<tr id="bib_Senel2023Feb" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Senel2023Feb,
  author = {Senel, Numan and Elger, Gordon and Kefferpifmmodeuelse&uuml;&#64257;tz, Klaus and Doycheva, Kristina},
  title = {Multi-Sensor Data Fusion for Real-Time Multi-Object Tracking},
  journal = {Processes},
  publisher = {Multidisciplinary Digital Publishing Institute},
  year = {2023},
  volume = {11},
  number = {2},
  pages = {501},
  url = {https://doi.org/10.3390/pr11020501}
}
</pre></td>
</tr>
<tr id="Shi2023Feb" class="entry">
	<td>Shi, P., Peng, J., Qiu, J., Ju, X., Lo, F.P.W. and Lo, B.</td>
	<td>EVEN: An Event-Based Framework for Monocular Depth Estimation at Adverse Night Conditions <p class="infolinks">[<a href="javascript:toggleInfo('Shi2023Feb','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Shi2023Feb','bibtex')">BibTeX</a>]</p></td>
	<td>2023</td>
	<td>ArXiv e-prints&nbsp;</td>
	<td>article</td>
	<td><a href="https://doi.org/10.48550/arXiv.2302.03860">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Shi2023Feb" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Accurate depth estimation under adverse night conditions has practical impact and applications, such as on autonomous driving and rescue robots. In this work, we studied monocular depth estimation at night time in which various adverse weather, light, and different road conditions exist, with data captured in both RGB and event modalities. Event camera can better capture intensity changes by virtue of its high dynamic range (HDR), which is particularly suitable to be applied at adverse night conditions in which the amount of light is limited in the scene. Although event data can retain visual perception that conventional RGB camera may fail to capture, the lack of texture and color information of event data hinders its applicability to accurately estimate depth alone. To tackle this problem, we propose an event-vision based framework that integrates low-light enhancement for the RGB source, and exploits the complementary merits of RGB and event data. A dataset that includes paired RGB and event streams, and ground truth depth maps has been constructed. Comprehensive experiments have been conducted, and the impact of different adverse weather combinations on the performance of framework has also been investigated. The results have shown that our proposed framework can better estimate monocular depth at adverse nights than six baselines.</td>
</tr>
<tr id="bib_Shi2023Feb" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Shi2023Feb,
  author = {Shi, Peilun and Peng, Jiachuan and Qiu, Jianing and Ju, Xinwei and Lo, Frank Po Wen and Lo, Benny},
  title = {EVEN: An Event-Based Framework for Monocular Depth Estimation at Adverse Night Conditions},
  journal = {ArXiv e-prints},
  year = {2023},
  url = {https://doi.org/10.48550/arXiv.2302.03860}
}
</pre></td>
</tr>
<tr id="Liu2021Feb" class="entry">
	<td>Liu, Z., Cai, Y., Wang, H., Chen, L., Gao, H., Jia, Y. and Li, Y.</td>
	<td>Robust Target Recognition and Tracking of Self-Driving Cars With Radar and Camera Information Fusion Under Severe Weather Conditions <p class="infolinks">[<a href="javascript:toggleInfo('Liu2021Feb','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Liu2021Feb','bibtex')">BibTeX</a>]</p></td>
	<td>2021</td>
	<td>IEEE Transactions on Intelligent Transportation Systems<br/>Vol. 23(7), pp. 6640-6653&nbsp;</td>
	<td>article</td>
	<td><a href="https://doi.org/10.1109/TITS.2021.3059674">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Liu2021Feb" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Radar and camera information fusion sensing methods are used to solve the inherent shortcomings of the single sensor in severe weather. Our fusion scheme uses radar as the main hardware and camera as the auxiliary hardware framework. At the same time, the Mahalanobis distance is used to match the observed values of the target sequence. Data fusion based on the joint probability function method. Moreover, the algorithm was tested using actual sensor data collected from a vehicle, performing real-time environment perception. The test results show that radar and camera fusion algorithms perform better than single sensor environmental perception in severe weather, which can effectively reduce the missed detection rate of autonomous vehicle environment perception in severe weather. The fusion algorithm improves the robustness of the environment perception system and provides accurate environment perception information for the decision-making system and control system of autonomous vehicles.</td>
</tr>
<tr id="bib_Liu2021Feb" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Liu2021Feb,
  author = {Liu, Ze and Cai, Yingfeng and Wang, Hai and Chen, Long and Gao, Hongbo and Jia, Yunyi and Li, Yicheng},
  title = {Robust Target Recognition and Tracking of Self-Driving Cars With Radar and Camera Information Fusion Under Severe Weather Conditions},
  journal = {IEEE Transactions on Intelligent Transportation Systems},
  publisher = {IEEE},
  year = {2021},
  volume = {23},
  number = {7},
  pages = {6640--6653},
  url = {https://doi.org/10.1109/TITS.2021.3059674}
}
</pre></td>
</tr>
<tr id="Li2023Feb" class="entry">
	<td>Li, H., Wang, M., Liu, S. and Chen, P.-y.</td>
	<td>A Theoretical Understanding of shallow Vision Transformers: Learning, Generalization, and Sample Complexity <p class="infolinks">[<a href="javascript:toggleInfo('Li2023Feb','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Li2023Feb','bibtex')">BibTeX</a>]</p></td>
	<td>2023</td>
	<td>ArXiv e-prints&nbsp;</td>
	<td>article</td>
	<td><a href="https://doi.org/10.48550/arXiv.2302.06015">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Li2023Feb" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Vision Transformers (ViTs) with self-attention modules have recently achieved great empirical success in many vision tasks. Due to non-convex interactions across layers, however, theoretical learning and generalization analysis is mostly elusive. Based on a data model characterizing both label-relevant and label-irrelevant tokens, this paper provides the first theoretical analysis of training a shallow ViT, i.e., one self-attention layer followed by a two-layer perceptron, for a classification task. We characterize the sample complexity to achieve a zero generalization error. Our sample complexity bound is positively correlated with the inverse of the fraction of label-relevant tokens, the token noise level, and the initial model error. We also prove that a training process using stochastic gradient descent (SGD) leads to a sparse attention map, which is a formal verification of the general intuition about the success of attention. Moreover, this paper indicates that a proper token sparsification can improve the test performance by removing label-irrelevant and/or noisy tokens, including spurious correlations. Empirical experiments on synthetic data and CIFAR-10 dataset justify our theoretical results and generalize to deeper ViTs.</td>
</tr>
<tr id="bib_Li2023Feb" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Li2023Feb,
  author = {Li, Hongkang and Wang, Meng and Liu, Sijia and Chen, Pin-yu},
  title = {A Theoretical Understanding of shallow Vision Transformers: Learning, Generalization, and Sample Complexity},
  journal = {ArXiv e-prints},
  year = {2023},
  url = {https://doi.org/10.48550/arXiv.2302.06015}
}
</pre></td>
</tr>
<tr id="Singh2023Feb" class="entry">
	<td>Singh, A.</td>
	<td>Vision-RADAR fusion for Robotics BEV Detections: A Survey <p class="infolinks">[<a href="javascript:toggleInfo('Singh2023Feb','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Singh2023Feb','comment')">Comment</a>] [<a href="javascript:toggleInfo('Singh2023Feb','bibtex')">BibTeX</a>]</p></td>
	<td>2023</td>
	<td>ArXiv e-prints&nbsp;</td>
	<td>article</td>
	<td><a href="https://doi.org/10.48550/arXiv.2302.06643">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Singh2023Feb" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Due to the trending need of building autonomous robotic perception system, sensor fusion has attracted a lot of attention amongst researchers and engineers to make best use of cross-modality information. However, in order to build a robotic platform at scale we need to emphasize on autonomous robot platform bring-up cost as well. Cameras and radars, which inherently includes complementary perception information, has potential for developing autonomous robotic platform at scale. However, there is a limited work around radar fused with Vision, compared to LiDAR fused with vision work. In this paper, we tackle this gap with a survey on Vision-Radar fusion approaches for a BEV object detection system. First we go through the background information viz., object detection tasks, choice of sensors, sensor setup, benchmark datasets and evaluation metrics for a robotic perception system. Later, we cover per-modality (Camera and RADAR) data representation, then we go into detail about sensor fusion techniques based on sub-groups viz., early-fusion, deep-fusion, and late-fusion to easily understand the pros and cons of each method. Finally, we propose possible future trends for vision-radar fusion to enlighten future research. Regularly updated summary can be found at: this https URL</td>
</tr>
<tr id="rev_Singh2023Feb" class="comment noshow">
	<td colspan="6"><b>Comment</b>: 3D object detection</td>
</tr>
<tr id="bib_Singh2023Feb" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Singh2023Feb,
  author = {Singh, Apoorv},
  title = {Vision-RADAR fusion for Robotics BEV Detections: A Survey},
  journal = {ArXiv e-prints},
  year = {2023},
  url = {https://doi.org/10.48550/arXiv.2302.06643}
}
</pre></td>
</tr>
<tr id="Zheng2023Feb" class="entry">
	<td>Zheng, T., Li, A., Chen, Z., Wang, H. and Luo, J.</td>
	<td>AutoFed: Heterogeneity-Aware Federated Multimodal Learning for Robust Autonomous Driving <p class="infolinks">[<a href="javascript:toggleInfo('Zheng2023Feb','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Zheng2023Feb','bibtex')">BibTeX</a>]</p></td>
	<td>2023</td>
	<td>ArXiv e-prints&nbsp;</td>
	<td>article</td>
	<td><a href="https://doi.org/10.48550/arXiv.2302.08646">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Zheng2023Feb" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Object detection with on-board sensors (e.g., lidar, radar, and camera) play a crucial role in autonomous driving (AD), and these sensors complement each other in modalities. While crowdsensing may potentially exploit these sensors (of huge quantity) to derive more comprehensive knowledge, ifmmodebackslashelse&bsol;&#64257;textitlbracefederated learningrbrace (FL) appears to be the necessary tool to reach this potential: it enables autonomous vehicles (AVs) to train machine learning models without explicitly sharing raw sensory data. However, the multimodal sensors introduce various data heterogeneity across distributed AVs (e.g., label quantity skews and varied modalities), posing critical challenges to effective FL. To this end, we present AutoFed as a heterogeneity-aware FL framework to fully exploit multimodal sensory data on AVs and thus enable robust AD. Specifically, we first propose a novel model leveraging pseudo-labeling to avoid mistakenly treating unlabeled objects as the background. We also propose an autoencoder-based data imputation method to fill missing data modality (of certain AVs) with the available ones. To further reconcile the heterogeneity, we finally present a client selection mechanism exploiting the similarities among client models to improve both training stability and convergence rate. Our experiments on benchmark dataset confirm that AutoFed substantially improves over status quo approaches in both precision and recall, while demonstrating strong robustness to adverse weather conditions.</td>
</tr>
<tr id="bib_Zheng2023Feb" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Zheng2023Feb,
  author = {Zheng, Tianyue and Li, Ang and Chen, Zhe and Wang, Hongbo and Luo, Jun},
  title = {AutoFed: Heterogeneity-Aware Federated Multimodal Learning for Robust Autonomous Driving},
  journal = {ArXiv e-prints},
  year = {2023},
  url = {https://doi.org/10.48550/arXiv.2302.08646}
}
</pre></td>
</tr>
<tr id="Singh2023Feb" class="entry">
	<td>Singh, A.</td>
	<td>Transformer-Based Sensor Fusion for Autonomous Driving: A Survey <p class="infolinks">[<a href="javascript:toggleInfo('Singh2023Feb','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Singh2023Feb','bibtex')">BibTeX</a>]</p></td>
	<td>2023</td>
	<td>ArXiv e-prints&nbsp;</td>
	<td>article</td>
	<td><a href="https://doi.org/10.48550/arXiv.2302.11481">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Singh2023Feb" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Sensor fusion is an essential topic in many perception systems, such as autonomous driving and robotics. Transformers-based detection head and CNN-based feature encoder to extract features from raw sensor-data has emerged as one of the best performing sensor-fusion 3D-detection-framework, according to the dataset leaderboards. In this work we provide an in-depth literature survey of transformer based 3D-object detection task in the recent past, primarily focusing on the sensor fusion. We also briefly go through the Vision transformers (ViT) basics, so that readers can easily follow through the paper. Moreover, we also briefly go through few of the non-transformer based less-dominant methods for sensor fusion for autonomous driving. In conclusion we summarize with sensor-fusion trends to follow and provoke future research. More updated summary can be found at: this https URL</td>
</tr>
<tr id="bib_Singh2023Feb" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Singh2023Feb,
  author = {Singh, Apoorv},
  title = {Transformer-Based Sensor Fusion for Autonomous Driving: A Survey},
  journal = {ArXiv e-prints},
  year = {2023},
  url = {https://doi.org/10.48550/arXiv.2302.11481}
}
</pre></td>
</tr>
<tr id="Palffy2022Feb" class="entry">
	<td>Palffy, A., Pool, E., Baratam, S., Kooij, J.F.P. and Gavrila, D.M.</td>
	<td>Multi-Class Road User Detection With 3+1D Radar in the View-of-Delft Dataset <p class="infolinks">[<a href="javascript:toggleInfo('Palffy2022Feb','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Palffy2022Feb','bibtex')">BibTeX</a>]</p></td>
	<td>2022</td>
	<td>IEEE Robotics and Automation Letters<br/>Vol. 7(2), pp. 4961-4968&nbsp;</td>
	<td>article</td>
	<td><a href="https://doi.org/10.1109/LRA.2022.3147324">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Palffy2022Feb" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Next-generation automotive radars provide elevation data in addition to range-, azimuth- and Doppler velocity. In this experimental study, we apply a state-of-the-art object detector (PointPillars), previously used for LiDAR 3D data, to such 3+1D radar data (where 1D refers to Doppler). In ablation studies, we first explore the benefits of the additional elevation information, together with that of Doppler, radar cross section and temporal accumulation, in the context of multi-class road user detection. We subsequently compare object detection performance on the radar and LiDAR point clouds, object class-wise and as a function of distance. To facilitate our experimental study, we present the novel View-of-Delft (VoD) automotive dataset. It contains 8693 frames of synchronized and calibrated 64-layer LiDAR-, (stereo) camera-, and 3+1D radar-data acquired in complex, urban traffic. It consists of 123106 3D bounding box annotations of both moving and static objects, including 26587 pedestrian, 10800 cyclist and 26949 car labels. Our results show that object detection on 64-layer LiDAR data still outperforms that on 3+1D radar data, but the addition of elevation information and integration of successive radar scans helps close the gap. The VoD dataset is made freely available for scientific benchmarking at https://intelligent-vehicles.org/datasets/view-of-delft/ .</td>
</tr>
<tr id="bib_Palffy2022Feb" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Palffy2022Feb,
  author = {Palffy, Andras and Pool, Ewoud and Baratam, Srimannarayana and Kooij, Julian F. P. and Gavrila, Dariu M.},
  title = {Multi-Class Road User Detection With 3+1D Radar in the View-of-Delft Dataset},
  journal = {IEEE Robotics and Automation Letters},
  publisher = {IEEE},
  year = {2022},
  volume = {7},
  number = {2},
  pages = {4961--4968},
  url = {https://doi.org/10.1109/LRA.2022.3147324}
}
</pre></td>
</tr>
<tr id="Prakash2021Apr" class="entry">
	<td>Prakash, A., Chitta, K. and Geiger, A.</td>
	<td>Multi-Modal Fusion Transformer for End-to-End Autonomous Driving <p class="infolinks">[<a href="javascript:toggleInfo('Prakash2021Apr','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Prakash2021Apr','bibtex')">BibTeX</a>]</p></td>
	<td>2021</td>
	<td>ArXiv e-prints&nbsp;</td>
	<td>article</td>
	<td><a href="https://doi.org/10.48550/arXiv.2104.09224">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Prakash2021Apr" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: How should representations from complementary sensors be integrated for autonomous driving? Geometry-based sensor fusion has shown great promise for perception tasks such as object detection and motion forecasting. However, for the actual driving task, the global context of the 3D scene is key, e.g. a change in traffic light state can affect the behavior of a vehicle geometrically distant from that traffic light. Geometry alone may therefore be insufficient for effectively fusing representations in end-to-end driving models. In this work, we demonstrate that imitation learning policies based on existing sensor fusion methods under-perform in the presence of a high density of dynamic agents and complex scenarios, which require global contextual reasoning, such as handling traffic oncoming from multiple directions at uncontrolled intersections. Therefore, we propose TransFuser, a novel Multi-Modal Fusion Transformer, to integrate image and LiDAR representations using attention. We experimentally validate the efficacy of our approach in urban settings involving complex scenarios using the CARLA urban driving simulator. Our approach achieves state-of-the-art driving performance while reducing collisions by 76% compared to geometry-based fusion.</td>
</tr>
<tr id="bib_Prakash2021Apr" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Prakash2021Apr,
  author = {Prakash, Aditya and Chitta, Kashyap and Geiger, Andreas},
  title = {Multi-Modal Fusion Transformer for End-to-End Autonomous Driving},
  journal = {ArXiv e-prints},
  year = {2021},
  url = {https://doi.org/10.48550/arXiv.2104.09224}
}
</pre></td>
</tr>
<tr id="Pinto2023Feb" class="entry">
	<td>Pinto, A.S., Kolesnikov, A., Shi, Y., Beyer, L. and Zhai, X.</td>
	<td>Tuning computer vision models with task rewards <p class="infolinks">[<a href="javascript:toggleInfo('Pinto2023Feb','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Pinto2023Feb','bibtex')">BibTeX</a>]</p></td>
	<td>2023</td>
	<td>ArXiv e-prints&nbsp;</td>
	<td>article</td>
	<td><a href="https://doi.org/10.48550/arXiv.2302.08242">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Pinto2023Feb" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Misalignment between model predictions and intended usage can be detrimental for the deployment of computer vision models. The issue is exacerbated when the task involves complex structured outputs, as it becomes harder to design procedures which address this misalignment. In natural language processing, this is often addressed using reinforcement learning techniques that align models with a task reward. We adopt this approach and show its surprising effectiveness across multiple computer vision tasks, such as object detection, panoptic segmentation, colorization and image captioning. We believe this approach has the potential to be widely useful for better aligning models with a diverse range of computer vision tasks.</td>
</tr>
<tr id="bib_Pinto2023Feb" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Pinto2023Feb,
  author = {Pinto, Andrifmmodeeelse&eacute;&#64257; Susano and Kolesnikov, Alexander and Shi, Yuge and Beyer, Lucas and Zhai, Xiaohua},
  title = {Tuning computer vision models with task rewards},
  journal = {ArXiv e-prints},
  year = {2023},
  url = {https://doi.org/10.48550/arXiv.2302.08242}
}
</pre></td>
</tr>
<tr id="Lin2023Mar" class="entry">
	<td>Lin, J.-J., Guo, J.-I., Shivanna, V.M. and Chang, S.-Y.</td>
	<td>Deep Learning Derived Object Detection and Tracking Technology Based on Sensor Fusion of Millimeter-Wave Radar/Video and Its Application on Embedded Systems <p class="infolinks">[<a href="javascript:toggleInfo('Lin2023Mar','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Lin2023Mar','bibtex')">BibTeX</a>]</p></td>
	<td>2023</td>
	<td>Sensors<br/>Vol. 23(5), pp. 2746&nbsp;</td>
	<td>article</td>
	<td><a href="https://doi.org/10.3390/s23052746">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Lin2023Mar" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: This paper proposes a deep learning-based mmWave radar and RGB camera sensor early fusion method for object detection and tracking and its embedded system realization for ADAS applications. The proposed system can be used not only in ADAS systems but also to be applied to smart Road Side Units (RSU) in transportation systems to monitor real-time traffic flow and warn road users of probable dangerous situations. As the signals of mmWave radar are less affected by bad weather and lighting such as cloudy, sunny, snowy, night-light, and rainy days, it can work efficiently in both normal and adverse conditions. Compared to using an RGB camera alone for object detection and tracking, the early fusion of the mmWave radar and RGB camera technology can make up for the poor performance of the RGB camera when it fails due to bad weather and/or lighting conditions. The proposed method combines the features of radar and RGB cameras and directly outputs the results from an end-to-end trained deep neural network. Additionally, the complexity of the overall system is also reduced such that the proposed method can be implemented on PCs as well as on embedded systems like NVIDIA Jetson Xavier at 17.39 fps. Keywords: millimeter-wave radar; depth sensor; sensor fusion; object detection and tracking; early fusion; deep learning</td>
</tr>
<tr id="bib_Lin2023Mar" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Lin2023Mar,
  author = {Lin, Jia-Jheng and Guo, Jiun-In and Shivanna, Vinay Malligere and Chang, Ssu-Yuan},
  title = {Deep Learning Derived Object Detection and Tracking Technology Based on Sensor Fusion of Millimeter-Wave Radar/Video and Its Application on Embedded Systems},
  journal = {Sensors},
  publisher = {Multidisciplinary Digital Publishing Institute},
  year = {2023},
  volume = {23},
  number = {5},
  pages = {2746},
  url = {https://doi.org/10.3390/s23052746}
}
</pre></td>
</tr>
<tr id="Ouaknine2022Mar" class="entry">
	<td>Ouaknine, A.</td>
	<td>Deep learning for radar data exploitation of autonomous vehicle <p class="infolinks">[<a href="javascript:toggleInfo('Ouaknine2022Mar','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Ouaknine2022Mar','bibtex')">BibTeX</a>]</p></td>
	<td>2022</td>
	<td>ArXiv e-prints&nbsp;</td>
	<td>article</td>
	<td><a href="https://doi.org/10.48550/arXiv.2203.08038">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Ouaknine2022Mar" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Autonomous driving requires a detailed understanding of complex driving scenes. The redundancy and complementarity of the vehicle's sensors provide an accurate and robust comprehension of the environment, thereby increasing the level of performance and safety. This thesis focuses the on automotive RADAR, which is a low-cost active sensor measuring properties of surrounding objects, including their relative speed, and has the key advantage of not being impacted by adverse weather conditions. With the rapid progress of deep learning and the availability of public driving datasets, the perception ability of vision-based driving systems has considerably improved. The RADAR sensor is seldom used for scene understanding due to its poor angular resolution, the size, noise, and complexity of RADAR raw data as well as the lack of available datasets. This thesis proposes an extensive study of RADAR scene understanding, from the construction of an annotated dataset to the conception of adapted deep learning architectures. First, this thesis details approaches to tackle the current lack of data. A simple simulation as well as generative methods for creating annotated data will be presented. It will also describe the CARRADA dataset, composed of synchronised camera and RADAR data with a semi-automatic annotation method. This thesis then present a proposed set of deep learning architectures with their associated loss functions for RADAR semantic segmentation. It also introduces a method to open up research into the fusion of LiDAR and RADAR sensors for scene understanding. Finally, this thesis exposes a collaborative contribution, the RADIal dataset with synchronised High-Definition (HD) RADAR, LiDAR and camera. A deep learning architecture is also proposed to estimate the RADAR signal processing pipeline while performing multitask learning for object detection and free driving space segmentation.</td>
</tr>
<tr id="bib_Ouaknine2022Mar" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Ouaknine2022Mar,
  author = {Ouaknine, Arthur},
  title = {Deep learning for radar data exploitation of autonomous vehicle},
  journal = {ArXiv e-prints},
  year = {2022},
  url = {https://doi.org/10.48550/arXiv.2203.08038}
}
</pre></td>
</tr>
<tr id="Barbosa2023Mar" class="entry">
	<td>Barbosa, F.M. and Osifmmodeoelse&oacute;&#64257;rio, F.S.</td>
	<td>Camera-Radar Perception for Autonomous Vehicles and ADAS: Concepts, Datasets and Metrics <p class="infolinks">[<a href="javascript:toggleInfo('Barbosa2023Mar','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Barbosa2023Mar','bibtex')">BibTeX</a>]</p></td>
	<td>2023</td>
	<td>ArXiv e-prints&nbsp;</td>
	<td>article</td>
	<td><a href="https://doi.org/10.48550/arXiv.2303.04302">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Barbosa2023Mar" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: One of the main paths towards the reduction of traffic accidents is the increase in vehicle safety through driver assistance systems or even systems with a complete level of autonomy. In these types of systems, tasks such as obstacle detection and segmentation, especially the Deep Learning-based ones, play a fundamental role in scene understanding for correct and safe navigation. Besides that, the wide variety of sensors in vehicles nowadays provides a rich set of alternatives for improvement in the robustness of perception in challenging situations, such as navigation under lighting and weather adverse conditions. Despite the current focus given to the subject, the literature lacks studies on radar-based and radar-camera fusion-based perception. Hence, this work aims to carry out a study on the current scenario of camera and radar-based perception for ADAS and autonomous vehicles. Concepts and characteristics related to both sensors, as well as to their fusion, are presented. Additionally, we give an overview of the Deep Learning-based detection and segmentation tasks, and the main datasets, metrics, challenges, and open questions in vehicle perception.</td>
</tr>
<tr id="bib_Barbosa2023Mar" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Barbosa2023Mar,
  author = {Barbosa, Felipe Manfio and Osifmmodeoelse&oacute;&#64257;rio, Fernando Santos},
  title = {Camera-Radar Perception for Autonomous Vehicles and ADAS: Concepts, Datasets and Metrics},
  journal = {ArXiv e-prints},
  year = {2023},
  url = {https://doi.org/10.48550/arXiv.2303.04302}
}
</pre></td>
</tr>
<tr id="Paek2023Mar" class="entry">
	<td>Paek, D.-H., Kong, S.-H. and Wijaya, K.T.</td>
	<td>Enhanced K-Radar: Optimal Density Reduction to Improve Detection Performance and Accessibility of 4D Radar Tensor-based Object Detection <p class="infolinks">[<a href="javascript:toggleInfo('Paek2023Mar','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Paek2023Mar','bibtex')">BibTeX</a>]</p></td>
	<td>2023</td>
	<td>ArXiv e-prints&nbsp;</td>
	<td>article</td>
	<td><a href="https://doi.org/10.48550/arXiv.2303.06342">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Paek2023Mar" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Recent works have shown the superior robustness of four-dimensional (4D) Radar-based three-dimensional (3D) object detection in adverse weather conditions. However, processing 4D Radar data remains a challenge due to the large data size, which require substantial amount of memory for computing and storage. In previous work, an online density reduction is performed on the 4D Radar Tensor (4DRT) to reduce the data size, in which the density reduction level is chosen arbitrarily. However, the impact of density reduction on the detection performance and memory consumption remains largely unknown. In this paper, we aim to address this issue by conducting extensive hyperparamter tuning on the density reduction level. Experimental results show that increasing the density level from 0.01% to 50% of the original 4DRT density level proportionally improves the detection performance, at a cost of memory consumption. However, when the density level is increased beyond 5%, only the memory consumption increases, while the detection performance oscillates below the peak point. In addition to the optimized density hyperparameter, we also introduce 4D Sparse Radar Tensor (4DSRT), a new representation for 4D Radar data with offline density reduction, leading to a significantly reduced raw data size. An optimized development kit for training the neural networks is also provided, which along with the utilization of 4DSRT, improves training speed by a factor of 17.1 compared to the state-of-the-art 4DRT-based neural networks. All codes are available at: this https URL.</td>
</tr>
<tr id="bib_Paek2023Mar" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Paek2023Mar,
  author = {Paek, Dong-Hee and Kong, Seung-Hyun and Wijaya, Kevin Tirta},
  title = {Enhanced K-Radar: Optimal Density Reduction to Improve Detection Performance and Accessibility of 4D Radar Tensor-based Object Detection},
  journal = {ArXiv e-prints},
  year = {2023},
  url = {https://doi.org/10.48550/arXiv.2303.06342}
}
</pre></td>
</tr>
<tr id="Wei2022Mar" class="entry">
	<td>Wei, Z., Zhang, F., Chang, S., Liu, Y., Wu, H. and Feng, Z.</td>
	<td>MmWave Radar and Vision Fusion for Object Detection in Autonomous Driving: A Review <p class="infolinks">[<a href="javascript:toggleInfo('Wei2022Mar','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Wei2022Mar','bibtex')">BibTeX</a>]</p></td>
	<td>2022</td>
	<td>Sensors<br/>Vol. 22(7), pp. 2542&nbsp;</td>
	<td>article</td>
	<td><a href="https://doi.org/10.3390/s22072542">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Wei2022Mar" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: With autonomous driving developing in a booming stage, accurate object detection in complex scenarios attract wide attention to ensure the safety of autonomous driving. Millimeter wave (mmWave) radar and vision fusion is a mainstream solution for accurate obstacle detection. This article presents a detailed survey on mmWave radar and vision fusion based obstacle detection methods. First, we introduce the tasks, evaluation criteria, and datasets of object detection for autonomous driving. The process of mmWave radar and vision fusion is then divided into three parts: sensor deployment, sensor calibration, and sensor fusion, which are reviewed comprehensively. Specifically, we classify the fusion methods into data level, decision level, and feature level fusion methods. In addition, we introduce three-dimensional(3D) object detection, the fusion of lidar and vision in autonomous driving and multimodal information fusion, which are promising for the future. Finally, we summarize this article. Keywords: autonomous driving; radar and vision fusion; radar and camera fusion; object detection; data level fusion; decision level fusion; feature level fusion; lidar; survey; review</td>
</tr>
<tr id="bib_Wei2022Mar" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Wei2022Mar,
  author = {Wei, Zhiqing and Zhang, Fengkai and Chang, Shuo and Liu, Yangyang and Wu, Huici and Feng, Zhiyong},
  title = {MmWave Radar and Vision Fusion for Object Detection in Autonomous Driving: A Review},
  journal = {Sensors},
  publisher = {Multidisciplinary Digital Publishing Institute},
  year = {2022},
  volume = {22},
  number = {7},
  pages = {2542},
  url = {https://doi.org/10.3390/s22072542}
}
</pre></td>
</tr>
<tr id="Seo2023Feb" class="entry">
	<td>Seo, H. and Han, D.S.</td>
	<td>Radar Signal Abnormal Point Classification based on Camera-Radar Sensor Fusion <p class="infolinks">[<a href="javascript:toggleInfo('Seo2023Feb','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Seo2023Feb','bibtex')">BibTeX</a>]</p></td>
	<td>2023</td>
	<td>2023 International Conference on Artificial Intelligence in Information and Communication (ICAIIC)2023 International Conference on Artificial Intelligence in Information and Communication (ICAIIC), pp. 590-594&nbsp;</td>
	<td>incollection</td>
	<td><a href="https://doi.org/10.1109/ICAIIC57133.2023.10067112">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Seo2023Feb" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: For safe driving, it is essential to accept reliable information from recognition sensors. In this paper, we present a deep learning model that classifies whether radar signals coming in are normal or abnormal. The abnormal signal is defined as noise from the radar and all signals received when the radar fails or is in trouble. It is difficult to determine whether reflected signals are normal or not based only on radar data. Therefore, the camera and radar sensors are used together, considering the radar cross section (RCS) distribution varies by the angle and distance of the object. The proposed model uses data received from camera and radar sensors to determine the normality of object signals. The model shows an accuracy of 96.24%. Through the results of this study, the reliability of radar signals can be determined in the actual driving environment, thereby ensuring the safety of vehicles and pedestrians.</td>
</tr>
<tr id="bib_Seo2023Feb" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@incollection{Seo2023Feb,
  author = {Seo, Hyojeong and Han, Dong Seog},
  title = {Radar Signal Abnormal Point Classification based on Camera-Radar Sensor Fusion},
  booktitle = {2023 International Conference on Artificial Intelligence in Information and Communication (ICAIIC)},
  journal = {2023 International Conference on Artificial Intelligence in Information and Communication (ICAIIC)},
  publisher = {IEEE},
  year = {2023},
  pages = {590--594},
  url = {https://doi.org/10.1109/ICAIIC57133.2023.10067112}
}
</pre></td>
</tr>
<tr id="Yang2023Mar" class="entry">
	<td>Yang, B., Khatri, I., Happold, M. and Chen, C.</td>
	<td>ADCNet: End-to-end perception with raw radar ADC data <p class="infolinks">[<a href="javascript:toggleInfo('Yang2023Mar','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Yang2023Mar','bibtex')">BibTeX</a>]</p></td>
	<td>2023</td>
	<td>ArXiv e-prints&nbsp;</td>
	<td>article</td>
	<td><a href="https://doi.org/10.48550/arXiv.2303.11420">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Yang2023Mar" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: There is a renewed interest in radar sensors in the autonomous driving industry. As a relatively mature technology, radars have seen steady improvement over the last few years, making them an appealing alternative or complement to the commonly used LiDARs. An emerging trend is to leverage rich, low-level radar data for perception. In this work we push this trend to the extreme -- we propose a method to perform end-to-end learning on the raw radar analog-to-digital (ADC) data. Specifically, we design a learnable signal processing module inside the neural network, and a pre-training method guided by traditional signal processing algorithms. Experiment results corroborate the overall efficacy of the end-to-end learning method, while an ablation study validates the effectiveness of our individual innovations.</td>
</tr>
<tr id="bib_Yang2023Mar" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Yang2023Mar,
  author = {Yang, Bo and Khatri, Ishan and Happold, Michael and Chen, Chulong},
  title = {ADCNet: End-to-end perception with raw radar ADC data},
  journal = {ArXiv e-prints},
  year = {2023},
  url = {https://doi.org/10.48550/arXiv.2303.11420}
}
</pre></td>
</tr>
<tr id="Rebut2021Dec" class="entry">
	<td>Rebut, J., Ouaknine, A., Malik, W. and Pifmmodeeelse&eacute;&#64257;rez, P.</td>
	<td>Raw High-Definition Radar for Multi-Task Learning <p class="infolinks">[<a href="javascript:toggleInfo('Rebut2021Dec','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Rebut2021Dec','bibtex')">BibTeX</a>]</p></td>
	<td>2021</td>
	<td>ArXiv e-prints&nbsp;</td>
	<td>article</td>
	<td><a href="https://doi.org/10.48550/arXiv.2112.10646">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Rebut2021Dec" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: With their robustness to adverse weather conditions and ability to measure speeds, radar sensors have been part of the automotive landscape for more than two decades. Recent progress toward High Definition (HD) Imaging radar has driven the angular resolution below the degree, thus approaching laser scanning performance. However, the amount of data a HD radar delivers and the computational cost to estimate the angular positions remain a challenge. In this paper, we propose a novel HD radar sensing model, FFT-RadNet, that eliminates the overhead of computing the range-azimuth-Doppler 3D tensor, learning instead to recover angles from a range-Doppler spectrum. FFT-RadNet is trained both to detect vehicles and to segment free driving space. On both tasks, it competes with the most recent radar-based models while requiring less compute and memory. Also, we collected and annotated 2-hour worth of raw data from synchronized automotive-grade sensors (camera, laser, HD radar) in various environments (city street, highway, countryside road). This unique dataset, nick-named RADIal for "Radar, Lidar et al.", is available at this https URL.</td>
</tr>
<tr id="bib_Rebut2021Dec" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Rebut2021Dec,
  author = {Rebut, Julien and Ouaknine, Arthur and Malik, Waqas and Pifmmodeeelse&eacute;&#64257;rez, Patrick},
  title = {Raw High-Definition Radar for Multi-Task Learning},
  journal = {ArXiv e-prints},
  year = {2021},
  url = {https://doi.org/10.48550/arXiv.2112.10646}
}
</pre></td>
</tr>
<tr id="Broedermann2022Jun" class="entry">
	<td>Broedermann, T., Sakaridis, C., Dai, D. and Van Gool, L.</td>
	<td>HRFuser: A Multi-resolution Sensor Fusion Architecture for 2D Object Detection <p class="infolinks">[<a href="javascript:toggleInfo('Broedermann2022Jun','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Broedermann2022Jun','bibtex')">BibTeX</a>]</p></td>
	<td>2022</td>
	<td>arXiv.org&nbsp;</td>
	<td>article</td>
	<td><a href="https://aps.arxiv.org/abs/2206.15157#">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Broedermann2022Jun" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Besides standard cameras, autonomous vehicles typically include multiple additional sensors, such as lidars and radars, which help acquire richer information for perceiving the content of the driving scene. While several recent works focus on fusing certain pairs of sensors - such as camera and lidar or camera and radar - by using architectural components specific to the examined setting, a generic and modular sensor fusion architecture is missing from the literature. In this work, we focus on 2D object detection, a fundamental high-level task which is defined on the 2D image domain, and propose HRFuser, a multi-resolution sensor fusion architecture that scales straightforwardly to an arbitrary number of input modalities. The design of HRFuser is based on state-of-the-art high-resolution networks for image-only dense prediction and incorporates a novel multi-window cross-attention block as the means to perform fusion of multiple modalities at multiple resolutions. Even though cameras alone provide very informative features for 2D detection, we demonstrate via extensive experiments on the nuScenes and Seeing Through Fog datasets that our model effectively leverages complementary features from additional modalities, substantially improving upon camera-only performance and consistently outperforming state-of-the-art fusion methods for 2D detection both in normal and adverse conditions. The source code will be made publicly available.</td>
</tr>
<tr id="bib_Broedermann2022Jun" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Broedermann2022Jun,
  author = {Broedermann, Tim and Sakaridis, Christos and Dai, Dengxin and Van Gool, Luc},
  title = {HRFuser: A Multi-resolution Sensor Fusion Architecture for 2D Object Detection},
  journal = {arXiv.org},
  year = {2022},
  url = {https://aps.arxiv.org/abs/2206.15157#}
}
</pre></td>
</tr>
<tr id="Wang2022Mar" class="entry">
	<td>Wang, Y., Guan, Y., Li, S., Wu, J. and Cheng, H.</td>
	<td>Fusion Perception of Vision and Millimeter Wave Radar for Autonomous Driving <p class="infolinks">[<a href="javascript:toggleInfo('Wang2022Mar','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Wang2022Mar','bibtex')">BibTeX</a>]</p></td>
	<td>2022</td>
	<td>ICCAI '22: Proceedings of the 8th International Conference on Computing and Artificial Intelligence, pp. 767-772&nbsp;</td>
	<td>incollection</td>
	<td><a href="https://doi.org/10.1145/3532213.3532330">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Wang2022Mar" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Perceiving the surrounding target is one of the key technologies of automatic driving system. We propose a fusion sensing method based on camera and millimeter wave radar. The millimeter wave radar point cloud is projected onto the image plane by the calibration relation, and the point cloud frustum corresponding to each image target is intercepted. After predict the size, position, category and other information of the target through the image and point cloud, the radial basis function neural network is used to match the radar points with targets. Finally, multi-target tracking results based on target confidence and projection detection results are fused to complete the fusion perception of the target. We build a real vehicle experiment platform containing multiple sets of camera-millimeter wave radar, and verify the ability of our method to perceive and locate the surrounding target. The results show that the average positioning error for various types of targets does not exceed 0.3m.</td>
</tr>
<tr id="bib_Wang2022Mar" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@incollection{Wang2022Mar,
  author = {Wang, Yinan and Guan, Yingzhou and Li, Shuguang and Wu, Jiajun and Cheng, Hong},
  title = {Fusion Perception of Vision and Millimeter Wave Radar for Autonomous Driving},
  booktitle = {ICCAI '22: Proceedings of the 8th International Conference on Computing and Artificial Intelligence},
  publisher = {Association for Computing Machinery},
  year = {2022},
  pages = {767--772},
  url = {https://doi.org/10.1145/3532213.3532330}
}
</pre></td>
</tr>
<tr id="Abdu2021Mar" class="entry">
	<td>Abdu, F.J., Zhang, Y., Fu, M., Li, Y. and Deng, Z.</td>
	<td>Application of Deep Learning on Millimeter-Wave Radar Signals: A Review <p class="infolinks">[<a href="javascript:toggleInfo('Abdu2021Mar','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Abdu2021Mar','bibtex')">BibTeX</a>]</p></td>
	<td>2021</td>
	<td>Sensors<br/>Vol. 21(6), pp. 1951&nbsp;</td>
	<td>article</td>
	<td><a href="https://doi.org/10.3390/s21061951">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Abdu2021Mar" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: The progress brought by the deep learning technology over the last decade has inspired many research domains, such as radar signal processing, speech and audio recognition, etc., to apply it to their respective problems. Most of the prominent deep learning models exploit data representations acquired with either Lidar or camera sensors, leaving automotive radars rarely used. This is despite the vital potential of radars in adverse weather conditions, as well as their ability to simultaneously measure an object's range and radial velocity seamlessly. As radar signals have not been exploited very much so far, there is a lack of available benchmark data. However, recently, there has been a lot of interest in applying radar data as input to various deep learning algorithms, as more datasets are being provided. To this end, this paper presents a survey of various deep learning approaches processing radar signals to accomplish some significant tasks in an autonomous driving application, such as detection and classification. We have itemized the review based on different radar signal representations, as it is one of the critical aspects while using radar data with deep learning models. Furthermore, we give an extensive review of the recent deep learning-based multi-sensor fusion models exploiting radar signals and camera images for object detection tasks. We then provide a summary of the available datasets containing radar data. Finally, we discuss the gaps and important innovations in the reviewed papers and highlight some possible future research prospects. Keywords: automotive radars; object detection; object classification; deep learning; multi-sensor fusion; datasets; autonomous driving</td>
</tr>
<tr id="bib_Abdu2021Mar" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Abdu2021Mar,
  author = {Abdu, Fahad Jibrin and Zhang, Yixiong and Fu, Maozhong and Li, Yuhan and Deng, Zhenmiao},
  title = {Application of Deep Learning on Millimeter-Wave Radar Signals: A Review},
  journal = {Sensors},
  publisher = {Multidisciplinary Digital Publishing Institute},
  year = {2021},
  volume = {21},
  number = {6},
  pages = {1951},
  url = {https://doi.org/10.3390/s21061951}
}
</pre></td>
</tr>
<tr id="Prabhakara2022Oct" class="entry">
	<td>Prabhakara, A., Zhang, D., Li, C., Munir, S., Sankaranarayanan, A.C., Rowe, A. and Kumar, S.</td>
	<td>Exploring mmWave Radar and Camera Fusion for High-Resolution and Long-Range Depth Imaging <p class="infolinks">[<a href="javascript:toggleInfo('Prabhakara2022Oct','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Prabhakara2022Oct','bibtex')">BibTeX</a>]</p></td>
	<td>2022</td>
	<td>2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 3995-4002&nbsp;</td>
	<td>incollection</td>
	<td><a href="https://doi.org/10.1109/IROS47612.2022.9982080">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Prabhakara2022Oct" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Robotic geo-fencing and surveillance systems require accurate monitoring of objects if/when they violate perimeter restrictions. In this paper, we seek a solution for depth imaging of such objects of interest at high accuracy (few tens of cm) over extended ranges (up to 300 meters) from a single vantage point, such as a pole mounted platform. Unfortunately, the rich literature in depth imaging using camera, lidar and radar in isolation struggles to meet these tight requirements in real-world conditions. This paper proposes Metamoran, a solution that explores long-range depth imaging of objects of interest by fusing the strengths of two complementary technologies: mmWave radar and camera. Unlike cameras, mmWave radars offer excellent cm-scale depth resolution even at very long ranges. However, their angular resolution is at least 10x worse than camera systems. Fusing these two modalities is natural, but in scenes with high clutter and at long ranges, radar reflections are weak and experience spurious artifacts. Metamoran's core contribution is to leverage image segmentation and monocular depth estimation on camera images to help declutter radar and discover true object reflections. We perform a detailed evaluation of Metamoran's depth imaging capabilities in 400 diverse scenarios. Our evaluation shows that Metamoran estimates the depth of static objects up to 90 m away and moving objects up to 305 m away and with a median error of 28 cm, an improvement of 13 x over a naive radar+camera baseline and 23 x compared to monocular depth estimation.</td>
</tr>
<tr id="bib_Prabhakara2022Oct" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@incollection{Prabhakara2022Oct,
  author = {Prabhakara, Akarsh and Zhang, Diana and Li, Chao and Munir, Sirajum and Sankaranarayanan, Aswin C. and Rowe, Anthony and Kumar, Swarun},
  title = {Exploring mmWave Radar and Camera Fusion for High-Resolution and Long-Range Depth Imaging},
  booktitle = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  journal = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  publisher = {IEEE},
  year = {2022},
  pages = {3995--4002},
  url = {https://doi.org/10.1109/IROS47612.2022.9982080}
}
</pre></td>
</tr>
<tr id="Gallego2020Jul" class="entry">
	<td>Gallego, G., Delbrifmmodeuelse&uuml;&#64257;ck, T., Orchard, G., Bartolozzi, C., Taba, B., Censi, A., Leutenegger, S., Davison, A.J., Conradt, J., Daniilidis, K. and Scaramuzza, D.</td>
	<td>Event-Based Vision: A Survey <p class="infolinks">[<a href="javascript:toggleInfo('Gallego2020Jul','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Gallego2020Jul','bibtex')">BibTeX</a>]</p></td>
	<td>2020</td>
	<td>IEEE Transactions on Pattern Analysis and Machine Intelligence<br/>Vol. 44(1), pp. 154-180&nbsp;</td>
	<td>article</td>
	<td><a href="https://doi.org/10.1109/TPAMI.2020.3008413">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Gallego2020Jul" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Event cameras are bio-inspired sensors that differ from conventional frame cameras: Instead of capturing images at a fixed rate, they asynchronously measure per-pixel brightness changes, and output a stream of events that encode the time, location and sign of the brightness changes. Event cameras offer attractive properties compared to traditional cameras: high temporal resolution (in the order of</td>
</tr>
<tr id="bib_Gallego2020Jul" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Gallego2020Jul,
  author = {Gallego, Guillermo and Delbrifmmodeuelse&uuml;&#64257;ck, Tobi and Orchard, Garrick and Bartolozzi, Chiara and Taba, Brian and Censi, Andrea and Leutenegger, Stefan and Davison, Andrew J. and Conradt, Jifmmodeoelse&ouml;&#64257;rg and Daniilidis, Kostas and Scaramuzza, Davide},
  title = {Event-Based Vision: A Survey},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  publisher = {IEEE},
  year = {2020},
  volume = {44},
  number = {1},
  pages = {154--180},
  url = {https://doi.org/10.1109/TPAMI.2020.3008413}
}
</pre></td>
</tr>
<tr id="Gehrig2021Mar" class="entry">
	<td>Gehrig, M., Aarents, W., Gehrig, D. and Scaramuzza, D.</td>
	<td>DSEC: A Stereo Event Camera Dataset for Driving Scenarios <p class="infolinks">[<a href="javascript:toggleInfo('Gehrig2021Mar','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Gehrig2021Mar','bibtex')">BibTeX</a>]</p></td>
	<td>2021</td>
	<td>IEEE Robotics and Automation Letters<br/>Vol. 6(3), pp. 4947-4954&nbsp;</td>
	<td>article</td>
	<td><a href="https://doi.org/10.1109/LRA.2021.3068942">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Gehrig2021Mar" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Once an academic venture, autonomous driving has received unparalleled corporate funding in the last decade. Still, operating conditions of current autonomous cars are mostly restricted to ideal scenarios. This means that driving in challenging illumination conditions such as night, sunrise, and sunset remains an open problem. In these cases, standard cameras are being pushed to their limits in terms of low light and high dynamic range performance. To address these challenges, we propose, DSEC, a new dataset that contains such demanding illumination conditions and provides a rich set of sensory data. DSEC offers data from a wide-baseline stereo setup of two color frame cameras and two high-resolution monochrome event cameras. In addition, we collect lidar data and RTK GPS measurements, both hardware synchronized with all camera data. One of the distinctive features of this dataset is the inclusion of high-resolution event cameras. Event cameras have received increasing attention for their high temporal resolution and high dynamic range performance. However, due to their novelty, event camera datasets in driving scenarios are rare. This work presents the first high resolution, large scale stereo dataset with event cameras. The dataset contains 53 sequences collected by driving in a variety of illumination conditions and provides ground truth disparity for the development and evaluation of event-based stereo algorithms.</td>
</tr>
<tr id="bib_Gehrig2021Mar" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Gehrig2021Mar,
  author = {Gehrig, Mathias and Aarents, Willem and Gehrig, Daniel and Scaramuzza, Davide},
  title = {DSEC: A Stereo Event Camera Dataset for Driving Scenarios},
  journal = {IEEE Robotics and Automation Letters},
  publisher = {IEEE},
  year = {2021},
  volume = {6},
  number = {3},
  pages = {4947--4954},
  url = {https://doi.org/10.1109/LRA.2021.3068942}
}
</pre></td>
</tr>
<tr id="Zhou2022Sep" class="entry">
	<td>Zhou, Z., Wu, Z., Boutteau, R., Yang, F., Demonceaux, C. and Ginhac, D.</td>
	<td>RGB-Event Fusion for Moving Object Detection in Autonomous Driving <p class="infolinks">[<a href="javascript:toggleInfo('Zhou2022Sep','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Zhou2022Sep','bibtex')">BibTeX</a>]</p></td>
	<td>2022</td>
	<td>ArXiv e-prints&nbsp;</td>
	<td>article</td>
	<td><a href="https://doi.org/10.48550/arXiv.2209.08323">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Zhou2022Sep" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Moving Object Detection (MOD) is a critical vision task for successfully achieving safe autonomous driving. Despite plausible results of deep learning methods, most existing approaches are only frame-based and may fail to reach reasonable performance when dealing with dynamic traffic participants. Recent advances in sensor technologies, especially the Event camera, can naturally complement the conventional camera approach to better model moving objects. However, event-based works often adopt a pre-defined time window for event representation, and simply integrate it to estimate image intensities from events, neglecting much of the rich temporal information from the available asynchronous events. Therefore, from a new perspective, we propose RENet, a novel RGB-Event fusion Network, that jointly exploits the two complementary modalities to achieve more robust MOD under challenging scenarios for autonomous driving. Specifically, we first design a temporal multi-scale aggregation module to fully leverage event frames from both the RGB exposure time and larger intervals. Then we introduce a bi-directional fusion module to attentively calibrate and fuse multi-modal features. To evaluate the performance of our network, we carefully select and annotate a sub-MOD dataset from the commonly used DSEC dataset. Extensive experiments demonstrate that our proposed method performs significantly better than the state-of-the-art RGB-Event fusion alternatives. The source code and dataset are publicly available at: this https URL.</td>
</tr>
<tr id="bib_Zhou2022Sep" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Zhou2022Sep,
  author = {Zhou, Zhuyun and Wu, Zongwei and Boutteau, Rifmmodeeelse&eacute;&#64257;mi and Yang, Fan and Demonceaux, Cifmmodeeelse&eacute;&#64257;dric and Ginhac, Dominique},
  title = {RGB-Event Fusion for Moving Object Detection in Autonomous Driving},
  journal = {ArXiv e-prints},
  year = {2022},
  url = {https://doi.org/10.48550/arXiv.2209.08323}
}
</pre></td>
</tr>
<tr id="Tomy2022May" class="entry">
	<td>Tomy, A., Paigwar, A., Mann, K.S., Renzaglia, A. and Laugier, C.</td>
	<td>Fusing Event-based and RGB camera for Robust Object Detection in Adverse Conditions <p class="infolinks">[<a href="javascript:toggleInfo('Tomy2022May','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Tomy2022May','bibtex')">BibTeX</a>]</p></td>
	<td>2022</td>
	<td>2022 International Conference on Robotics and Automation (ICRA)2022 International Conference on Robotics and Automation (ICRA), pp. 933-939&nbsp;</td>
	<td>incollection</td>
	<td><a href="https://doi.org/10.1109/ICRA46639.2022.9812059">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Tomy2022May" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: The ability to detect objects, under image corruptions and different weather conditions is vital for deep learning models especially when applied to real-world applications such as autonomous driving. Traditional RGB-based detection fails under these conditions and it is thus important to design a sensor suite that is redundant to failures of the primary frame-based detection. Event-based cameras can complement frame-based cameras in low-light conditions and high dynamic range scenarios that an autonomous vehicle can encounter during navigation. Accordingly, we propose a redundant sensor fusion model of event-based and frame-based cameras that is robust to common image corruptions. The method utilizes a voxel grid representation for events as input and proposes a two-parallel feature extractor network for frames and events. Our sensor fusion approach is more robust to corruptions by over 30% compared to only frame-based detections and outperforms the only event-based detection. The model is trained and evaluated on the publicly released DSEC dataset.</td>
</tr>
<tr id="bib_Tomy2022May" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@incollection{Tomy2022May,
  author = {Tomy, Abhishek and Paigwar, Anshul and Mann, Khushdeep S. and Renzaglia, Alessandro and Laugier, Christian},
  title = {Fusing Event-based and RGB camera for Robust Object Detection in Adverse Conditions},
  booktitle = {2022 International Conference on Robotics and Automation (ICRA)},
  journal = {2022 International Conference on Robotics and Automation (ICRA)},
  publisher = {IEEE},
  year = {2022},
  pages = {933--939},
  url = {https://doi.org/10.1109/ICRA46639.2022.9812059}
}
</pre></td>
</tr>
<tr id="Cai2023Mar" class="entry">
	<td>Cai, H., Zhang, Z., Zhou, Z., Li, Z., Ding, W. and Zhao, J.</td>
	<td>BEVFusion4D: Learning LiDAR-Camera Fusion Under Bird's-Eye-View via Cross-Modality Guidance and Temporal Aggregation <p class="infolinks">[<a href="javascript:toggleInfo('Cai2023Mar','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Cai2023Mar','bibtex')">BibTeX</a>]</p></td>
	<td>2023</td>
	<td>ArXiv e-prints&nbsp;</td>
	<td>article</td>
	<td><a href="https://doi.org/10.48550/arXiv.2303.17099">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Cai2023Mar" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Integrating LiDAR and Camera information into Bird's-Eye-View (BEV) has become an essential topic for 3D object detection in autonomous driving. Existing methods mostly adopt an independent dual-branch framework to generate LiDAR and camera BEV, then perform an adaptive modality fusion. Since point clouds provide more accurate localization and geometry information, they could serve as a reliable spatial prior to acquiring relevant semantic information from the images. Therefore, we design a LiDAR-Guided View Transformer (LGVT) to effectively obtain the camera representation in BEV space and thus benefit the whole dual-branch fusion system. LGVT takes camera BEV as the primitive semantic query, repeatedly leveraging the spatial cue of LiDAR BEV for extracting image features across multiple camera views. Moreover, we extend our framework into the temporal domain with our proposed Temporal Deformable Alignment (TDA) module, which aims to aggregate BEV features from multiple historical frames. Including these two modules, our framework dubbed BEVFusion4D achieves state-of-the-art results in 3D object detection, with 72.0% mAP and 73.5% NDS on the nuScenes validation set, and 73.3% mAP and 74.7% NDS on nuScenes test set, respectively.</td>
</tr>
<tr id="bib_Cai2023Mar" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Cai2023Mar,
  author = {Cai, Hongxiang and Zhang, Zeyuan and Zhou, Zhenyu and Li, Ziyin and Ding, Wenbo and Zhao, Jiuhua},
  title = {BEVFusion4D: Learning LiDAR-Camera Fusion Under Bird's-Eye-View via Cross-Modality Guidance and Temporal Aggregation},
  journal = {ArXiv e-prints},
  year = {2023},
  url = {https://doi.org/10.48550/arXiv.2303.17099}
}
</pre></td>
</tr>
<tr id="Jamil2022Nov" class="entry">
	<td>Jamil, S., Piran, relaxMd.J. and Kwon, O.-J.</td>
	<td>A Comprehensive Survey of Transformers for Computer Vision <p class="infolinks">[<a href="javascript:toggleInfo('Jamil2022Nov','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Jamil2022Nov','bibtex')">BibTeX</a>]</p></td>
	<td>2022</td>
	<td>ArXiv e-prints&nbsp;</td>
	<td>article</td>
	<td><a href="https://doi.org/10.48550/arXiv.2211.06004">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Jamil2022Nov" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: As a special type of transformer, Vision Transformers (ViTs) are used to various computer vision applications (CV), such as image recognition. There are several potential problems with convolutional neural networks (CNNs) that can be solved with ViTs. For image coding tasks like compression, super-resolution, segmentation, and denoising, different variants of the ViTs are used. The purpose of this survey is to present the first application of ViTs in CV. The survey is the first of its kind on ViTs for CVs to the best of our knowledge. In the first step, we classify different CV applications where ViTs are applicable. CV applications include image classification, object detection, image segmentation, image compression, image super-resolution, image denoising, and anomaly detection. Our next step is to review the state-of-the-art in each category and list the available models. Following that, we present a detailed analysis and comparison of each model and list its pros and cons. After that, we present our insights and lessons learned for each category. Moreover, we discuss several open research challenges and future research directions.</td>
</tr>
<tr id="bib_Jamil2022Nov" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Jamil2022Nov,
  author = {Jamil, Sonain and Piran,  Md. Jalil and Kwon, Oh-Jin},
  title = {A Comprehensive Survey of Transformers for Computer Vision},
  journal = {ArXiv e-prints},
  year = {2022},
  url = {https://doi.org/10.48550/arXiv.2211.06004}
}
</pre></td>
</tr>
<tr id="Li2022Apr" class="entry">
	<td>Li, P., Wang, P., Berntorp, K. and Liu, H.</td>
	<td>Exploiting Temporal Relations on Radar Perception for Autonomous Driving <p class="infolinks">[<a href="javascript:toggleInfo('Li2022Apr','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Li2022Apr','bibtex')">BibTeX</a>]</p></td>
	<td>2022</td>
	<td>ArXiv e-prints&nbsp;</td>
	<td>article</td>
	<td><a href="https://doi.org/10.48550/arXiv.2204.01184">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Li2022Apr" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: We consider the object recognition problem in autonomous driving using automotive radar sensors. Comparing to Lidar sensors, radar is cost-effective and robust in all-weather conditions for perception in autonomous driving. However, radar signals suffer from low angular resolution and precision in recognizing surrounding objects. To enhance the capacity of automotive radar, in this work, we exploit the temporal information from successive ego-centric bird-eye-view radar image frames for radar object recognition. We leverage the consistency of an object's existence and attributes (size, orientation, etc.), and propose a temporal relational layer to explicitly model the relations between objects within successive radar images. In both object detection and multiple object tracking, we show the superiority of our method compared to several baseline approaches.</td>
</tr>
<tr id="bib_Li2022Apr" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Li2022Apr,
  author = {Li, Peizhao and Wang, Pu and Berntorp, Karl and Liu, Hongfu},
  title = {Exploiting Temporal Relations on Radar Perception for Autonomous Driving},
  journal = {ArXiv e-prints},
  year = {2022},
  url = {https://doi.org/10.48550/arXiv.2204.01184}
}
</pre></td>
</tr>
<tr id="Li2023Apr" class="entry">
	<td>Li, Y., Moreau, J. and Ibanez-Guzman, J.</td>
	<td>Emergent Visual Sensors for Autonomous Vehicles <p class="infolinks">[<a href="javascript:toggleInfo('Li2023Apr','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Li2023Apr','bibtex')">BibTeX</a>]</p></td>
	<td>2023</td>
	<td>IEEE Transactions on Intelligent Transportation Systems, pp. 1-22&nbsp;</td>
	<td>article</td>
	<td><a href="https://doi.org/10.1109/TITS.2023.3248483">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Li2023Apr" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: For vehicles to navigate autonomously, they need to perceive and understand their immediate surroundings. Currently, cameras are the preferred sensors, due to their high performance and relatively low-cost compared with other sensors like LiDARs and Radars. However, their performance is limited by inherent imaging constraints, a standard RGB camera may perform poorly in extreme conditions, including low illumination, high contrast, bad weather (e.g. fog, rain, snow, etc.), glare, etc. Further, when using monocular cameras, it is more challenging to determine spatial distances than when using active range sensors such as LiDARs or Radars. Over the past years, novel image sensors, namely, infrared cameras, range-gated cameras, polarization cameras, and event cameras, have demonstrated strong potential. Some of them could be game-changers for future autonomous vehicles, they are the result of progress in sensor technology and the development of the accompanying perception algorithms. This paper presents in a systematic manner their principles, comparative advantages, data processing algorithms, and related applications. The purpose is to provide practitioners with an in-depth overview of novel sensing technologies that can contribute to the safe deployment of autonomous vehicles.</td>
</tr>
<tr id="bib_Li2023Apr" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Li2023Apr,
  author = {Li, You and Moreau, Julien and Ibanez-Guzman, Javier},
  title = {Emergent Visual Sensors for Autonomous Vehicles},
  journal = {IEEE Transactions on Intelligent Transportation Systems},
  publisher = {IEEE},
  year = {2023},
  pages = {1--22},
  url = {https://doi.org/10.1109/TITS.2023.3248483}
}
</pre></td>
</tr>
<tr id="Yang2022Nov" class="entry">
	<td>Yang, Y., Liu, J., Huang, T., Han, Q.-L., Ma, G. and Zhu, B.</td>
	<td>RaLiBEV: Radar and LiDAR BEV Fusion Learning for Anchor Box Free Object Detection System <p class="infolinks">[<a href="javascript:toggleInfo('Yang2022Nov','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Yang2022Nov','bibtex')">BibTeX</a>]</p></td>
	<td>2022</td>
	<td>ArXiv e-prints&nbsp;</td>
	<td>article</td>
	<td><a href="https://doi.org/10.48550/arXiv.2211.06108">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Yang2022Nov" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: In autonomous driving systems, LiDAR and radar play important roles in the perception of the surrounding environment.LiDAR provides accurate 3D spatial sensing information but cannot work in adverse weather like fog. On the other hand, the radar signal can be diffracted when encountering raindrops or mist particles thanks to its wavelength, but it suffers from large noise. Recent state-of-the-art works reveal that fusion of radar and LiDAR can lead to robust detection in adverse weather. The existing works adopt convolutional neural network architecture to extract features from each sensor data stream, then align and aggregate the two branch features to predict object detection results. However, these methods have low accuracy of bounding box estimations due to a simple design of label assignment and fusion strategies. In this paper, we propose a bird's-eye view fusion learning-based anchor box-free object detection system, which fuses the feature derived from the radar range-azimuth heatmap and the LiDAR point cloud to estimate the possible objects. Different label assignment strategies have been designed to facilitate the consistency between the classification of foreground or background anchor points and the corresponding bounding box regressions. In addition, the performance of the proposed object detector is further enhanced by employing a novel interactive transformer module. The superior performance of the proposed methods in this paper has been demonstrated using the recently published Oxford radar robotCar dataset, showing that the average precision of our system significantly outperforms the best state-of-the-art method by 14.4% and 20.5% at IoU equals 0.8 in clear and foggy weather testing, respectively.</td>
</tr>
<tr id="bib_Yang2022Nov" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Yang2022Nov,
  author = {Yang, Yanlong and Liu, Jianan and Huang, Tao and Han, Qing-Long and Ma, Gang and Zhu, Bing},
  title = {RaLiBEV: Radar and LiDAR BEV Fusion Learning for Anchor Box Free Object Detection System},
  journal = {ArXiv e-prints},
  year = {2022},
  url = {https://doi.org/10.48550/arXiv.2211.06108}
}
</pre></td>
</tr>
<tr id="Zhou2022Aug" class="entry">
	<td>Zhou, T., Shi, Y., Chen, J., Jiang, K., Yang, M. and Yang, D.</td>
	<td>Bridging the View Disparity Between Radar and Camera Features for Multi-modal Fusion 3D Object Detection <p class="infolinks">[<a href="javascript:toggleInfo('Zhou2022Aug','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Zhou2022Aug','bibtex')">BibTeX</a>]</p></td>
	<td>2022</td>
	<td>ArXiv e-prints&nbsp;</td>
	<td>article</td>
	<td><a href="https://doi.org/10.48550/arXiv.2208.12079">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Zhou2022Aug" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Environmental perception with the multi-modal fusion of radar and camera is crucial in autonomous driving to increase accuracy, completeness, and robustness. This paper focuses on utilizing millimeter-wave (MMW) radar and camera sensor fusion for 3D object detection. A novel method that realizes the feature-level fusion under the bird's-eye view (BEV) for a better feature representation is proposed. Firstly, radar points are augmented with temporal accumulation and sent to a spatial-temporal encoder for radar feature extraction. Meanwhile, multi-scale image 2D features which adapt to various spatial scales are obtained by image backbone and neck model. Then, image features are transformed to BEV with the designed view transformer. In addition, this work fuses the multi-modal features with a two-stage fusion model called point-fusion and ROI-fusion, respectively. Finally, a detection head regresses objects category and 3D locations. Experimental results demonstrate that the proposed method realizes the state-of-the-art (SOTA) performance under the most crucial detection metrics-mean average precision (mAP) and nuScenes detection score (NDS) on the challenging nuScenes dataset.</td>
</tr>
<tr id="bib_Zhou2022Aug" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Zhou2022Aug,
  author = {Zhou, Taohua and Shi, Yining and Chen, Junjie and Jiang, Kun and Yang, Mengmeng and Yang, Diange},
  title = {Bridging the View Disparity Between Radar and Camera Features for Multi-modal Fusion 3D Object Detection},
  journal = {ArXiv e-prints},
  year = {2022},
  url = {https://doi.org/10.48550/arXiv.2208.12079}
}
</pre></td>
</tr>
<tr id="Li2022" class="entry">
	<td>Li, P., Wang, P., Berntorp, K. and Liu, H.</td>
	<td>Exploiting Temporal Relations on Radar Perception for Autonomous Driving <p class="infolinks">[<a href="javascript:toggleInfo('Li2022','bibtex')">BibTeX</a>]</p></td>
	<td>2022</td>
	<td>, pp. 17071-17080&nbsp;</td>
	<td>misc</td>
	<td><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Li_Exploiting_Temporal_Relations_on_Radar_Perception_for_Autonomous_Driving_CVPR_2022_paper.html">URL</a>&nbsp;</td>
</tr>
<tr id="bib_Li2022" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@misc{Li2022,
  author = {Li, Peizhao and Wang, Pu and Berntorp, Karl and Liu, Hongfu},
  title = {Exploiting Temporal Relations on Radar Perception for Autonomous Driving},
  year = {2022},
  pages = {17071--17080},
  note = {[Online; accessed 8. Apr. 2023]},
  url = {https://openaccess.thecvf.com/content/CVPR2022/html/Li_Exploiting_Temporal_Relations_on_Radar_Perception_for_Autonomous_Driving_CVPR_2022_paper.html}
}
</pre></td>
</tr>
<tr id="Scheiner2021Dec" class="entry">
	<td>Scheiner, N., Kraus, F., Appenrodt, N., Dickmann, J. and Sick, B.</td>
	<td>Object detection for automotive radar point clouds &ndash; a comparison <p class="infolinks">[<a href="javascript:toggleInfo('Scheiner2021Dec','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Scheiner2021Dec','bibtex')">BibTeX</a>]</p></td>
	<td>2021</td>
	<td>AI Perspectives<br/>Vol. 3(1), pp. 1-23&nbsp;</td>
	<td>article</td>
	<td><a href="https://doi.org/10.1186/s42467-021-00012-z">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Scheiner2021Dec" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Automotive radar perception is an integral part of automated driving systems. Radar sensors benefit from their excellent robustness against adverse weather conditions such as snow, fog, or heavy rain. Despite the fact that machine-learning-based object detection is traditionally a camera-based domain, vast progress has been made for lidar sensors, and radar is also catching up. Recently, several new techniques for using machine learning algorithms towards the correct detection and classification of moving road users in automotive radar data have been introduced. However, most of them have not been compared to other methods or require next generation radar sensors which are far more advanced than current conventional automotive sensors. This article makes a thorough comparison of existing and novel radar object detection algorithms with some of the most successful candidates from the image and lidar domain. All experiments are conducted using a conventional automotive radar system. In addition to introducing all architectures, special attention is paid to the necessary point cloud preprocessing for all methods. By assessing all methods on a large and open real world data set, this evaluation provides the first representative algorithm comparison in this domain and outlines future research directions.</td>
</tr>
<tr id="bib_Scheiner2021Dec" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Scheiner2021Dec,
  author = {Scheiner, Nicolas and Kraus, Florian and Appenrodt, Nils and Dickmann, Jifmmodeuelse&uuml;&#64257;rgen and Sick, Bernhard},
  title = {Object detection for automotive radar point clouds &ndash; a comparison},
  journal = {AI Perspectives},
  publisher = {SpringerOpen},
  year = {2021},
  volume = {3},
  number = {1},
  pages = {1--23},
  url = {https://doi.org/10.1186/s42467-021-00012-z}
}
</pre></td>
</tr>
<tr id="Bansal2022Aug" class="entry">
	<td>Bansal, K., Rungta, K. and Bharadia, D.</td>
	<td>RadSegNet: A Reliable Approach to Radar Camera Fusion <p class="infolinks">[<a href="javascript:toggleInfo('Bansal2022Aug','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Bansal2022Aug','bibtex')">BibTeX</a>]</p></td>
	<td>2022</td>
	<td>ArXiv e-prints&nbsp;</td>
	<td>article</td>
	<td><a href="https://doi.org/10.48550/arXiv.2208.03849">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Bansal2022Aug" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Perception systems for autonomous driving have seen significant advancements in their performance over last few years. However, these systems struggle to show robustness in extreme weather conditions because sensors like lidars and cameras, which are the primary sensors in a sensor suite, see a decline in performance under these conditions. In order to solve this problem, camera-radar fusion systems provide a unique opportunity for all weather reliable high quality perception. Cameras provides rich semantic information while radars can work through occlusions and in all weather conditions. In this work, we show that the state-of-the-art fusion methods perform poorly when camera input is degraded, which essentially results in losing the all-weather reliability they set out to achieve. Contrary to these approaches, we propose a new method, RadSegNet, that uses a new design philosophy of independent information extraction and truly achieves reliability in all conditions, including occlusions and adverse weather. We develop and validate our proposed system on the benchmark Astyx dataset and further verify these results on the RADIATE dataset. When compared to state-of-the-art methods, RadSegNet achieves a 27% improvement on Astyx and 41.46% increase on RADIATE, in average precision score and maintains a significantly better performance in adverse weather conditions</td>
</tr>
<tr id="bib_Bansal2022Aug" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Bansal2022Aug,
  author = {Bansal, Kshitiz and Rungta, Keshav and Bharadia, Dinesh},
  title = {RadSegNet: A Reliable Approach to Radar Camera Fusion},
  journal = {ArXiv e-prints},
  year = {2022},
  url = {https://doi.org/10.48550/arXiv.2208.03849}
}
</pre></td>
</tr>
<tr id="Deng2022Sep" class="entry">
	<td>Deng, K., Zhao, D., Han, Q., Wang, S., Zhang, Z., Zhou, A. and Ma, H.</td>
	<td>Geryon: Edge Assisted Real-time and Robust Object Detection on Drones via mmWave Radar and Camera Fusion <p class="infolinks">[<a href="javascript:toggleInfo('Deng2022Sep','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Deng2022Sep','bibtex')">BibTeX</a>]</p></td>
	<td>2022</td>
	<td>Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies<br/>Vol. 6(3), pp. 1-27&nbsp;</td>
	<td>article</td>
	<td><a href="https://doi.org/10.1145/3550298">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Deng2022Sep" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Vision-based drone-view object detection suffers from severe performance degradation under adverse conditions (e.g., foggy weather, poor illumination). To remedy this, leveraging complementary mmWave radar has become a trend. However, existing fusion approaches seldom apply to drones due to i) the aggravated sparsity and noise of point clouds from low-cost commodity radars, and ii) explosive sensing data and intensive computations leading to high latency. To address these issues, we design Geryon, an edge assisted object detection system on drones, which utilizes a suit of approaches to fully exploit the complementary advantages of camera and mmWave radar on three levels: (i) a novel multi-frame compositing approach utilizes camera to assist radar to address the aggravated sparsity and noise of radar point clouds; (ii) a saliency area extraction and encoding approach utilizes radar to assist camera to reduce the bandwidth consumption and offloading latency; (iii) a parallel transmission and inference approach with a lightweight box enhancement scheme further reduces the offloading latency while ensuring the edge-side accuracy-latency trade-off by the parallelism and better camera-radar fusion. We implement and evaluate Geryon with four datasets we collect under foggy/rainy/snowy weather and poor illumination conditions, demonstrating its great advantages over other state-of-the-art approaches in terms of both accuracy and latency.</td>
</tr>
<tr id="bib_Deng2022Sep" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Deng2022Sep,
  author = {Deng, Kaikai and Zhao, Dong and Han, Qiaoyue and Wang, Shuyue and Zhang, Zihan and Zhou, Anfu and Ma, Huadong},
  title = {Geryon: Edge Assisted Real-time and Robust Object Detection on Drones via mmWave Radar and Camera Fusion},
  journal = {Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies},
  publisher = {Association for Computing Machinery},
  year = {2022},
  volume = {6},
  number = {3},
  pages = {1--27},
  url = {https://doi.org/10.1145/3550298}
}
</pre></td>
</tr>
<tr id="Wang2020Aug" class="entry">
	<td>Wang, X., Yu, F., Dunlap, L., Ma, Y.-A., Wang, R., Mirhoseini, A., Darrell, T. and Gonzalez, J.E.</td>
	<td>Deep Mixture of Experts via Shallow Embedding <p class="infolinks">[<a href="javascript:toggleInfo('Wang2020Aug','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Wang2020Aug','bibtex')">BibTeX</a>]</p></td>
	<td>2020</td>
	<td>PMLRUncertainty in Artificial Intelligence, pp. 552-562&nbsp;</td>
	<td>incollection</td>
	<td><a href="https://proceedings.mlr.press/v115/wang20d.html">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Wang2020Aug" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Deep Mixture of Experts via Shallow EmbeddingXin Wang, Fisher Yu, Lisa Dunlap, Yi-An Ma, Ruth Wang, Azalia Mirhoseini, Trevor D...</td>
</tr>
<tr id="bib_Wang2020Aug" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@incollection{Wang2020Aug,
  author = {Wang, Xin and Yu, Fisher and Dunlap, Lisa and Ma, Yi-An and Wang, Ruth and Mirhoseini, Azalia and Darrell, Trevor and Gonzalez, Joseph E.},
  title = {Deep Mixture of Experts via Shallow Embedding},
  booktitle = {Uncertainty in Artificial Intelligence},
  journal = {PMLR},
  publisher = {PMLR},
  year = {2020},
  pages = {552--562},
  url = {https://proceedings.mlr.press/v115/wang20d.html}
}
</pre></td>
</tr>
<tr id="Kim2023Apr" class="entry">
	<td>Kim, Y., Kim, S., Shin, J., Choi, J.W. and Kum, D.</td>
	<td>CRN: Camera Radar Net for Accurate, Robust, Efficient 3D Perception <p class="infolinks">[<a href="javascript:toggleInfo('Kim2023Apr','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Kim2023Apr','bibtex')">BibTeX</a>]</p></td>
	<td>2023</td>
	<td>ArXiv e-prints&nbsp;</td>
	<td>article</td>
	<td><a href="https://doi.org/10.48550/arXiv.2304.00670">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Kim2023Apr" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Autonomous driving requires an accurate and fast 3D perception system that includes 3D object detection, tracking, and segmentation. Although recent low-cost camera-based approaches have shown promising results, they are susceptible to poor illumination or bad weather conditions and have a large localization error. Hence, fusing camera with low-cost radar, which provides precise long-range measurement and operates reliably in all environments, is promising but has not yet been thoroughly investigated. In this paper, we propose Camera Radar Net (CRN), a novel camera-radar fusion framework that generates a semantically rich and spatially accurate bird's-eye-view (BEV) feature map for various tasks. To overcome the lack of spatial information in an image, we transform perspective view image features to BEV with the help of sparse but accurate radar points. We further aggregate image and radar feature maps in BEV using multi-modal deformable attention designed to tackle the spatial misalignment between inputs. CRN with real-time setting operates at 20 FPS while achieving comparable performance to LiDAR detectors on nuScenes, and even outperforms at a far distance on 100m setting. Moreover, CRN with offline setting yields 62.4% NDS, 57.5% mAP on nuScenes test set and ranks first among all camera and camera-radar 3D object detectors.</td>
</tr>
<tr id="bib_Kim2023Apr" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Kim2023Apr,
  author = {Kim, Youngseok and Kim, Sanmin and Shin, Juyeb and Choi, Jun Won and Kum, Dongsuk},
  title = {CRN: Camera Radar Net for Accurate, Robust, Efficient 3D Perception},
  journal = {ArXiv e-prints},
  year = {2023},
  url = {https://doi.org/10.48550/arXiv.2304.00670}
}
</pre></td>
</tr>
<tr id="Abdu2023Apr" class="entry">
	<td>Abdu, F.J., Zhang, Y. and Deng, Z.</td>
	<td>CCA-Based Fusion of Camera and Radar Features for Target Classification Under Adverse Weather Conditions <p class="infolinks">[<a href="javascript:toggleInfo('Abdu2023Apr','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Abdu2023Apr','bibtex')">BibTeX</a>]</p></td>
	<td>2023</td>
	<td>Neural Processing Letters, pp. 1-27&nbsp;</td>
	<td>article</td>
	<td><a href="https://doi.org/10.1007/s11063-023-11261-w">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Abdu2023Apr" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Deep learning models such as deep convolutional neural networks (DCNNs) image classifiers have achieved outstanding performance over the last decade. However, these models are mostly trained with high-quality images drawn from publicly available datasets such as ImageNet. Recently, many researchers have evaluated the impact of low-quality image degradations on the performance of different neural network-based image classifiers. But, most of these studies generate low-quality images by synthetic modification of the high-quality images. Besides, most of the studies employed various image processing techniques to remove the image degradations and trained the DCNNs again to achieve better performance. But it has since been discovered that such methods could not improve the classification accuracy of DCNNs. The robustness of DCNNs based image classifiers trained on low-quality images resulting from natural factors common in autonomous driving and other intelligent system settings was rarely studied over the recent years. In this paper, we proposed a canonical correlation analysis (CCA) based fusion of camera and radar features for improving the performance of DCNNs image classifiers trained on natural adverse weather data. CCA is a statistical approach that creates a highly discriminative feature vector by measuring the linear relationship between the camera and radar features. A spatial attention network was designed to re-weight the camera features before associating them with radar features in the CCA-feature fusion block. Our findings based on experimental evaluations have proven that, indeed, the performance of the DCNN models (i.e., Alex-Net and VGG-16-Net) is heavily affected by degradations arising from natural factors. Specifically, the DCNN models are more affected by the degradations arising from rainfall, foggy and nighttime conditions using Radiate and Carrada datasets. However, the proposed fusion frameworks have improved the performance of the individual sensing modalities significantly. The radar data has helped substantially in enhancing the fusion performance, mainly using rainfall data where the camera data is heavily affected.</td>
</tr>
<tr id="bib_Abdu2023Apr" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Abdu2023Apr,
  author = {Abdu, Fahad Jibrin and Zhang, Yixiong and Deng, Zhenmiao},
  title = {CCA-Based Fusion of Camera and Radar Features for Target Classification Under Adverse Weather Conditions},
  journal = {Neural Processing Letters},
  publisher = {Springer US},
  year = {2023},
  pages = {1--27},
  url = {https://doi.org/10.1007/s11063-023-11261-w}
}
</pre></td>
</tr>
<tr id="Yao2023Apr" class="entry">
	<td>Yao, S., Guan, R., Huang, X., Li, Z., Sha, X., Yue, Y., Lim, E.G., Seo, H., Man, K.L., Zhu, X. and Yue, Y.</td>
	<td>Radar-Camera Fusion for Object Detection and Semantic Segmentation in Autonomous Driving: A Comprehensive Review <p class="infolinks">[<a href="javascript:toggleInfo('Yao2023Apr','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Yao2023Apr','bibtex')">BibTeX</a>]</p></td>
	<td>2023</td>
	<td>ArXiv e-prints&nbsp;</td>
	<td>article</td>
	<td><a href="https://doi.org/10.48550/arXiv.2304.10410">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Yao2023Apr" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Driven by deep learning techniques, perception technology in autonomous driving has developed rapidly in recent years. To achieve accurate and robust perception capabilities, autonomous vehicles are often equipped with multiple sensors, making sensor fusion a crucial part of the perception system. Among these fused sensors, radars and cameras enable a complementary and cost-effective perception of the surrounding environment regardless of lighting and weather conditions. This review aims to provide a comprehensive guideline for radar-camera fusion, particularly concentrating on perception tasks related to object detection and semantic segmentation. Based on the principles of the radar and camera sensors, we delve into the data processing process and representations, followed by an in-depth analysis and summary of radar-camera fusion datasets. In the review of methodologies in radar-camera fusion, we address interrogative questions, including "why to fuse", "what to fuse", "where to fuse", "when to fuse", and "how to fuse", subsequently discussing various challenges and potential research directions within this domain. To ease the retrieval and comparison of datasets and fusion methods, we also provide an interactive website: this https URL.</td>
</tr>
<tr id="bib_Yao2023Apr" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Yao2023Apr,
  author = {Yao, Shanliang and Guan, Runwei and Huang, Xiaoyu and Li, Zhuoxiao and Sha, Xiangyu and Yue, Yong and Lim, Eng Gee and Seo, Hyungjoon and Man, Ka Lok and Zhu, Xiaohui and Yue, Yutao},
  title = {Radar-Camera Fusion for Object Detection and Semantic Segmentation in Autonomous Driving: A Comprehensive Review},
  journal = {ArXiv e-prints},
  year = {2023},
  url = {https://doi.org/10.48550/arXiv.2304.10410}
}
</pre></td>
</tr>
<tr id="Liu2023Apr" class="entry">
	<td>Liu, Z., Cheng, J., Fan, J., Lin, S., Wang, Y. and Zhao, X.</td>
	<td>Multi-Modal Fusion Based on Depth Adaptive Mechanism for 3D Object Detection <p class="infolinks">[<a href="javascript:toggleInfo('Liu2023Apr','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Liu2023Apr','bibtex')">BibTeX</a>]</p></td>
	<td>2023</td>
	<td>IEEE Transactions on Multimedia, pp. 1-11&nbsp;</td>
	<td>article</td>
	<td><a href="https://doi.org/10.1109/TMM.2023.3270638">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Liu2023Apr" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Lidars and cameras are critical sensors for 3D object detection in autonomous driving. Despite the increasing popularity of sensor fusion in this field, accurate and robust fusion methods are still under exploration due to non-homogenous representations. In this paper, we find that the complementary roles of point clouds and images vary with depth. An important reason is that the point cloud appearance changes significantly with increasing distance from the Lidar, while the image's edge, color, and texture information are not sensitive to depth. To address this, we propose a fusion module based on the Depth Attention Mechanism (DAM), which mainly consists of two operations: gated feature generation and point cloud division. The former adaptively learns the importance of bimodal features without additional annotations, while the latter divides point clouds to achieve differential fusion of multi-modal features at different depths. This fusion module can enhance the representation ability of original features for different point sets and provide more comprehensive features by using the dual splicing strategy of concatenation and index connection. Additionally, considering point density as a feature and its negative correlation with depth, we build an Adaptive Threshold Generation Network (ATGN) to generate the depth threshold by extracting density information, which can divide point clouds more reasonably. Experiments on the KITTI dataset demonstrate the effectiveness and competitiveness of our proposed models.</td>
</tr>
<tr id="bib_Liu2023Apr" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Liu2023Apr,
  author = {Liu, Zhanwen and Cheng, Juanru and Fan, Jin and Lin, Shan and Wang, Yang and Zhao, Xiangmo},
  title = {Multi-Modal Fusion Based on Depth Adaptive Mechanism for 3D Object Detection},
  journal = {IEEE Transactions on Multimedia},
  publisher = {IEEE},
  year = {2023},
  pages = {1--11},
  url = {https://doi.org/10.1109/TMM.2023.3270638}
}
</pre></td>
</tr>
<tr id="Liu2023Apr" class="entry">
	<td>Liu, Z., Cheng, J., Fan, J., Lin, S., Wang, Y. and Zhao, X.</td>
	<td>Multi-Modal Fusion Based on Depth Adaptive Mechanism for 3D Object Detection <p class="infolinks">[<a href="javascript:toggleInfo('Liu2023Apr','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Liu2023Apr','bibtex')">BibTeX</a>]</p></td>
	<td>2023</td>
	<td>IEEE Transactions on Multimedia, pp. 1-11&nbsp;</td>
	<td>article</td>
	<td><a href="https://doi.org/10.1109/TMM.2023.3270638">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Liu2023Apr" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Lidars and cameras are critical sensors for 3D object detection in autonomous driving. Despite the increasing popularity of sensor fusion in this field, accurate and robust fusion methods are still under exploration due to non-homogenous representations. In this paper, we find that the complementary roles of point clouds and images vary with depth. An important reason is that the point cloud appearance changes significantly with increasing distance from the Lidar, while the image's edge, color, and texture information are not sensitive to depth. To address this, we propose a fusion module based on the Depth Attention Mechanism (DAM), which mainly consists of two operations: gated feature generation and point cloud division. The former adaptively learns the importance of bimodal features without additional annotations, while the latter divides point clouds to achieve differential fusion of multi-modal features at different depths. This fusion module can enhance the representation ability of original features for different point sets and provide more comprehensive features by using the dual splicing strategy of concatenation and index connection. Additionally, considering point density as a feature and its negative correlation with depth, we build an Adaptive Threshold Generation Network (ATGN) to generate the depth threshold by extracting density information, which can divide point clouds more reasonably. Experiments on the KITTI dataset demonstrate the effectiveness and competitiveness of our proposed models.</td>
</tr>
<tr id="bib_Liu2023Apr" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Liu2023Apr,
  author = {Liu, Zhanwen and Cheng, Juanru and Fan, Jin and Lin, Shan and Wang, Yang and Zhao, Xiangmo},
  title = {Multi-Modal Fusion Based on Depth Adaptive Mechanism for 3D Object Detection},
  journal = {IEEE Transactions on Multimedia},
  publisher = {IEEE},
  year = {2023},
  pages = {1--11},
  url = {https://doi.org/10.1109/TMM.2023.3270638}
}
</pre></td>
</tr>
<tr id="Liu2023Apr" class="entry">
	<td>Liu, Z., Cheng, J., Fan, J., Lin, S., Wang, Y. and Zhao, X.</td>
	<td>Multi-Modal Fusion Based on Depth Adaptive Mechanism for 3D Object Detection <p class="infolinks">[<a href="javascript:toggleInfo('Liu2023Apr','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Liu2023Apr','bibtex')">BibTeX</a>]</p></td>
	<td>2023</td>
	<td>IEEE Transactions on Multimedia, pp. 1-11&nbsp;</td>
	<td>article</td>
	<td><a href="https://doi.org/10.1109/TMM.2023.3270638">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Liu2023Apr" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Lidars and cameras are critical sensors for 3D object detection in autonomous driving. Despite the increasing popularity of sensor fusion in this field, accurate and robust fusion methods are still under exploration due to non-homogenous representations. In this paper, we find that the complementary roles of point clouds and images vary with depth. An important reason is that the point cloud appearance changes significantly with increasing distance from the Lidar, while the image's edge, color, and texture information are not sensitive to depth. To address this, we propose a fusion module based on the Depth Attention Mechanism (DAM), which mainly consists of two operations: gated feature generation and point cloud division. The former adaptively learns the importance of bimodal features without additional annotations, while the latter divides point clouds to achieve differential fusion of multi-modal features at different depths. This fusion module can enhance the representation ability of original features for different point sets and provide more comprehensive features by using the dual splicing strategy of concatenation and index connection. Additionally, considering point density as a feature and its negative correlation with depth, we build an Adaptive Threshold Generation Network (ATGN) to generate the depth threshold by extracting density information, which can divide point clouds more reasonably. Experiments on the KITTI dataset demonstrate the effectiveness and competitiveness of our proposed models.</td>
</tr>
<tr id="bib_Liu2023Apr" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Liu2023Apr,
  author = {Liu, Zhanwen and Cheng, Juanru and Fan, Jin and Lin, Shan and Wang, Yang and Zhao, Xiangmo},
  title = {Multi-Modal Fusion Based on Depth Adaptive Mechanism for 3D Object Detection},
  journal = {IEEE Transactions on Multimedia},
  publisher = {IEEE},
  year = {2023},
  pages = {1--11},
  url = {https://doi.org/10.1109/TMM.2023.3270638}
}
</pre></td>
</tr>
<tr id="Togelius2023Mar" class="entry">
	<td>Togelius, J. and Yannakakis, G.N.</td>
	<td>Choose Your Weapon: Survival Strategies for Depressed AI Academics <p class="infolinks">[<a href="javascript:toggleInfo('Togelius2023Mar','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Togelius2023Mar','bibtex')">BibTeX</a>]</p></td>
	<td>2023</td>
	<td>ArXiv e-prints&nbsp;</td>
	<td>article</td>
	<td><a href="https://doi.org/10.48550/arXiv.2304.06035">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Togelius2023Mar" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Are you an AI researcher at an academic institution? Are you anxious you are not coping with the current pace of AI advancements? Do you feel you have no (or very limited) access to the computational and human resources required for an AI research breakthrough? You are not alone; we feel the same way. A growing number of AI academics can no longer find the means and resources to compete at a global scale. This is a somewhat recent phenomenon, but an accelerating one, with private actors investing enormous compute resources into cutting edge AI research. Here, we discuss what you can do to stay competitive while remaining an academic. We also briefly discuss what universities and the private sector could do improve the situation, if they are so inclined. This is not an exhaustive list of strategies, and you may not agree with all of them, but it serves to start a discussion.</td>
</tr>
<tr id="bib_Togelius2023Mar" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Togelius2023Mar,
  author = {Togelius, Julian and Yannakakis, Georgios N.},
  title = {Choose Your Weapon: Survival Strategies for Depressed AI Academics},
  journal = {ArXiv e-prints},
  year = {2023},
  url = {https://doi.org/10.48550/arXiv.2304.06035}
}
</pre></td>
</tr>
<tr id="Sezgin2023May" class="entry">
	<td>Sezgin, F., Vriesman, D., Steinhauser, D., Lugner, R. and Brandmeier, T.</td>
	<td>Safe Autonomous Driving in Adverse Weather: Sensor Evaluation and Performance Monitoring <p class="infolinks">[<a href="javascript:toggleInfo('Sezgin2023May','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Sezgin2023May','bibtex')">BibTeX</a>]</p></td>
	<td>2023</td>
	<td>ArXiv e-prints&nbsp;</td>
	<td>article</td>
	<td><a href="https://doi.org/10.48550/arXiv.2305.01336">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Sezgin2023May" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: The vehicle's perception sensors radar, lidar and camera, which must work continuously and without restriction, especially with regard to automated/autonomous driving, can lose performance due to unfavourable weather conditions. This paper analyzes the sensor signals of these three sensor technologies under rain and fog as well as day and night. A data set of a driving test vehicle as an object target under different weather conditions was recorded in a controlled environment with adjustable, defined, and reproducible weather conditions. Based on the sensor performance evaluation, a method has been developed to detect sensor degradation, including determining the affected data areas and estimating how severe they are. Through this sensor monitoring, measures can be taken in subsequent algorithms to reduce the influences or to take them into account in safety and assistance systems to avoid malfunctions.</td>
</tr>
<tr id="bib_Sezgin2023May" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Sezgin2023May,
  author = {Sezgin, Fatih and Vriesman, Daniel and Steinhauser, Dagmar and Lugner, Robert and Brandmeier, Thomas},
  title = {Safe Autonomous Driving in Adverse Weather: Sensor Evaluation and Performance Monitoring},
  journal = {ArXiv e-prints},
  year = {2023},
  url = {https://doi.org/10.48550/arXiv.2305.01336}
}
</pre></td>
</tr>
<tr id="Huch2023May" class="entry">
	<td>Huch, S., Sauerbeck, F. and Betz, J.</td>
	<td>DeepSTEP -- Deep Learning-Based Spatio-Temporal End-To-End Perception for Autonomous Vehicles <p class="infolinks">[<a href="javascript:toggleInfo('Huch2023May','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Huch2023May','bibtex')">BibTeX</a>]</p></td>
	<td>2023</td>
	<td>ArXiv e-prints&nbsp;</td>
	<td>article</td>
	<td><a href="https://doi.org/10.48550/arXiv.2305.06820">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Huch2023May" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Autonomous vehicles demand high accuracy and robustness of perception algorithms. To develop efficient and scalable perception algorithms, the maximum information should be extracted from the available sensor data. In this work, we present our concept for an end-to-end perception architecture, named DeepSTEP. The deep learning-based architecture processes raw sensor data from the camera, LiDAR, and RaDAR, and combines the extracted data in a deep fusion network. The output of this deep fusion network is a shared feature space, which is used by perception head networks to fulfill several perception tasks, such as object detection or local mapping. DeepSTEP incorporates multiple ideas to advance state of the art: First, combining detection and localization into a single pipeline allows for efficient processing to reduce computational overhead and further improves overall performance. Second, the architecture leverages the temporal domain by using a self-attention mechanism that focuses on the most important features. We believe that our concept of DeepSTEP will advance the development of end-to-end perception systems. The network will be deployed on our research vehicle, which will be used as a platform for data collection, real-world testing, and validation. In conclusion, DeepSTEP represents a significant advancement in the field of perception for autonomous vehicles. The architecture's end-to-end design, time-aware attention mechanism, and integration of multiple perception tasks make it a promising solution for real-world deployment. This research is a work in progress and presents the first concept of establishing a novel perception pipeline.</td>
</tr>
<tr id="bib_Huch2023May" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Huch2023May,
  author = {Huch, Sebastian and Sauerbeck, Florian and Betz, Johannes},
  title = {DeepSTEP -- Deep Learning-Based Spatio-Temporal End-To-End Perception for Autonomous Vehicles},
  journal = {ArXiv e-prints},
  year = {2023},
  url = {https://doi.org/10.48550/arXiv.2305.06820}
}
</pre></td>
</tr>
<tr id="Gu2023May" class="entry">
	<td>Gu, J., Lind, A., Chhetri, T.R., Bellone, M. and Sell, R.</td>
	<td>End-to-End Multimodal Sensor Dataset Collection Framework for Autonomous Vehicles <p class="infolinks">[<a href="javascript:toggleInfo('Gu2023May','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Gu2023May','bibtex')">BibTeX</a>]</p></td>
	<td>2023</td>
	<td>Preprints&nbsp;</td>
	<td>article</td>
	<td><a href="https://doi.org/10.20944/preprints202305.1376.v1">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Gu2023May" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Autonomous driving vehicles rely on sensors for the robust perception of surroundings. Such vehicles are equipped with multiple perceptive sensors with a high level of redundancy to ensure safety and reliability in any driving condition. However, multi-sensor systems bring up the requirements related to sensor calibration and synchronization, which are the fundamental blocks of any autonomous system. On the other hand, sensor fusion and integration have become important aspects of autonomous driving research and directly determine the efficiency and accuracy of advanced functions such as object detection and path planning. Classical model-based estimation and data-driven models are two mainstream approaches to achieving such integration. Most recent research is shifting to the latter, showing high robustness in real-world applications but requiring large quantities of data to be collected, synchronized, and properly categorized. To generalize the implementation of the multi-sensor perceptive system, we introduce an end-to-end generic sensor dataset collection framework that includes both hardware deploying solutions and sensor fusion algorithms. The framework prototype combines camera and range sensors LiDAR and radar. Furthermore, we present a universal toolbox to calibrate and synchronize three types of sensors based on their characteristics. The framework also includes the fusion algorithms, which utilize the merits of three sensors, and fuse their sensory information in a manner that is helpful for object detection and tracking research. The generality of this framework makes it applicable in any robotic or autonomous applications, also suitable for quick and large-scale practical deployment.</td>
</tr>
<tr id="bib_Gu2023May" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Gu2023May,
  author = {Gu, Junyi and Lind, Artjom and Chhetri, Tek Raj and Bellone, Mauro and Sell, Raivo},
  title = {End-to-End Multimodal Sensor Dataset Collection Framework for Autonomous Vehicles},
  journal = {Preprints},
  publisher = {Preprints},
  year = {2023},
  url = {https://doi.org/10.20944/preprints202305.1376.v1}
}
</pre></td>
</tr>
<tr id="Cui2023May" class="entry">
	<td>Cui, C., Ma, Y., Lu, J. and Wang, Z.</td>
	<td>Radar Enlighten the Dark: Enhancing Low-Visibility Perception for Automated Vehicles with Camera-Radar Fusion <p class="infolinks">[<a href="javascript:toggleInfo('Cui2023May','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Cui2023May','bibtex')">BibTeX</a>]</p></td>
	<td>2023</td>
	<td>ArXiv e-prints&nbsp;</td>
	<td>article</td>
	<td><a href="https://doi.org/10.48550/arXiv.2305.17318">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Cui2023May" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Sensor fusion is a crucial augmentation technique for improving the accuracy and reliability of perception systems for automated vehicles under diverse driving conditions. However, adverse weather and low-light conditions remain challenging, where sensor performance degrades significantly, exposing vehicle safety to potential risks. Advanced sensors such as LiDARs can help mitigate the issue but with extremely high marginal costs. In this paper, we propose a novel transformer-based 3D object detection model "REDFormer" to tackle low visibility conditions, exploiting the power of a more practical and cost-effective solution by leveraging bird's-eye-view camera-radar fusion. Using the nuScenes dataset with multi-radar point clouds, weather information, and time-of-day data, our model outperforms state-of-the-art (SOTA) models on classification and detection accuracy. Finally, we provide extensive ablation studies of each model component on their contributions to address the above-mentioned challenges. Particularly, it is shown in the experiments that our model achieves a significant performance improvement over the baseline model in low-visibility scenarios, specifically exhibiting a 31.31% increase in rainy scenes and a 46.99% enhancement in nighttime scenes.The source code of this study is publicly available.</td>
</tr>
<tr id="bib_Cui2023May" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Cui2023May,
  author = {Cui, Can and Ma, Yunsheng and Lu, Juanwu and Wang, Ziran},
  title = {Radar Enlighten the Dark: Enhancing Low-Visibility Perception for Automated Vehicles with Camera-Radar Fusion},
  journal = {ArXiv e-prints},
  year = {2023},
  url = {https://doi.org/10.48550/arXiv.2305.17318}
}
</pre></td>
</tr>
<tr id="Chen2022Mar" class="entry">
	<td>Chen, X., Zhang, T., Wang, Y., Wang, Y. and Zhao, H.</td>
	<td>FUTR3D: A Unified Sensor Fusion Framework for 3D Detection <p class="infolinks">[<a href="javascript:toggleInfo('Chen2022Mar','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Chen2022Mar','bibtex')">BibTeX</a>]</p></td>
	<td>2022</td>
	<td>ArXiv e-prints&nbsp;</td>
	<td>article</td>
	<td><a href="https://doi.org/10.48550/arXiv.2203.10642">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Chen2022Mar" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Sensor fusion is an essential topic in many perception systems, such as autonomous driving and robotics. Existing multi-modal 3D detection models usually involve customized designs depending on the sensor combinations or setups. In this work, we propose the first unified end-to-end sensor fusion framework for 3D detection, named FUTR3D, which can be used in (almost) any sensor configuration. FUTR3D employs a query-based Modality-Agnostic Feature Sampler (MAFS), together with a transformer decoder with a set-to-set loss for 3D detection, thus avoiding using late fusion heuristics and post-processing tricks. We validate the effectiveness of our framework on various combinations of cameras, low-resolution LiDARs, high-resolution LiDARs, and Radars. On NuScenes dataset, FUTR3D achieves better performance over specifically designed methods across different sensor combinations. Moreover, FUTR3D achieves great flexibility with different sensor configurations and enables low-cost autonomous driving. For example, only using a 4-beam LiDAR with cameras, FUTR3D (58.0 mAP) achieves on par performance with state-of-the-art 3D detection model CenterPoint (56.6 mAP) using a 32-beam LiDAR.</td>
</tr>
<tr id="bib_Chen2022Mar" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Chen2022Mar,
  author = {Chen, Xuanyao and Zhang, Tianyuan and Wang, Yue and Wang, Yilun and Zhao, Hang},
  title = {FUTR3D: A Unified Sensor Fusion Framework for 3D Detection},
  journal = {ArXiv e-prints},
  year = {2022},
  url = {https://doi.org/10.48550/arXiv.2203.10642}
}
</pre></td>
</tr>
<tr id="Man2023" class="entry">
	<td>Man, Y., Gui, L.-Y. and Wang, Y.-X.</td>
	<td>BEV-Guided Multi-Modality Fusion for Driving Perception <p class="infolinks">[<a href="javascript:toggleInfo('Man2023','bibtex')">BibTeX</a>]</p></td>
	<td>2023</td>
	<td>, pp. 21960-21969&nbsp;</td>
	<td>misc</td>
	<td><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Man_BEV-Guided_Multi-Modality_Fusion_for_Driving_Perception_CVPR_2023_paper.html">URL</a>&nbsp;</td>
</tr>
<tr id="bib_Man2023" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@misc{Man2023,
  author = {Man, Yunze and Gui, Liang-Yan and Wang, Yu-Xiong},
  title = {BEV-Guided Multi-Modality Fusion for Driving Perception},
  year = {2023},
  pages = {21960--21969},
  note = {[Online; accessed 1. Jun. 2023]},
  url = {https://openaccess.thecvf.com/content/CVPR2023/html/Man_BEV-Guided_Multi-Modality_Fusion_for_Driving_Perception_CVPR_2023_paper.html}
}
</pre></td>
</tr>
<tr id="Ogunrinde2023May" class="entry">
	<td>Ogunrinde, I.O. and Bernadin, S.</td>
	<td>Deep Camera-Radar Fusion with Attention Framework for Autonomous Vehicle Vision in Foggy Weather Conditions <p class="infolinks">[<a href="javascript:toggleInfo('Ogunrinde2023May','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Ogunrinde2023May','bibtex')">BibTeX</a>]</p></td>
	<td>2023</td>
	<td>Preprints&nbsp;</td>
	<td>article</td>
	<td><a href="https://doi.org/10.20944/preprints202305.2180.v1">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Ogunrinde2023May" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: AVs suffer reduced maneuverability and performance due to the degradation in sensor performances in fog. Such degradation causes significant object detection errors essential for AVs' safety-critical conditions. For instance, YOLOv5 performs significantly well under favorable weather but suffers miss detections and false positives due to atmospheric scattering caused by fog particles. Existing deep object detection techniques often exhibit a high degree of accuracy. The drawback is being sluggish at object detection in fog. Object detection methods with fast detection speed have been obtained using deep learning at the expense of accuracy. The problem of the lack of balance between detection speed and accuracy in fog persist. This paper presents an improved YOLOv5-based multi-sensor fusion network that combines radar's object detection with a camera image bounding box. We transformed radar detection by mapping the radar detections into a two-dimensional image coordinate and projected the resultant radar image on the camera image. Using the attention mechanism, we emphasized and improved important feature representation used for object detection while reducing high-level feature information loss. We trained and tested our multi-sensor fusion network on clear and multi-fog weather datasets obtained from the CARLA simulator. Our result shows that the proposed method significantly enhances the detection of distant and small objects. Our small CR-YOLOnet model best strikes a balance between accuracy and speed with an accuracy of 0.849 at 69 fps.</td>
</tr>
<tr id="bib_Ogunrinde2023May" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Ogunrinde2023May,
  author = {Ogunrinde, Isaac Oluwadunsin and Bernadin, Shonda},
  title = {Deep Camera-Radar Fusion with Attention Framework for Autonomous Vehicle Vision in Foggy Weather Conditions},
  journal = {Preprints},
  publisher = {Preprints},
  year = {2023},
  url = {https://doi.org/10.20944/preprints202305.2180.v1}
}
</pre></td>
</tr>
<tr id="Zhang2022Jul" class="entry">
	<td>Zhang, C., Wang, H., Cai, Y., Chen, L., Li, Y., Sotelo, M.A. and Li, Z.</td>
	<td>Robust-FusionNet: Deep Multimodal Sensor Fusion for 3-D Object Detection Under Severe Weather Conditions <p class="infolinks">[<a href="javascript:toggleInfo('Zhang2022Jul','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Zhang2022Jul','bibtex')">BibTeX</a>]</p></td>
	<td>2022</td>
	<td>IEEE Transactions on Instrumentation and Measurement<br/>Vol. 71, pp. 1-13&nbsp;</td>
	<td>article</td>
	<td><a href="https://doi.org/10.1109/TIM.2022.3191724">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Zhang2022Jul" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: The LiDAR point cloud data and camera images are distorted to a different degree under various severe weather conditions. Due to this, the traditional single-modal object detection methods are unable to use the complementary information between different sensors. Consequently, these algorithms are unable to address various issues caused by severe weather conditions. Recently, the multimodal data fusion methods are applied to the road object detection under severe weather conditions. However, the multimodal algorithms suffer from low data alignment accuracy and the inability to suppress the changes in exposure under severe weather conditions. In this work, we propose a new multimodal sensor fusion object detection network. The proposed network effectively overcomes the shortcomings caused by the camera and LiDAR distortions in severe weather conditions and achieves robust environment perception. We propose: 1) pointwise aligned data fusion method based on K-means++ clustering to improve the accuracy of data alignment; 2) implicit feature pyramid network (i-FPN) to fuse the image features for suppressing the distortions caused by the changes in exposure; and 3) hybrid attention mechanism (HAM) to deal with the fusion features and improve the adaptability toward different working conditions. We perform experiments using the One Million Scenes for Autonomous Driving (ONCE) and Karlsruhe Institute of Technology and Toyota Technological Institute (KITTI) datasets. The experimental results and analysis show that the proposed method effectively improves the performance of multimodal deep fusion network under both clear and severe weather conditions.</td>
</tr>
<tr id="bib_Zhang2022Jul" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Zhang2022Jul,
  author = {Zhang, Cheng and Wang, Hai and Cai, Yingfeng and Chen, Long and Li, Yicheng and Sotelo, Miguel Angel and Li, Zhixiong},
  title = {Robust-FusionNet: Deep Multimodal Sensor Fusion for 3-D Object Detection Under Severe Weather Conditions},
  journal = {IEEE Transactions on Instrumentation and Measurement},
  publisher = {IEEE},
  year = {2022},
  volume = {71},
  pages = {1--13},
  url = {https://doi.org/10.1109/TIM.2022.3191724}
}
</pre></td>
</tr>
<tr id="Srivastav2023Jun" class="entry">
	<td>Srivastav, A. and Mandal, S.</td>
	<td>Radars for Autonomous Driving: A Review of Deep Learning Methods and Challenges <p class="infolinks">[<a href="javascript:toggleInfo('Srivastav2023Jun','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Srivastav2023Jun','bibtex')">BibTeX</a>]</p></td>
	<td>2023</td>
	<td>ArXiv e-prints&nbsp;</td>
	<td>article</td>
	<td><a href="https://doi.org/10.48550/arXiv.2306.09304">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Srivastav2023Jun" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Radar is a key component of the suite of perception sensors used for safe and reliable navigation of autonomous vehicles. Its unique capabilities include high-resolution velocity imaging, detection of agents in occlusion and over long ranges, and robust performance in adverse weather conditions. However, the usage of radar data presents some challenges: it is characterized by low resolution, sparsity, clutter, high uncertainty, and lack of good datasets. These challenges have limited radar deep learning research. As a result, current radar models are often influenced by lidar and vision models, which are focused on optical features that are relatively weak in radar data, thus resulting in under-utilization of radar's capabilities and diminishing its contribution to autonomous perception. This review seeks to encourage further deep learning research on autonomous radar data by 1) identifying key research themes, and 2) offering a comprehensive overview of current opportunities and challenges in the field. Topics covered include early and late fusion, occupancy flow estimation, uncertainty modeling, and multipath detection. The paper also discusses radar fundamentals and data representation, presents a curated list of recent radar datasets, and reviews state-of-the-art lidar and vision models relevant for radar research. For a summary of the paper and more results, visit the website: this http URL.</td>
</tr>
<tr id="bib_Srivastav2023Jun" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Srivastav2023Jun,
  author = {Srivastav, Arvind and Mandal, Soumyajit},
  title = {Radars for Autonomous Driving: A Review of Deep Learning Methods and Challenges},
  journal = {ArXiv e-prints},
  year = {2023},
  url = {https://doi.org/10.48550/arXiv.2306.09304}
}
</pre></td>
</tr>
<tr id="Sun2023Jul" class="entry">
	<td>Sun, H., Feng, H., Stettinger, G., Servadei, L. and Wille, R.</td>
	<td>Multi-Task Cross-Modality Attention-Fusion for 2D Object Detection <p class="infolinks">[<a href="javascript:toggleInfo('Sun2023Jul','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Sun2023Jul','bibtex')">BibTeX</a>]</p></td>
	<td>2023</td>
	<td>arXiv&nbsp;</td>
	<td>article</td>
	<td><a href="https://arxiv.org/abs/2307.08339v1">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Sun2023Jul" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Accurate and robust object detection is critical for autonomous driving. Image-based detectors face difficulties caused by low visibility in adverse weather conditions. Thus, radar-camera fusion is of particular interest but presents challenges in optimally fusing heterogeneous data sources. To approach this issue, we propose two new radar preprocessing techniques to better align radar and camera data. In addition, we introduce a Multi-Task Cross-Modality Attention-Fusion Network (MCAF-Net) for object detection, which includes two new fusion blocks. These allow for exploiting information from the feature maps more comprehensively. The proposed algorithm jointly detects objects and segments free space, which guides the model to focus on the more relevant part of the scene, namely, the occupied space. Our approach outperforms current state-of-the-art radar-camera fusion-based object detectors in the nuScenes dataset and achieves more robust results in adverse weather conditions and nighttime scenarios.</td>
</tr>
<tr id="bib_Sun2023Jul" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Sun2023Jul,
  author = {Sun, Huawei and Feng, Hao and Stettinger, Georg and Servadei, Lorenzo and Wille, Robert},
  title = {Multi-Task Cross-Modality Attention-Fusion for 2D Object Detection},
  journal = {arXiv},
  year = {2023},
  url = {https://arxiv.org/abs/2307.08339v1}
}
</pre></td>
</tr>
<tr id="Guan2023Jul" class="entry">
	<td>Guan, R., Yao, S., Zhu, X., Man, K.L., Lim, E.G., Smith, J., Yue, Y. and Yue, Y.</td>
	<td>Achelous: A Fast Unified Water-surface Panoptic Perception Framework based on Fusion of Monocular Camera and 4D mmWave Radar <p class="infolinks">[<a href="javascript:toggleInfo('Guan2023Jul','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Guan2023Jul','bibtex')">BibTeX</a>]</p></td>
	<td>2023</td>
	<td>arXiv&nbsp;</td>
	<td>article</td>
	<td><a href="https://arxiv.org/abs/2307.07102v1">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Guan2023Jul" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Current perception models for different tasks usually exist in modular forms on Unmanned Surface Vehicles (USVs), which infer extremely slowly in parallel on edge devices, causing the asynchrony between perception results and USV position, and leading to error decisions of autonomous navigation. Compared with Unmanned Ground Vehicles (UGVs), the robust perception of USVs develops relatively slowly. Moreover, most current multi-task perception models are huge in parameters, slow in inference and not scalable. Oriented on this, we propose Achelous, a low-cost and fast unified panoptic perception framework for water-surface perception based on the fusion of a monocular camera and 4D mmWave radar. Achelous can simultaneously perform five tasks, detection and segmentation of visual targets, drivable-area segmentation, waterline segmentation and radar point cloud segmentation. Besides, models in Achelous family, with less than around 5 million parameters, achieve about 18 FPS on an NVIDIA Jetson AGX Xavier, 11 FPS faster than HybridNets, and exceed YOLOX-Tiny and Segformer-B0 on our collected dataset about 5 mAP_50-95 and 0.7 mIoU, especially under situations of adverse weather, dark environments and camera failure. To our knowledge, Achelous is the first comprehensive panoptic perception framework combining vision-level and point-cloud-level tasks for water-surface perception. To promote the development of the intelligent transportation community, we release our codes in ifmmodebackslashelse&bsol;&#64257;urllbracethis https URLrbrace.</td>
</tr>
<tr id="bib_Guan2023Jul" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Guan2023Jul,
  author = {Guan, Runwei and Yao, Shanliang and Zhu, Xiaohui and Man, Ka Lok and Lim, Eng Gee and Smith, Jeremy and Yue, Yong and Yue, Yutao},
  title = {Achelous: A Fast Unified Water-surface Panoptic Perception Framework based on Fusion of Monocular Camera and 4D mmWave Radar},
  journal = {arXiv},
  year = {2023},
  url = {https://arxiv.org/abs/2307.07102v1}
}
</pre></td>
</tr>
<tr id="Liu2023Jul" class="entry">
	<td>Liu, L., Zhi, S., Du, Z., Liu, L., Zhang, X., Huo, K. and Jiang, W.</td>
	<td>ROFusion: Efficient Object Detection using Hybrid Point-wise Radar-Optical Fusion <p class="infolinks">[<a href="javascript:toggleInfo('Liu2023Jul','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Liu2023Jul','bibtex')">BibTeX</a>]</p></td>
	<td>2023</td>
	<td>arXiv&nbsp;</td>
	<td>article</td>
	<td><a href="https://arxiv.org/abs/2307.08233v1">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Liu2023Jul" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Radars, due to their robustness to adverse weather conditions and ability to measure object motions, have served in autonomous driving and intelligent agents for years. However, Radar-based perception suffers from its unintuitive sensing data, which lack of semantic and structural information of scenes. To tackle this problem, camera and Radar sensor fusion has been investigated as a trending strategy with low cost, high reliability and strong maintenance. While most recent works explore how to explore Radar point clouds and images, rich contextual information within Radar observation are discarded. In this paper, we propose a hybrid point-wise Radar-Optical fusion approach for object detection in autonomous driving scenarios. The framework benefits from dense contextual information from both the range-doppler spectrum and images which are integrated to learn a multi-modal feature representation. Furthermore, we propose a novel local coordinate formulation, tackling the object detection task in an object-centric coordinate. Extensive results show that with the information gained from optical images, we could achieve leading performance in object detection (97.69ifmmodebackslashelse&bsol;&#64257;% recall) compared to recent state-of-the-art methods FFT-RadNet (82.86ifmmodebackslashelse&bsol;&#64257;% recall). Ablation studies verify the key design choices and practicability of our approach given machine generated imperfect detections. The code will be available at this https URL.</td>
</tr>
<tr id="bib_Liu2023Jul" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Liu2023Jul,
  author = {Liu, Liu and Zhi, Shuaifeng and Du, Zhenhua and Liu, Li and Zhang, Xinyu and Huo, Kai and Jiang, Weidong},
  title = {ROFusion: Efficient Object Detection using Hybrid Point-wise Radar-Optical Fusion},
  journal = {arXiv},
  year = {2023},
  url = {https://arxiv.org/abs/2307.08233v1}
}
</pre></td>
</tr>
</tbody>
</table>
<footer>
 <small>Created by <a href="http://jabref.sourceforge.net">JabRef</a> on 24/07/2023.</small>
</footer>
<!-- file generated by JabRef -->
</body>
</html>