@Article{Macenski2022May,
  author    = {Macenski, Steven and Foote, Tully and Gerkey, Brian and Lalancette, Chris and Woodall, William},
  journal   = {Sci. Rob.},
  title     = {{Robot Operating System 2: Design, architecture, and uses in the wild}},
  year      = {2022},
  issn      = {2470-9476},
  month     = may,
  number    = {66},
  pages     = {eabm6074},
  volume    = {7},
  abstract  = {{The next chapter of the robotics revolution is well underway with the deployment of robots for a broad range of commercial use cases. Even in a myriad of applications and environments, there exists a common vocabulary of components that robots share{\ifmmode---\else\textemdash\fi}the need for a modular, scalable, and reliable architecture; sensing; planning; mobility; and autonomy. The Robot Operating System (ROS) was an integral part of the last chapter, demonstrably expediting robotics research with freely available components and a modular framework. However, ROS 1 was not designed with many necessary production-grade features and algorithms. ROS 2 and its related projects have been redesigned from the ground up to meet the challenges set forth by modern robotic systems in new and exploratory domains at all scales. In this Review, we highlight the philosophical and architectural changes of ROS 2 powering this new chapter in the robotics revolution. We also show through case studies the influence ROS 2 and its adoption has had on accelerating real robot systems to reliable deployment in an assortment of challenging environments.}},
  citation  = {43},
  groups    = {Robotics},
  publisher = {American Association for the Advancement of Science},
  url       = {https://doi.org/10.1126/scirobotics.abm6074},
}

@Article{Fayyad2020Jul,
  author           = {Fayyad, Jamil and Jaradat, Mohammad A. and Gruyer, Dominique and Najjaran, Homayoun},
  journal          = {Sensors},
  title            = {{Deep Learning Sensor Fusion for Autonomous Vehicle Perception and Localization: A Review}},
  year             = {2020},
  issn             = {1424-8220},
  month            = jul,
  number           = {15},
  pages            = {4220},
  volume           = {20},
  abstract         = {{Autonomous vehicles (AV) are expected to improve, reshape, and revolutionize the future of ground transportation. It is anticipated that ordinary vehicles will one day be replaced with smart vehicles that are able to make decisions and perform driving tasks on their own. In order to achieve this objective, self-driving vehicles are equipped with sensors that are used to sense and perceive both their surroundings and the faraway environment, using further advances in communication technologies, such as 5G. In the meantime, local perception, as with human beings, will continue to be an effective means for controlling the vehicle at short range. In the other hand, extended perception allows for anticipation of distant events and produces smarter behavior to guide the vehicle to its destination while respecting a set of criteria (safety, energy management, traffic optimization, comfort). In spite of the remarkable advancements of sensor technologies in terms of their effectiveness and applicability for AV systems in recent years, sensors can still fail because of noise, ambient conditions, or manufacturing defects, among other factors; hence, it is not advisable to rely on a single sensor for any of the autonomous driving tasks. The practical solution is to incorporate multiple competitive and complementary sensors that work synergistically to overcome their individual shortcomings. This article provides a comprehensive review of the state-of-the-art methods utilized to improve the performance of AV systems in short-range or local vehicle environments. Specifically, it focuses on recent studies that use deep learning sensor fusion algorithms for perception, localization, and mapping. The article concludes by highlighting some of the current trends and possible future research directions. Keywords: autonomous vehicles; self-driving cars; deep learning; sensor fusion; perception; localization and mapping}},
  citation         = {177},
  file             = {:/home/kvnptl/HBRS/semester_3/advance_scientific_writing/papers/survey/Deep learning sensor fusion for autonomous vehicle_221212_160934.pdf:PDF},
  groups           = {Review, Sensor Fusion},
  keywords         = {autonomous vehicles, self-driving cars, deep learning, sensor fusion, perception, localization and mapping},
  modificationdate = {2023-04-02T17:42:50},
  priority         = {prio1},
  publisher        = {Multidisciplinary Digital Publishing Institute},
  readstatus       = {read},
  url              = {https://www.mdpi.com/1424-8220/20/15/4220},
}

@Article{Nobis2020May,
  author           = {Nobis, Felix and Geisslinger, Maximilian and Weber, Markus and Betz, Johannes and Lienkamp, Markus},
  journal          = {arXiv},
  title            = {{A Deep Learning-based Radar and Camera Sensor Fusion Architecture for Object Detection}},
  year             = {2020},
  month            = may,
  abstract         = {{Object detection in camera images, using deep learning has been proven successfully in recent years. Rising detection rates and computationally efficient network structures are pushing this technique towards application in production vehicles. Nevertheless, the sensor quality of the camera is limited in severe weather conditions and through increased sensor noise in sparsely lit areas and at night. Our approach enhances current 2D object detection networks by fusing camera data and projected sparse radar data in the network layers. The proposed CameraRadarFusionNet (CRF-Net) automatically learns at which level the fusion of the sensor data is most beneficial for the detection result. Additionally, we introduce BlackIn, a training strategy inspired by Dropout, which focuses the learning on a specific sensor type. We show that the fusion network is able to outperform a state-of-the-art image-only network for two different datasets. The code for this research will be made available to the public at: this https URL.}},
  citation         = {181},
  file             = {:/home/kvnptl/HBRS/semester_3/advance_scientific_writing/papers/techniques/A deep learning-based radar and camera sensor _230109_182903.pdf:PDF},
  groups           = {Camera-Radar fusion, Sensor Fusion},
  keywords         = {Computer Vision and Pattern Recognition (cs.CV)},
  modificationdate = {2023-04-04T00:24:08},
  priority         = {prio1},
  ranking          = {rank4},
  readstatus       = {read},
  url              = {https://arxiv.org/abs/2005.07431v1},
}

@Article{10.1162/neco_a_01273,
  author           = {Gao, Jing and Li, Peng and Chen, Zhikui and Zhang, Jianing},
  journal          = {Neural Computation},
  title            = {{A Survey on Deep Learning for Multimodal Data Fusion}},
  year             = {2020},
  issn             = {0899-7667},
  month            = {05},
  number           = {5},
  pages            = {829-864},
  volume           = {32},
  abstract         = {{With the wide deployments of heterogeneous networks, huge amounts of data with characteristics of high volume, high variety, high velocity, and high veracity are generated. These data, referred to multimodal big data, contain abundant intermodality and cross-modality information and pose vast challenges on traditional data fusion methods. In this review, we present some pioneering deep learning models to fuse these multimodal big data. With the increasing exploration of the multimodal big data, there are still some challenges to be addressed. Thus, this review presents a survey on deep learning for multimodal data fusion to provide readers, regardless of their original community, with the fundamentals of multimodal deep learning fusion method and to motivate new multimodal data fusion techniques of deep learning. Specifically, representative architectures that are widely used are summarized as fundamental to the understanding of multimodal deep learning. Then the current pioneering multimodal data fusion deep learning models are summarized. Finally, some challenges and future topics of multimodal data fusion deep learning models are described.}},
  citation         = {239},
  groups           = {Survey, Sensor Fusion},
  modificationdate = {2023-04-02T17:09:40},
  priority         = {prio2},
  url              = {https://direct.mit.edu/neco/article/32/5/829/95591/A-Survey-on-Deep-Learning-for-Multimodal-Data},
}

@Article{Chung2019Apr,
  author           = {Chung, Seungeun and Lim, Jiyoun and Noh, Kyoung Ju and Kim, Gague and Jeong, Hyuntae},
  journal          = {Sensors},
  title            = {{Sensor Data Acquisition and Multimodal Sensor Fusion for Human Activity Recognition Using Deep Learning}},
  year             = {2019},
  issn             = {1424-8220},
  month            = apr,
  number           = {7},
  pages            = {1716},
  volume           = {19},
  abstract         = {{In this paper, we perform a systematic study about the on-body sensor positioning and data acquisition details for Human Activity Recognition (HAR) systems. We build a testbed that consists of eight body-worn Inertial Measurement Units (IMU) sensors and an Android mobile device for activity data collection. We develop a Long Short-Term Memory (LSTM) network framework to support training of a deep learning model on human activity data, which is acquired in both real-world and controlled environments. From the experiment results, we identify that activity data with sampling rate as low as 10 Hz from four sensors at both sides of wrists, right ankle, and waist is sufficient in recognizing Activities of Daily Living (ADLs) including eating and driving activity. We adopt a two-level ensemble model to combine class-probabilities of multiple sensor modalities, and demonstrate that a classifier-level sensor fusion technique can improve the classification performance. By analyzing the accuracy of each sensor on different types of activity, we elaborate custom weights for multimodal sensor fusion that reflect the characteristic of individual activities. Keywords: mobile sensing; sensor position; human activity recognition; multimodal sensor fusion; classifier-level ensemble; Long Short-Term Memory network; deep learning}},
  citation         = {110},
  groups           = {Misc fusion, Sensor Fusion},
  keywords         = {mobile sensing, sensor position, human activity recognition, multimodal sensor fusion, classifier-level ensemble, Long Short-Term Memory network, deep learning},
  modificationdate = {2023-01-01T22:34:36},
  publisher        = {Multidisciplinary Digital Publishing Institute},
  url              = {https://www.mdpi.com/1424-8220/19/7/1716},
}

@InCollection{Ge,
  author           = {Ge, Chenjie and Gu, Irene Yu-Hua and Jakola, Asgeir Store and Yang, Jie},
  booktitle        = {{2018 40th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)}},
  publisher        = {IEEE},
  title            = {{Deep Learning and Multi-Sensor Fusion for Glioma Classification Using Multistream 2D Convolutional Networks}},
  year             = {2018},
  pages            = {18--21},
  abstract         = {{This paper addresses issues of brain tumor, glioma, grading from multi-sensor images. Different types of scanners (or sensors) like enhanced T1-MRI, T2-MRI and FLAIR, show different contrast and are sensitive to different brain tissues and fluid regions. Most existing works use 3D brain images from single sensor. In this paper, we propose a novel multistream deep Convolutional Neural Network (CNN) architecture that extracts and fuses the features from multiple sensors for glioma tumor grading/subcategory grading. The main contributions of the paper are: (a) propose a novel multistream deep CNN architecture for glioma grading; (b) apply sensor fusion from T1-MRI, T2-MRI and/or FLAIR for enhancing performance through feature aggregation; (c) mitigate overfitting by using 2D brain image slices in combination with 2D image augmentation. Two datasets were used for our experiments, one for classifying low/high grade gliomas, another for classifying glioma with/without 1p19q codeletion. Experiments using the proposed scheme have shown good results (with test accuracy of 90.87{\%} for former case, and 89.39 {\%} for the latter case). Comparisons with several existing methods have provided further support to the proposed scheme. keywords: brain tumor classification, glioma, 1p19q codeletion, glioma grading, deep learning, multi-stream convolutional neural networks, sensor fusion, T1-MR image, T2-MR image, FLAIR.}},
  citation         = {71},
  groups           = {Misc fusion, Sensor Fusion},
  journal          = {Published in: 2018 40th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)},
  modificationdate = {2023-01-01T22:34:36},
  url              = {https://ieeexplore.ieee.org/abstract/document/8513556?casa_token=Eyoe0IwA5YEAAAAA:eoRdQl19Zvhfj2dTAI_q3y2DUR1Gq95Q8GOxv5MJ8w_XlsEEI3tjl9wbTF4EKWPvoPVqY163AWw},
}

@Article{Vielzeuf2018Oct,
  author           = {Vielzeuf, Valentin and Lechervy, Alexis and Pateux, St{\ifmmode\acute{e}\else\'{e}\fi}phane and Jurie, Fr{\ifmmode\acute{e}\else\'{e}\fi}d{\ifmmode\acute{e}\else\'{e}\fi}ric},
  journal          = {IEEE Sens. Lett.},
  title            = {{Multilevel Sensor Fusion With Deep Learning}},
  year             = {2018},
  issn             = {2475-1472},
  month            = oct,
  number           = {1},
  pages            = {ArticleSequenceNumber:7100304},
  volume           = {3},
  abstract         = {{In the context of deep learning, this article presents an original deep network, namely CentralNet, for the fusion of information coming from different sensors. This approach is designed to efficiently and automatically balance the tradeoff between early and late fusion (i.e., between the fusion of low-level versus high-level information). More specifically, at each level of abstraction{\ifmmode---\else\textemdash\fi}the different levels of deep networks{\ifmmode---\else\textemdash\fi}unimodal representations of the data are fed to a central neural network which combines them into a common embedding. In addition, a multiobjective regularization is also introduced, helping to both optimize the central network and the unimodal networks. Experiments on four multimodal datasets not only show the state-of-the-art performance but also demonstrate that CentralNet can actually choose the best possible fusion strategy for a given problem.}},
  citation         = {20},
  groups           = {Sensor Fusion},
  modificationdate = {2023-01-01T22:34:36},
  publisher        = {IEEE},
  url              = {https://ieeexplore.ieee.org/abstract/document/8516399?casa_token=XrtIQITSUOwAAAAA:CU4QWaWg-ERJkzwLaJBL7v110QCZ0oFotPxeBr4Qxfyl03poLraDU7_ALcLFG8u8--mCHD9nNfE},
}

@Article{Huang2021Aug,
  author           = {Huang, Pao-Ming and Lee, Ching-Hung},
  journal          = {Sensors},
  title            = {{Estimation of Tool Wear and Surface Roughness Development Using Deep Learning and Sensors Fusion}},
  year             = {2021},
  issn             = {1424-8220},
  month            = aug,
  number           = {16},
  pages            = {5338},
  volume           = {21},
  abstract         = {{This paper proposes an estimation approach for tool wear and surface roughness using deep learning and sensor fusion. The one-dimensional convolutional neural network (1D-CNN) is utilized as the estimation model with X- and Y-coordinate vibration signals and sound signal fusion using sensor influence analysis. First, machining experiments with computer numerical control (CNC) parameters are designed using a uniform experimental design (UED) method to guarantee the variety of collected data. The vibration, sound, and spindle current signals are collected and labeled according to the machining parameters. To speed up the degree of tool wear, an accelerated experiment is designed, and the corresponding tool wear and surface roughness are measured. An influential sensor selection analysis is proposed to preserve the estimation accuracy and to minimize the number of sensors. After sensor selection analysis, the sensor signals with better estimation capability are selected and combined using the sensor fusion method. The proposed estimation system combined with sensor selection analysis performs well in terms of accuracy and computational effort. Finally, the proposed approach is applied for on-line monitoring of tool wear with an alarm, which demonstrates the effectiveness of our approach. Keywords: deep learning; vibration; sound; fusion; tool wear; surface roughness; convolution neural network}},
  citation         = {10},
  groups           = {Misc fusion, Sensor Fusion},
  keywords         = {deep learning, vibration, sound, fusion, tool wear, surface roughness, convolution neural network},
  modificationdate = {2023-01-01T22:34:36},
  publisher        = {Multidisciplinary Digital Publishing Institute},
  url              = {https://www.mdpi.com/1424-8220/21/16/5338},
}

@Article{Wang2018Sep,
  author           = {Wang, Weiming and Chen, Biao and Xia, Peng and Hu, Jie and Peng, Yinghong},
  journal          = {Artif. Organs},
  title            = {{Sensor Fusion for Myoelectric Control Based on Deep Learning With Recurrent Convolutional Neural Networks}},
  year             = {2018},
  issn             = {0160-564X},
  month            = sep,
  number           = {9},
  pages            = {E272--E282},
  volume           = {42},
  abstract         = {{Electromyogram (EMG) signal decoding is the essential part of myoelectric control. However, traditional machine learning methods lack the capability of learning and expressing the information contained in EMG signals, and the robustness of the myoelectric control system is not sufficient for real life applications. In this article, a novel model based on recurrent convolutional neural networks (RCNNs) is proposed for hand movement classification and tested on the noninvasive EMG dataset. The proposed model uses deep architecture, which has advantages of dealing with complex time-series data, such as EMG signals. Transfer learning is used in the training of multimodal model. The classification performance is compared with support vector machine (SVM) and convolutional neural networks (CNNs) on the same dataset. To improve the adaptability to the effect of arm movements, we fused the EMG signals and acceleration data that are the multimodal input of the model. The parameter transferring of deep neural networks is used to accelerate the training process and avoid over-fitting. The experimental results show that time domain input and 1-dimensional convolution have higher accuracy in the RCNN model. Compared with SVM and CNNs, the proposed model has higher classification accuracy. Sensor fusion can improve the model performance in the condition of arm movements. The RCNN model is a promising decoder of EMG and the sensor fusion can increase the accuracy and robustness of the myoelectric control system.}},
  citation         = {32},
  groups           = {Misc fusion, Sensor Fusion},
  keywords         = {Electromyogram, Deep learning, Recurrent convolutional neural networks, Transfer learning, Sensor fusion},
  modificationdate = {2023-01-01T22:34:36},
  publisher        = {John Wiley {\&} Sons, Ltd},
  url              = {https://doi.org/10.1111/aor.13153},
}

@Article{Xu2017Nov,
  author           = {Xu, Danfei and Anguelov, Dragomir and Jain, Ashesh},
  journal          = {arXiv},
  title            = {{PointFusion: Deep Sensor Fusion for 3D Bounding Box Estimation}},
  year             = {2017},
  month            = nov,
  abstract         = {{We present PointFusion, a generic 3D object detection method that leverages both image and 3D point cloud information. Unlike existing methods that either use multi-stage pipelines or hold sensor and dataset-specific assumptions, PointFusion is conceptually simple and application-agnostic. The image data and the raw point cloud data are independently processed by a CNN and a PointNet architecture, respectively. The resulting outputs are then combined by a novel fusion network, which predicts multiple 3D box hypotheses and their confidences, using the input 3D points as spatial anchors. We evaluate PointFusion on two distinctive datasets: the KITTI dataset that features driving scenes captured with a lidar-camera setup, and the SUN-RGBD dataset that captures indoor environments with RGB-D cameras. Our model is the first one that is able to perform better or on-par with the state-of-the-art on these diverse datasets without any dataset-specific model tuning.}},
  citation         = {484},
  eprint           = {1711.10871},
  groups           = {Camera-Lidar fusion, Sensor Fusion, 3D},
  keywords         = {Computer Vision and Pattern Recognition (cs.CV)},
  modificationdate = {2023-04-04T09:55:06},
  url              = {https://arxiv.org/abs/1711.10871v2},
}

@Article{Liang2020Dec,
  author           = {Liang, Ming and Yang, Bin and Chen, Yun and Hu, Rui and Urtasun, Raquel},
  journal          = {arXiv},
  title            = {{Multi-Task Multi-Sensor Fusion for 3D Object Detection}},
  year             = {2020},
  month            = dec,
  abstract         = {{In this paper we propose to exploit multiple related tasks for accurate multi-sensor 3D object detection. Towards this goal we present an end-to-end learnable architecture that reasons about 2D and 3D object detection as well as ground estimation and depth completion. Our experiments show that all these tasks are complementary and help the network learn better representations by fusing information at various levels. Importantly, our approach leads the KITTI benchmark on 2D, 3D and BEV object detection, while being real time.}},
  citation         = {474},
  eprint           = {2012.12397},
  groups           = {Camera-Lidar fusion, Sensor Fusion, 3D},
  keywords         = {Computer Vision and Pattern Recognition (cs.CV)},
  modificationdate = {2023-04-04T09:54:33},
  url              = {https://arxiv.org/abs/2012.12397v1},
}

@Article{Yeong2021,
  author           = {Yeong, De Jong and Velasco-Hernandez, Gustavo and Barry, John and Walsh, Joseph},
  journal          = {Sensors},
  title            = {{Sensor and Sensor Fusion Technology in Autonomous Vehicles: A Review}},
  year             = {2021},
  issn             = {1424-8220},
  month            = mar,
  number           = {6},
  pages            = {2140},
  volume           = {21},
  abstract         = {{With the significant advancement of sensor and communication technology and the reliable application of obstacle detection techniques and algorithms, automated driving is becoming a pivotal technology that can revolutionize the future of transportation and mobility. Sensors are fundamental to the perception of vehicle surroundings in an automated driving system, and the use and performance of multiple integrated sensors can directly determine the safety and feasibility of automated driving vehicles. Sensor calibration is the foundation block of any autonomous system and its constituent sensors and must be performed correctly before sensor fusion and obstacle detection processes may be implemented. This paper evaluates the capabilities and the technical performance of sensors which are commonly employed in autonomous vehicles, primarily focusing on a large selection of vision cameras, LiDAR sensors, and radar sensors and the various conditions in which such sensors may operate in practice. We present an overview of the three primary categories of sensor calibration and review existing open-source calibration packages for multi-sensor calibration and their compatibility with numerous commercial sensors. We also summarize the three main approaches to sensor fusion and review current state-of-the-art multi-sensor fusion techniques and algorithms for object detection in autonomous driving applications. The current paper, therefore, provides an end-to-end review of the hardware and software methods required for sensor fusion object detection. We conclude by highlighting some of the challenges in the sensor fusion field and propose possible future research directions for automated driving systems. Keywords: autonomous vehicles; self-driving cars; perception; camera; lidar; radar; sensor fusion; calibration; obstacle detection}},
  citation         = {264},
  file             = {:/home/kvnptl/HBRS/semester_3/advance_scientific_writing/papers/survey/Sensor and sensor fusion technology in autonomous _221128_001041.pdf:PDF},
  groups           = {Review, Sensor Fusion},
  keywords         = {autonomous vehicles, self-driving cars, perception, camera, lidar, radar, sensor fusion, calibration, obstacle detection},
  modificationdate = {2023-04-02T17:42:16},
  priority         = {prio1},
  publisher        = {Multidisciplinary Digital Publishing Institute},
  readstatus       = {read},
  url              = {https://www.mdpi.com/1424-8220/21/6/2140},
}

@Article{Wang2019Dec,
  author           = {Wang, Zhangjing and Wu, Yu and Niu, Qingqing},
  journal          = {IEEE Access},
  title            = {{Multi-Sensor Fusion in Automated Driving: A Survey}},
  year             = {2019},
  issn             = {2169-3536},
  month            = dec,
  pages            = {2847--2868},
  volume           = {8},
  abstract         = {{With the significant development of practicability in deep learning and the ultra-high-speed information transmission rate of 5G communication technology will overcome the barrier of data transmission on the Internet of Vehicles, automated driving is becoming a pivotal technology affecting the future industry. Sensors are the key to the perception of the outside world in the automated driving system and whose cooperation performance directly determines the safety of automated driving vehicles. In this survey, we mainly discuss the different strategies of multi-sensor fusion in automated driving in recent years. The performance of conventional sensors and the necessity of multi-sensor fusion are analyzed, including radar, LiDAR, camera, ultrasonic, GPS, IMU, and V2X. According to the differences in the latest studies, we divide the fusion strategies into four categories and point out some shortcomings. Sensor fusion is mainly applied for multi-target tracking and environment reconstruction. We discuss the method of establishing a motion model and data association in multi-target tracking. At the end of the paper, we analyzed the deficiencies in the current studies and put forward some suggestions for further improvement in the future. Through this investigation, we hope to analyze the current situation of multi-sensor fusion in the automated driving process and provide more efficient and reliable fusion strategies.}},
  citation         = {197},
  groups           = {Survey, Sensor Fusion},
  keywords         = {Sensor fusion, Sensor systems, Sensor phenomena and characterization, Cameras, Laser radar, 5G mobile communication, V2X, Automated driving},
  modificationdate = {2023-04-02T17:10:22},
  priority         = {prio3},
  publisher        = {IEEE},
  url              = {https://ieeexplore.ieee.org/abstract/document/8943388/keywords#keywords},
}

@Article{Liang2020Dec,
  author           = {Liang, Ming and Yang, Bin and Wang, Shenlong and Urtasun, Raquel},
  journal          = {arXiv},
  title            = {{Deep Continuous Fusion for Multi-Sensor 3D Object Detection}},
  year             = {2020},
  month            = dec,
  abstract         = {{In this paper, we propose a novel 3D object detector that can exploit both LIDAR as well as cameras to perform very accurate localization. Towards this goal, we design an end-to-end learnable architecture that exploits continuous convolutions to fuse image and LIDAR feature maps at different levels of resolution. Our proposed continuous fusion layer encode both discrete-state image features as well as continuous geometric information. This enables us to design a novel, reliable and efficient end-to-end learnable 3D object detector based on multiple sensors. Our experimental evaluation on both KITTI as well as a large scale 3D object detection benchmark shows significant improvements over the state of the art.}},
  citation         = {630},
  eprint           = {2012.10992},
  groups           = {Camera-Lidar fusion, Sensor Fusion, 3D},
  keywords         = {Computer Vision and Pattern Recognition (cs.CV)},
  modificationdate = {2023-04-04T09:55:17},
  url              = {https://arxiv.org/abs/2012.10992v1},
}

@Article{Bijelic2019Feb,
  author           = {Bijelic, Mario and Gruber, Tobias and Mannan, Fahim and Kraus, Florian and Ritter, Werner and Dietmayer, Klaus and Heide, Felix},
  journal          = {arXiv},
  title            = {{Seeing Through Fog Without Seeing Fog: Deep Multimodal Sensor Fusion in Unseen Adverse Weather}},
  year             = {2019},
  month            = feb,
  abstract         = {{The fusion of multimodal sensor streams, such as camera, lidar, and radar measurements, plays a critical role in object detection for autonomous vehicles, which base their decision making on these inputs. While existing methods exploit redundant information in good environmental conditions, they fail in adverse weather where the sensory streams can be asymmetrically distorted. These rare "edge-case" scenarios are not represented in available datasets, and existing fusion architectures are not designed to handle them. To address this challenge we present a novel multimodal dataset acquired in over 10,000km of driving in northern Europe. Although this dataset is the first large multimodal dataset in adverse weather, with 100k labels for lidar, camera, radar, and gated NIR sensors, it does not facilitate training as extreme weather is rare. To this end, we present a deep fusion network for robust fusion without a large corpus of labeled training data covering all asymmetric distortions. Departing from proposal-level fusion, we propose a single-shot model that adaptively fuses features, driven by measurement entropy. We validate the proposed method, trained on clean data, on our extensive validation dataset. Code and data are available here this https URL.}},
  citation         = {162},
  file             = {:/home/kvnptl/HBRS/semester_3/advance_scientific_writing/papers/dataset/Seeing Through Fog Without Seeing Fog_221231_172355.pdf:PDF},
  groups           = {Camera-Radar-Lidar, Adverse Weather, Sensor Fusion, Datasets, Camera-Radar-Lidar fusion},
  keywords         = {Computer Vision and Pattern Recognition (cs.CV)},
  modificationdate = {2023-04-02T17:43:36},
  priority         = {prio1},
  ranking          = {rank5},
  readstatus       = {read},
  url              = {https://arxiv.org/abs/1902.08913v3},
}

@InCollection{Deng2019Oct,
  author           = {Deng, Zijun and Zhu, Lei and Hu, Xiaowei and Fu, Chi-Wing and Xu, Xuemiao and Zhang, Qing and Qin, Jing and Heng, Pheng-Ann},
  booktitle        = {{2019 IEEE/CVF International Conference on Computer Vision (ICCV)}},
  publisher        = {IEEE},
  title            = {{Deep Multi-Model Fusion for Single-Image Dehazing}},
  year             = {2019},
  month            = oct,
  pages            = {2453--2462},
  abstract         = {{This paper presents a deep multi-model fusion network to attentively integrate multiple models to separate layers and boost the performance in single-image dehazing. To do so, we first formulate the attentional feature integration module to maximize the integration of the convolutional neural network (CNN) features at different CNN layers and generate the attentional multi-level integrated features (AMLIF). Then, from the AMLIF, we further predict a haze-free result for an atmospheric scattering model, as well as for four haze-layer separation models, and then fuse the results together to produce the final haze-free image. To evaluate the effectiveness of our method, we compare our network with several state-of-the-art methods on two widely-used dehazing benchmark datasets, as well as on two sets of real-world hazy images. Experimental results demonstrate clear quantitative and qualitative improvements of our method over the state-of-the-arts.}},
  citation         = {77},
  groups           = {Sensor Fusion},
  issn             = {2380-7504},
  journal          = {2019 IEEE/CVF International Conference on Computer Vision (ICCV)},
  modificationdate = {2023-01-01T22:34:36},
  url              = {https://ieeexplore.ieee.org/document/9009514},
}

@Misc{Qian2021,
  author           = {Qian, Kun and Zhu, Shilin and Zhang, Xinyu and Li, Li Erran},
  note             = {[Online; accessed 28. Nov. 2022]},
  title            = {{Robust Multimodal Vehicle Detection in Foggy Weather Using Complementary Lidar and Radar Signals}},
  year             = {2021},
  citation         = {27},
  groups           = {Sensor Fusion},
  modificationdate = {2023-01-01T22:34:36},
  pages            = {444--453},
  priority         = {prio3},
  url              = {https://openaccess.thecvf.com/content/CVPR2021/html/Qian_Robust_Multimodal_Vehicle_Detection_in_Foggy_Weather_Using_Complementary_Lidar_CVPR_2021_paper.html},
}

@InCollection{xn--Muat-rxb2021Oct,
  author           = {xn--Muat-rxb, Valentina and Fursa, Ivan and Newman, Paul and Cuzzolin, Fabio and Bradley, Andrew},
  booktitle        = {{2021 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW)}},
  publisher        = {IEEE},
  title            = {{Multi-weather city: Adverse weather stacking for autonomous driving}},
  year             = {2021},
  month            = oct,
  pages            = {2906--2915},
  abstract         = {{Autonomous vehicles make use of sensors to perceive the world around them, with heavy reliance on vision-based sensors such as RGB cameras. Unfortunately, since these sensors are affected by adverse weather, perception pipelines require extensive training on visual data under harsh conditions in order to improve the robustness of downstream tasks - data that is difficult and expensive to acquire. Based on GAN and CycleGAN architectures, we propose an overall (modular) architecture for constructing datasets, which allows one to add, swap out and combine components in order to generate images with diverse weather conditions. Starting from a single dataset with ground-truth, we generate 7 versions of the same data in diverse weather, and propose an extension to augment the generated conditions, thus resulting in a total of 14 adverse weather conditions, requiring a single ground truth. We test the quality of the generated conditions both in terms of perceptual quality and suitability for training downstream tasks, using real world, out-of-distribution adverse weather extracted from various datasets. We show improvements in both object detection and instance segmentation across all conditions, in many cases exceeding 10 percentage points increase in AP, and provide the materials and instructions needed to re-construct the multi-weather dataset, based upon the original Cityscapes dataset.}},
  citation         = {7},
  groups           = {Misc, Adverse Weather, Sensor Fusion, Datasets},
  issn             = {2473-9944},
  journal          = {2021 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW)},
  modificationdate = {2023-01-01T22:34:36},
  priority         = {prio3},
  url              = {https://ieeexplore.ieee.org/document/9607439},
}

@Article{Lai2022Sep,
  author           = {Lai, Haowen and Yin, Peng and Scherer, Sebastian},
  journal          = {IEEE Rob. Autom. Lett.},
  title            = {{AdaFusion: Visual-LiDAR Fusion With Adaptive Weights for Place Recognition}},
  year             = {2022},
  month            = sep,
  number           = {4},
  pages            = {12038--12045},
  volume           = {7},
  abstract         = {{Recent years have witnessed the increasing application of place recognition in various environments, such as city roads, large buildings, and a mix of indoor and outdoor places. This task, however, still remains challenging due to the limitations of different sensors and the changing appearance of environments. Current works only consider the use of individual sensors or simply combine different sensors, ignoring the fact that the importance of different sensors varies as the environment changes. In this letter, an adaptive weighting visual-LiDAR fusion method, named AdaFusion, is proposed to learn the weights for both image and point cloud features. Features of these two modalities are thus contributed differently according to the current environmental situation. Weights are learned by the multi-scale attention branch of the network, which is then fused with the multi-modality feature extraction branch. Furthermore, to better utilize the potential relationship between images and point clouds, we design a two-stage fusion approach to combine the 2D and 3D attention. Our work is tested on two public datasets. Experiments show that the adaptive weights help improve recognition accuracy and system robustness to varying environments while being efficient in runtime.}},
  citation         = {3},
  groups           = {Camera-Lidar fusion, Sensor Fusion},
  modificationdate = {2023-01-01T22:34:36},
  publisher        = {IEEE},
  url              = {https://ieeexplore.ieee.org/abstract/document/9905898?casa_token=f-aZkuzBdY0AAAAA:B5CQVbH-nh5Qd6KsEEMXPWiG5VoQBCa880OdHPxIwpbMumQSPThhMhsvn9WdTQO7tnYicfqsq3Q},
}

@InCollection{Barnes,
  author           = {Barnes, Dan and Gadd, Matthew and Murcutt, Paul and Newman, Paul and Posner, Ingmar},
  booktitle        = {{2020 IEEE International Conference on Robotics and Automation (ICRA)}},
  publisher        = {IEEE},
  title            = {{The Oxford Radar RobotCar Dataset: A Radar Extension to the Oxford RobotCar Dataset}},
  year             = {2020},
  pages            = {2020--31},
  abstract         = {{In this paper we present The Oxford Radar RobotCar Dataset, a new dataset for researching scene understanding using Millimetre-Wave FMCW scanning radar data. The target application is autonomous vehicles where this modality is robust to environmental conditions such as fog, rain, snow, or lens flare, which typically challenge other sensor modalities such as vision and LIDAR.(/P)(P)The data were gathered in January 2019 over thirty-two traversals of a central Oxford route spanning a total of 280 km of urban driving. It encompasses a variety of weather, traffic, and lighting conditions. This 4.7 TB dataset consists of over 240,000 scans from a Navtech CTS350-X radar and 2.4 million scans from two Velodyne HDL-32E 3D LIDARs; along with six cameras, two 2D LIDARs, and a GPS/INS receiver. In addition we release ground truth optimised radar odometry to provide an additional impetus to research in this domain. The full dataset is available for download at: ori.ox.ac.uk/datasets/radar-robotear-dataset.}},
  citation         = {207},
  groups           = {Camera-Radar-Lidar, Datasets},
  journal          = {Published in: 2020 IEEE International Conference on Robotics and Automation (ICRA)},
  modificationdate = {2023-01-01T22:46:07},
  priority         = {prio3},
  url              = {https://ieeexplore.ieee.org/abstract/document/9196884?casa_token=2BTLePzGRJUAAAAA:-8e1s7lpxTQvbOC02GFlYATkuaRuuV4c_6f9lLSVov180BuKv_sHcU5rhZ5qXwLt6-GoZwC-0uQ},
}

@Article{Maddern2016Nov,
  author           = {Maddern, Will and Pascoe, Geoffrey and Linegar, Chris and Newman, Paul},
  journal          = {Int. J. Rob. Res.},
  title            = {{1 year, 1000 km: The Oxford RobotCar dataset}},
  year             = {2016},
  issn             = {0278-3649},
  month            = nov,
  number           = {1},
  pages            = {3--15},
  volume           = {36},
  abstract         = {{We present a challenging new dataset for autonomous driving: the Oxford RobotCar Dataset. Over the period of May 2014 to December 2015 we traversed a route through central Oxford twice a week on av...}},
  citation         = {1097},
  groups           = {Camera-Lidar, Datasets},
  keywords         = {Dataset, autonomous vehicles, long-term autonomy, mobile robotics, computer vision, cameras, LIDAR, GPS, stereo, localization, mapping, SLAM, RobotCar},
  modificationdate = {2023-01-01T22:33:42},
  publisher        = {SAGE Publications Ltd STM},
  url              = {https://doi.org/10.1177/0278364916679498},
}

@Article{Carlevaris-Bianco2015Dec,
  author           = {Carlevaris-Bianco, Nicholas and Ushani, Arash K. and Eustice, Ryan M.},
  journal          = {Int. J. Rob. Res.},
  title            = {{University of Michigan North Campus long-term vision and lidar dataset}},
  year             = {2015},
  issn             = {0278-3649},
  month            = dec,
  number           = {9},
  pages            = {1023--1035},
  volume           = {35},
  abstract         = {{This paper documents a large scale, long-term autonomy dataset for robotics research collected on the University of Michigan{'}s North Campus. The dataset consists of omnidirectional imagery, 3D lida...}},
  citation         = {296},
  groups           = {Camera-Lidar, Datasets},
  keywords         = {Long-term SLAM, place recognition, lidar, computer vision, field and service robotics},
  modificationdate = {2023-01-01T22:33:42},
  publisher        = {SAGE Publications Ltd STM},
  url              = {https://doi.org/10.1177/0278364915614638},
}

@Article{Huang2021Nov,
  author           = {Huang, Xiaoshui and Qu, Wentao and Zuo, Yifan and Fang, Yuming and Zhao, Xiaowei},
  journal          = {arXiv},
  title            = {{IMFNet: Interpretable Multimodal Fusion for Point Cloud Registration}},
  year             = {2021},
  month            = nov,
  abstract         = {{The existing state-of-the-art point descriptor relies on structure information only, which omit the texture information. However, texture information is crucial for our humans to distinguish a scene part. Moreover, the current learning-based point descriptors are all black boxes which are unclear how the original points contribute to the final descriptor. In this paper, we propose a new multimodal fusion method to generate a point cloud registration descriptor by considering both structure and texture information. Specifically, a novel attention-fusion module is designed to extract the weighted texture information for the descriptor extraction. In addition, we propose an interpretable module to explain the original points in contributing to the final descriptor. We use the descriptor element as the loss to backpropagate to the target layer and consider the gradient as the significance of this point to the final descriptor. This paper moves one step further to explainable deep learning in the registration task. Comprehensive experiments on 3DMatch, 3DLoMatch and KITTI demonstrate that the multimodal fusion descriptor achieves state-of-the-art accuracy and improve the descriptor's distinctiveness. We also demonstrate that our interpretable module in explaining the registration descriptor extraction.}},
  citation         = {1},
  eprint           = {2111.09624},
  groups           = {Sensor Fusion},
  keywords         = {Computer Vision and Pattern Recognition (cs.CV)},
  modificationdate = {2023-01-01T22:34:36},
  url              = {https://arxiv.org/abs/2111.09624v1},
}

@Article{Liu2022May,
  author           = {Liu, Zhijian and Tang, Haotian and Amini, Alexander and Yang, Xinyu and Mao, Huizi and Rus, Daniela and Han, Song},
  journal          = {arXiv},
  title            = {{BEVFusion: Multi-Task Multi-Sensor Fusion with Unified Bird's-Eye View Representation}},
  year             = {2022},
  month            = may,
  abstract         = {{Multi-sensor fusion is essential for an accurate and reliable autonomous driving system. Recent approaches are based on point-level fusion: augmenting the LiDAR point cloud with camera features. However, the camera-to-LiDAR projection throws away the semantic density of camera features, hindering the effectiveness of such methods, especially for semantic-oriented tasks (such as 3D scene segmentation). In this paper, we break this deeply-rooted convention with BEVFusion, an efficient and generic multi-task multi-sensor fusion framework. It unifies multi-modal features in the shared bird's-eye view (BEV) representation space, which nicely preserves both geometric and semantic information. To achieve this, we diagnose and lift key efficiency bottlenecks in the view transformation with optimized BEV pooling, reducing latency by more than 40x. BEVFusion is fundamentally task-agnostic and seamlessly supports different 3D perception tasks with almost no architectural changes. It establishes the new state of the art on nuScenes, achieving 1.3{\%} higher mAP and NDS on 3D object detection and 13.6{\%} higher mIoU on BEV map segmentation, with 1.9x lower computation cost. Code to reproduce our results is available at this https URL.}},
  citation         = {35},
  eprint           = {2205.13542},
  groups           = {Camera-Lidar fusion, Sensor Fusion, 3D},
  keywords         = {Computer Vision and Pattern Recognition (cs.CV)},
  modificationdate = {2023-04-04T09:55:39},
  url              = {https://arxiv.org/abs/2205.13542v2},
}

@Article{Chitta2022May,
  author           = {Chitta, Kashyap and Prakash, Aditya and Jaeger, Bernhard and Yu, Zehao and Renz, Katrin and Geiger, Andreas},
  journal          = {arXiv},
  title            = {{TransFuser: Imitation with Transformer-Based Sensor Fusion for Autonomous Driving}},
  year             = {2022},
  month            = may,
  abstract         = {{How should we integrate representations from complementary sensors for autonomous driving? Geometry-based fusion has shown promise for perception (e.g. object detection, motion forecasting). However, in the context of end-to-end driving, we find that imitation learning based on existing sensor fusion methods underperforms in complex driving scenarios with a high density of dynamic agents. Therefore, we propose TransFuser, a mechanism to integrate image and LiDAR representations using self-attention. Our approach uses transformer modules at multiple resolutions to fuse perspective view and bird's eye view feature maps. We experimentally validate its efficacy on a challenging new benchmark with long routes and dense traffic, as well as the official leaderboard of the CARLA urban driving simulator. At the time of submission, TransFuser outperforms all prior work on the CARLA leaderboard in terms of driving score by a large margin. Compared to geometry-based fusion, TransFuser reduces the average collisions per kilometer by 48{\%}.}},
  citation         = {5},
  eprint           = {2205.15997},
  groups           = {Camera-Lidar fusion, Sensor Fusion},
  keywords         = {Computer Vision and Pattern Recognition (cs.CV), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), Robotics (cs.RO)},
  modificationdate = {2023-01-01T22:34:36},
  priority         = {prio2},
  url              = {https://arxiv.org/abs/2205.15997v1},
}

@Article{Matuszka2022Nov,
  author           = {Matuszka, Tam{\ifmmode\acute{a}\else\'{a}\fi}s and Barton, Iv{\ifmmode\acute{a}\else\'{a}\fi}n and Butykai, {\ifmmode\acute{A}\else\'{A}\fi}d{\ifmmode\acute{a}\else\'{a}\fi}m and Hajas, P{\ifmmode\acute{e}\else\'{e}\fi}ter and Kiss, D{\ifmmode\acute{a}\else\'{a}\fi}vid and Kov{\ifmmode\acute{a}\else\'{a}\fi}cs, Domonkos and Kuns{\ifmmode\acute{a}\else\'{a}\fi}gi-M{\ifmmode\acute{a}\else\'{a}\fi}t{\ifmmode\acute{e}\else\'{e}\fi}, S{\ifmmode\acute{a}\else\'{a}\fi}ndor and Lengyel, P{\ifmmode\acute{e}\else\'{e}\fi}ter and N{\ifmmode\acute{e}\else\'{e}\fi}meth, G{\ifmmode\acute{a}\else\'{a}\fi}bor and Pet{\ifmmode\mbox{\H{o}}\else\H{o}\fi}, Levente and Ribli, Dezs{\ifmmode\mbox{\H{o}}\else\H{o}\fi} and Szeghy, D{\ifmmode\acute{a}\else\'{a}\fi}vid and Vajna, Szabolcs and Varga, B{\ifmmode\acute{a}\else\'{a}\fi}lint},
  journal          = {arXiv},
  title            = {{aiMotive Dataset: A Multimodal Dataset for Robust Autonomous Driving with Long-Range Perception}},
  year             = {2022},
  month            = nov,
  citation         = {0},
  doi              = {10.48550/arXiv.2211.09445},
  eprint           = {2211.09445},
  groups           = {Camera-Radar-Lidar, Datasets},
  modificationdate = {2023-01-01T22:33:42},
  priority         = {prio1},
}

@Article{Feng2020Feb,
  author           = {Feng, Di and Haase-Sch{\ifmmode\ddot{u}\else\"{u}\fi}tz, Christian and Rosenbaum, Lars and Hertlein, Heinz and Gl{\ifmmode\ddot{a}\else\"{a}\fi}ser, Claudius and Timm, Fabian and Wiesbeck, Werner and Dietmayer, Klaus},
  journal          = {IEEE Trans. Intell. Transp. Syst.},
  title            = {{Deep Multi-Modal Object Detection and Semantic Segmentation for Autonomous Driving: Datasets, Methods, and Challenges}},
  year             = {2020},
  month            = feb,
  number           = {3},
  pages            = {1341--1360},
  volume           = {22},
  abstract         = {{Recent advancements in perception for autonomous driving are driven by deep learning. In order to achieve robust and accurate scene understanding, autonomous vehicles are usually equipped with different sensors (e.g. cameras, LiDARs, Radars), and multiple sensing modalities can be fused to exploit their complementary properties. In this context, many methods have been proposed for deep multi-modal perception problems. However, there is no general guideline for network architecture design, and questions of {\textquotedblleft}what to fuse{\textquotedblright}, {\textquotedblleft}when to fuse{\textquotedblright}, and {\textquotedblleft}how to fuse{\textquotedblright} remain open. This review paper attempts to systematically summarize methodologies and discuss challenges for deep multi-modal object detection and semantic segmentation in autonomous driving. To this end, we first provide an overview of on-board sensors on test vehicles, open datasets, and background information for object detection and semantic segmentation in autonomous driving research. We then summarize the fusion methodologies and discuss challenges and open questions. In the appendix, we provide tables that summarize topics and methods. We also provide an interactive online platform to navigate each reference: https://boschresearch.github.io/multimodalperception/.}},
  citation         = {586},
  comment          = {from Bosch},
  file             = {:/home/kvnptl/HBRS/semester_3/advance_scientific_writing/papers/survey/Deep Multi-modal Object Detection and Semantic_230107_194140.pdf:PDF},
  groups           = {Sensor Fusion, Survey},
  modificationdate = {2023-04-02T17:06:58},
  priority         = {prio1},
  publisher        = {IEEE},
  ranking          = {rank5},
  readstatus       = {read},
  url              = {https://ieeexplore.ieee.org/abstract/document/9000872?casa_token=xxfsDn9JltkAAAAA:UHwwFqAb6b8pfvisl3U9BR4WXfeoYAJdQldUUQBxu_zt9HSzbWeRKQzV1mrkjzEdeHvZH9tSuumG},
}

@InCollection{Gohil,
  author           = {Gohil, Priteshkumar and Thoduka, Santosh and Pl{\ifmmode\ddot{o}\else\"{o}\fi}ger, Paul G.},
  booktitle        = {{2022 26th International Conference on Pattern Recognition (ICPR)}},
  publisher        = {IEEE},
  title            = {{Sensor Fusion and Multimodal Learning for Robotic Grasp Verification Using Neural Networks}},
  year             = {2022},
  pages            = {21--25},
  abstract         = {{Different sensors on a robot help in understanding different aspects of the environment they are working in; however, each sensor modality is often processed individually and information from other sensors is not utilized jointly. One of the reasons is different sampling rates and different dimensions of input modalities. In this paper, we use multimodal data fusion techniques such as early, late and intermediate fusion for grasp failure identification using four different 3D convolution-based multimodal neural networks (3D-MNN). Our results on a visual-tactile dataset shows that the performance of the classification task is improved while using multimodal data. In addition, a neural network trained with 30:22 train-test split of multimodal data achieved accuracy comparable to a network trained with 78:22 train-test split of unimodal data 1 .}},
  citation         = {0},
  groups           = {Misc fusion, Sensor Fusion},
  journal          = {Published in: 2022 26th International Conference on Pattern Recognition (ICPR)},
  modificationdate = {2023-01-01T22:34:36},
  url              = {https://ieeexplore.ieee.org/document/9955646},
}

@Article{Caesar2019Mar,
  author           = {Caesar, Holger and Bankiti, Varun and Lang, Alex H. and Vora, Sourabh and Liong, Venice Erin and Xu, Qiang and Krishnan, Anush and Pan, Yu and Baldan, Giancarlo and Beijbom, Oscar},
  journal          = {arXiv},
  title            = {{nuScenes: A multimodal dataset for autonomous driving}},
  year             = {2019},
  month            = mar,
  abstract         = {{Robust detection and tracking of objects is crucial for the deployment of autonomous vehicle technology. Image based benchmark datasets have driven development in computer vision tasks such as object detection, tracking and segmentation of agents in the environment. Most autonomous vehicles, however, carry a combination of cameras and range sensors such as lidar and radar. As machine learning based methods for detection and tracking become more prevalent, there is a need to train and evaluate such methods on datasets containing range sensor data along with images. In this work we present nuTonomy scenes (nuScenes), the first dataset to carry the full autonomous vehicle sensor suite: 6 cameras, 5 radars and 1 lidar, all with full 360 degree field of view. nuScenes comprises 1000 scenes, each 20s long and fully annotated with 3D bounding boxes for 23 classes and 8 attributes. It has 7x as many annotations and 100x as many images as the pioneering KITTI dataset. We define novel 3D detection and tracking metrics. We also provide careful dataset analysis as well as baselines for lidar and image based detection and tracking. Data, development kit and more information are available online.}},
  citation         = {2174},
  eprint           = {1903.11027},
  groups           = {Camera-Radar-Lidar, Datasets},
  keywords         = {Machine Learning (cs.LG), Computer Vision and Pattern Recognition (cs.CV), Robotics (cs.RO), Machine Learning (stat.ML)},
  modificationdate = {2023-01-01T22:46:12},
  priority         = {prio3},
  url              = {https://arxiv.org/abs/1903.11027v5},
}

@Article{Chang2020Feb,
  author           = {Chang, Shuo and Zhang, Yifan and Zhang, Fan and Zhao, Xiaotong and Huang, Sai and Feng, Zhiyong and Wei, Zhiqing},
  journal          = {Sensors},
  title            = {{Spatial Attention Fusion for Obstacle Detection Using MmWave Radar and Vision Sensor}},
  year             = {2020},
  issn             = {1424-8220},
  month            = feb,
  number           = {4},
  pages            = {956},
  volume           = {20},
  abstract         = {{For autonomous driving, it is important to detect obstacles in all scales accurately for safety consideration. In this paper, we propose a new spatial attention fusion (SAF) method for obstacle detection using mmWave radar and vision sensor, where the sparsity of radar points are considered in the proposed SAF. The proposed fusion method can be embedded in the feature-extraction stage, which leverages the features of mmWave radar and vision sensor effectively. Based on the SAF, an attention weight matrix is generated to fuse the vision features, which is different from the concatenation fusion and element-wise add fusion. Moreover, the proposed SAF can be trained by an end-to-end manner incorporated with the recent deep learning object detection framework. In addition, we build a generation model, which converts radar points to radar images for neural network training. Numerical results suggest that the newly developed fusion method achieves superior performance in public benchmarking. In addition, the source code will be released in the GitHub. Keywords: autonomous driving; obstacle detection; mmWave radar; vision; spatial attention fusion}},
  citation         = {67},
  groups           = {Camera-Radar fusion},
  keywords         = {autonomous driving, obstacle detection, mmWave radar, vision, spatial attention fusion},
  modificationdate = {2023-03-08T23:30:45},
  priority         = {prio1},
  publisher        = {Multidisciplinary Digital Publishing Institute},
  url              = {https://www.mdpi.com/1424-8220/20/4/956},
}

@InCollection{Cheng2021Oct,
  author    = {Cheng, Yuwei and Xu, Hu and Liu, Yimin},
  booktitle = {{2021 IEEE/CVF International Conference on Computer Vision (ICCV)}},
  publisher = {IEEE},
  title     = {{Robust Small Object Detection on the Water Surface through Fusion of Camera and Millimeter Wave Radar}},
  year      = {2021},
  month     = oct,
  pages     = {15243--15252},
  abstract  = {{In recent years, unmanned surface vehicles (USVs) have been experiencing growth in various applications. With the expansion of USVs{'} application scenes from the typical marine areas to inland waters, new challenges arise for the object detection task, which is an essential part of the perception system of USVs. In our work, we focus on a relatively unexplored task for USVs in inland waters: small object detection on water surfaces, which is of vital importance for safe autonomous navigation and USVs{'} certain missions such as floating waste cleaning. Considering the limitations of vision-based object detection, we propose a novel radar-vision fusion based method for robust small object detection on water surfaces. By using a novel representation format of millimeter wave radar point clouds and applying a deep-level multi-scale fusion of RGB images and radar data, the proposed method can efficiently utilize the characteristics of radar data and improve the accuracy and robustness for small object detection on water surfaces. We test the method on the real-world floating bottle dataset that we collected and released. The result shows that, our method improves the average detection accuracy significantly compared to the vision-based methods and achieves state-of-the-art performance. Besides, the proposed method performs robustly when single sensor degrades.}},
  citation  = {10},
  groups    = {Camera-Radar fusion},
  issn      = {2380-7504},
  journal   = {2021 IEEE/CVF International Conference on Computer Vision (ICCV)},
  priority  = {prio2},
  url       = {https://ieeexplore.ieee.org/document/9710582},
}

@InCollection{Shuai2021May,
  author    = {Shuai, Xian and Shen, Yulin and Tang, Yi and Shi, Shuyao and Ji, Luping and Xing, Guoliang},
  booktitle = {{IoTDI '21: Proceedings of the International Conference on Internet-of-Things Design and Implementation}},
  publisher = {Association for Computing Machinery},
  title     = {{milliEye: A Lightweight mmWave Radar and Camera Fusion System for Robust Object Detection}},
  year      = {2021},
  address   = {New York, NY, USA},
  isbn      = {978-1-45038354-7},
  month     = may,
  pages     = {145--157},
  abstract  = {{A wide range of advanced deep learning algorithms have recently been proposed for image classification and object detection. However, the effectiveness of these methods can be significantly restricted in many real-world scenarios where the visibility or illumination is poor. Compared to RGB cameras, millimeter-wave (mmWave) radars are immune to the above environmental variability and can assist cameras under adverse conditions. To this end, we propose milliEye, a lightweight mmWave radar and camera fusion system for robust object detection on the edge platforms. milliEye has several key advantages over existing sensor fusion approaches. First, while milliEye fuses two sensing modalities in a learning-based fashion, it requires only a small amount of labeled image/radar data of a new scene as it can fully utilize large public image datasets for extensive training. This salient feature enables milliEye to adapt to highly complex real-world environments. Second, based on a novel architecture that decouples the image-based object detector from other modules, milliEye is compatible with different off-the-shelf image-based object detectors. As a result, it can take advantage of the rapid progress of object detection algorithms. Moreover, thanks to the highly compute-efficient fusion approach, milliEye is lightweight and thus suitable for edge-based real-time applications. To evaluate the performance of milliEye, we collect a new radar and camera fusion dataset for object detection, which contains both ordinary-light and low-light illumination conditions. The results show that milliEye can provide substantial performance boosts over state-of-the-art image-based object detectors, including Tiny YOLOv3 and SSD, especially in low-light scenes, while incurring low compute overhead on edge platforms.}},
  citation  = {13},
  groups    = {Camera-Radar fusion},
  priority  = {prio2},
  url       = {https://dl.acm.org/doi/abs/10.1145/3450268.3453532},
}

@Article{Geyer2020Apr,
  author           = {Geyer, Jakob and Kassahun, Yohannes and Mahmudi, Mentar and Ricou, Xavier and Durgesh, Rupesh and Chung, Andrew S. and Hauswald, Lorenz and Pham, Viet Hoang and M{\ifmmode\ddot{u}\else\"{u}\fi}hlegg, Maximilian and Dorn, Sebastian and Fernandez, Tiffany and J{\ifmmode\ddot{a}\else\"{a}\fi}nicke, Martin and Mirashi, Sudesh and Savani, Chiragkumar and Sturm, Martin and Vorobiov, Oleksandr and Oelker, Martin and Garreis, Sebastian and Schuberth, Peter},
  journal          = {arXiv},
  title            = {{A2D2: Audi Autonomous Driving Dataset}},
  year             = {2020},
  month            = apr,
  abstract         = {{Research in machine learning, mobile robotics, and autonomous driving is accelerated by the availability of high quality annotated data. To this end, we release the Audi Autonomous Driving Dataset (A2D2). Our dataset consists of simultaneously recorded images and 3D point clouds, together with 3D bounding boxes, semantic segmentation, instance segmentation, and data extracted from the automotive bus. Our sensor suite consists of six cameras and five LiDAR units, providing full 360 degree coverage. The recorded data is time synchronized and mutually registered. Annotations are for non-sequential frames: 41,277 frames with semantic segmentation image and point cloud labels, of which 12,497 frames also have 3D bounding box annotations for objects within the field of view of the front camera. In addition, we provide 392,556 sequential frames of unannotated sensor data for recordings in three cities in the south of Germany. These sequences contain several loops. Faces and vehicle number plates are blurred due to GDPR legislation and to preserve anonymity. A2D2 is made available under the CC BY-ND 4.0 license, permitting commercial use subject to the terms of the license. Data and further information are available at http://www.a2d2.audi.}},
  citation         = {187},
  eprint           = {2004.06320},
  groups           = {Camera-Lidar, Datasets},
  keywords         = {Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), Image and Video Processing (eess.IV)},
  modificationdate = {2023-01-01T22:33:42},
  url              = {https://arxiv.org/abs/2004.06320v1},
}

@Misc{Sun2020,
  author           = {Sun, Pei and Kretzschmar, Henrik and Dotiwalla, Xerxes and Chouard, Aurelien and Patnaik, Vijaysai and Tsui, Paul and Guo, James and Zhou, Yin and Chai, Yuning and Caine, Benjamin and Vasudevan, Vijay and Han, Wei and Ngiam, Jiquan and Zhao, Hang and Timofeev, Aleksei and Ettinger, Scott and Krivokon, Maxim and Gao, Amy and Joshi, Aditya and Zhang, Yu and Shlens, Jonathon and Chen, Zhifeng and Anguelov, Dragomir},
  note             = {[Online; accessed 14. Dec. 2022]},
  title            = {{Scalability in Perception for Autonomous Driving: Waymo Open Dataset}},
  year             = {2020},
  abstract         = {The research community has increasing interest in autonomous driving research, despite the resource intensity of obtaining representative real world data. Existing self-driving datasets are limited in the scale and variation of the environments they capture, even though generalization within and between operating regions is crucial to the over-all viability of the technology. In an effort to help align the research community's contributions with real-world self-driving problems, we introduce a new large scale, high quality, diverse dataset. Our new dataset consists of 1150 scenes that each span 20 seconds, consisting of well synchronized and calibrated high quality LiDAR and camera data captured across a range of urban and suburban geographies. It is 15x more diverse than the largest camera+LiDAR dataset available based on our proposed diversity metric. We exhaustively annotated this data with 2D (camera image) and 3D (LiDAR) bounding boxes, with consistent identifiers across frames. Finally, we provide strong baselines for 2D as well as 3D detection and tracking tasks. We further study the effects of dataset size and generalization across geographies on 3D detection methods. Find data, code and more up-to-date information at http://www.waymo.com/open.},
  citation         = {1081},
  groups           = {Camera-Lidar, Datasets},
  modificationdate = {2023-01-01T22:33:42},
  pages            = {2446--2454},
  url              = {https://openaccess.thecvf.com/content_CVPR_2020/html/Sun_Scalability_in_Perception_for_Autonomous_Driving_Waymo_Open_Dataset_CVPR_2020_paper.html},
}

@InCollection{Nabati,
  author    = {Nabati, Ramin and Qi, Hairong},
  booktitle = {{2019 IEEE International Conference on Image Processing (ICIP)}},
  publisher = {IEEE},
  title     = {{RRPN: Radar Region Proposal Network for Object Detection in Autonomous Vehicles}},
  year      = {2019},
  pages     = {22--25},
  abstract  = {{Region proposal algorithms play an important role in most state-of-the-art two-stage object detection networks by hypothesizing object locations in the image. Nonetheless, region proposal algorithms are known to be the bottleneck in most two-stage object detection networks, increasing the processing time for each image and resulting in slow networks not suitable for real-time applications such as autonomous driving vehicles. In this paper we introduce RRPN, a Radar-based real-time region proposal algorithm for object detection in autonomous driving vehicles. RRPN generates object proposals by mapping Radar detections to the image coordinate system and generating pre-defined anchor boxes for each mapped Radar detection point. These anchor boxes are then transformed and scaled based on the object's distance from the vehicle, to provide more accurate proposals for the detected objects. We evaluate our method on the newly released NuScenes dataset [1] using the Fast R-CNN object detection network [2]. Compared to the Selective Search object proposal algorithm [3], our model operates more than 100{\ifmmode\times\else\texttimes\fi} faster while at the same time achieves higher detection precision and recall. Code has been made publicly available at https://github.com/mrnabati/RRPN.}},
  citation  = {79},
  groups    = {Camera-Radar fusion},
  journal   = {Published in: 2019 IEEE International Conference on Image Processing (ICIP)},
  url       = {https://ieeexplore.ieee.org/abstract/document/8803392?casa_token=kx0KF3J5AzUAAAAA:VWZllClS9535awmq5zCreagvZviF_VfuCE2wtAajpuP40M7N_AnCYx2wjPkGr6qR1jkUZwxlyJwo},
}

@InCollection{Chadwick,
  author           = {Chadwick, Simon and Maddern, Will and Newman, Paul},
  booktitle        = {{2019 International Conference on Robotics and Automation (ICRA)}},
  publisher        = {IEEE},
  title            = {{Distant Vehicle Detection Using Radar and Vision}},
  year             = {2019},
  pages            = {20--24},
  abstract         = {{For autonomous vehicles to be able to operate successfully they need to be aware of other vehicles with sufficient time to make safe, stable plans. Given the possible closing speeds between two vehicles, this necessitates the ability to accurately detect distant vehicles. Many current image-based object detectors using convolutional neural networks exhibit excellent performance on existing datasets such as KITTI. However, the performance of these networks falls when detecting small (distant) objects. We demonstrate that incorporating radar data can boost performance in these difficult situations. We also introduce an efficient automated method for training data generation using cameras of different focal lengths.}},
  citation         = {116},
  groups           = {Camera-Radar fusion},
  journal          = {Published in: 2019 International Conference on Robotics and Automation (ICRA)},
  modificationdate = {2023-01-07T21:40:45},
  url              = {https://ieeexplore.ieee.org/abstract/document/8794312?casa_token=O186EQMAZg0AAAAA:EylpF-Vb0ej42VXy5olYx3D7tL1YZLV9T_OJ1k8RzKJv6zT11BjV6ANxGzeSPrRBJduM5y-5aR6v},
}

@Article{Zhou2022May,
  author           = {Zhou, Yi and Liu, Lulu and Zhao, Haocheng and L{\ifmmode\acute{o}\else\'{o}\fi}pez-Ben{\ifmmode\acute{\imath}\else\'{\i}\fi}tez, Miguel and Yu, Limin and Yue, Yutao},
  journal          = {Sensors},
  title            = {{Towards Deep Radar Perception for Autonomous Driving: Datasets, Methods, and Challenges}},
  year             = {2022},
  issn             = {1424-8220},
  month            = may,
  number           = {11},
  pages            = {4208},
  volume           = {22},
  abstract         = {{With recent developments, the performance of automotive radar has improved significantly. The next generation of 4D radar can achieve imaging capability in the form of high-resolution point clouds. In this context, we believe that the era of deep learning for radar perception has arrived. However, studies on radar deep learning are spread across different tasks, and a holistic overview is lacking. This review paper attempts to provide a big picture of the deep radar perception stack, including signal processing, datasets, labelling, data augmentation, and downstream tasks such as depth and velocity estimation, object detection, and sensor fusion. For these tasks, we focus on explaining how the network structure is adapted to radar domain knowledge. In particular, we summarise three overlooked challenges in deep radar perception, including multi-path effects, uncertainty problems, and adverse weather effects, and present some attempts to solve them. Keywords: automotive radars; radar signal processing; object detection; multi-sensor fusion; deep learning; autonomous driving}},
  citation         = {7},
  file             = {:/home/kvnptl/HBRS/semester_3/advance_scientific_writing/papers/survey/Towards Deep Radar Perception_221227_141311.pdf:PDF},
  groups           = {Survey, Adverse Weather, Sensor Fusion},
  keywords         = {automotive radars, radar signal processing, object detection, multi-sensor fusion, deep learning, autonomous driving},
  modificationdate = {2023-04-02T17:03:49},
  priority         = {prio1},
  publisher        = {Multidisciplinary Digital Publishing Institute},
  ranking          = {rank5},
  readstatus       = {read},
  url              = {https://www.mdpi.com/1424-8220/22/11/4208},
}

@Article{Huang2022Feb,
  author           = {Huang, Keli and Shi, Botian and Li, Xiang and Li, Xin and Huang, Siyuan and Li, Yikang},
  journal          = {arXiv},
  title            = {{Multi-modal Sensor Fusion for Auto Driving Perception: A Survey}},
  year             = {2022},
  month            = feb,
  abstract         = {{Multi-modal fusion is a fundamental task for the perception of an autonomous driving system, which has recently intrigued many researchers. However, achieving a rather good performance is not an easy task due to the noisy raw data, underutilized information, and the misalignment of multi-modal sensors. In this paper, we provide a literature review of the existing multi-modal-based methods for perception tasks in autonomous driving. Generally, we make a detailed analysis including over 50 papers leveraging perception sensors including LiDAR and camera trying to solve object detection and semantic segmentation tasks. Different from traditional fusion methodology for categorizing fusion models, we propose an innovative way that divides them into two major classes, four minor classes by a more reasonable taxonomy in the view of the fusion stage. Moreover, we dive deep into the current fusion methods, focusing on the remaining problems and open-up discussions on the potential research opportunities. In conclusion, what we expect to do in this paper is to present a new taxonomy of multi-modal fusion methods for the autonomous driving perception tasks and provoke thoughts of the fusion-based techniques in the future.}},
  citation         = {15},
  groups           = {Survey},
  keywords         = {Computer Vision and Pattern Recognition (cs.CV)},
  modificationdate = {2023-04-02T17:05:13},
  priority         = {prio2},
  url              = {https://arxiv.org/abs/2202.02703v2},
}

@Article{Hwang2022Oct,
  author           = {Hwang, Jyh-Jing and Kretzschmar, Henrik and Manela, Joshua and Rafferty, Sean and Armstrong-Crews, Nicholas and Chen, Tiffany and Anguelov, Dragomir},
  journal          = {arXiv},
  title            = {{CramNet: Camera-Radar Fusion with Ray-Constrained Cross-Attention for Robust 3D Object Detection}},
  year             = {2022},
  month            = oct,
  abstract         = {{Robust 3D object detection is critical for safe autonomous driving. Camera and radar sensors are synergistic as they capture complementary information and work well under different environmental conditions. Fusing camera and radar data is challenging, however, as each of the sensors lacks information along a perpendicular axis, that is, depth is unknown to camera and elevation is unknown to radar. We propose the camera-radar matching network CramNet, an efficient approach to fuse the sensor readings from camera and radar in a joint 3D space. To leverage radar range measurements for better camera depth predictions, we propose a novel ray-constrained cross-attention mechanism that resolves the ambiguity in the geometric correspondences between camera features and radar features. Our method supports training with sensor modality dropout, which leads to robust 3D object detection, even when a camera or radar sensor suddenly malfunctions on a vehicle. We demonstrate the effectiveness of our fusion approach through extensive experiments on the RADIATE dataset, one of the few large-scale datasets that provide radar radio frequency imagery. A camera-only variant of our method achieves competitive performance in monocular 3D object detection on the Waymo Open Dataset.}},
  citation         = {0},
  eprint           = {2210.09267},
  groups           = {Camera-Radar fusion, 3D},
  keywords         = {Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), Robotics (cs.RO)},
  modificationdate = {2023-04-04T09:56:01},
  priority         = {prio2},
  url              = {https://arxiv.org/abs/2210.09267v2},
}

@Article{Paek2022Jun,
  author           = {Paek, Dong-Hee and Kong, Seung-Hyun and Wijaya, Kevin Tirta},
  journal          = {arXiv},
  title            = {{K-Radar: 4D Radar Object Detection for Autonomous Driving in Various Weather Conditions}},
  year             = {2022},
  month            = jun,
  abstract         = {{Unlike RGB cameras that use visible light bands (384{\ifmmode\$\else\textdollar\fi}{\ifmmode\backslash\else\textbackslash\fi}sim$769 THz) and Lidar that use infrared bands (361${\ifmmode\backslash\else\textbackslash\fi}sim$331 THz), Radars use relatively longer wavelength radio bands (77${\ifmmode\backslash\else\textbackslash\fi}sim{\ifmmode\$\else\textdollar\fi}81 GHz), resulting in robust measurements in adverse weathers. Unfortunately, existing Radar datasets only contain a relatively small number of samples compared to the existing camera and Lidar datasets. This may hinder the development of sophisticated data-driven deep learning techniques for Radar-based perception. Moreover, most of the existing Radar datasets only provide 3D Radar tensor (3DRT) data that contain power measurements along the Doppler, range, and azimuth dimensions. As there is no elevation information, it is challenging to estimate the 3D bounding box of an object from 3DRT. In this work, we introduce KAIST-Radar (K-Radar), a novel large-scale object detection dataset and benchmark that contains 35K frames of 4D Radar tensor (4DRT) data with power measurements along the Doppler, range, azimuth, and elevation dimensions, together with carefully annotated 3D bounding box labels of objects on the roads. K-Radar includes challenging driving conditions such as adverse weathers (fog, rain, and snow) on various road structures (urban, suburban roads, alleyways, and highways). In addition to the 4DRT, we provide auxiliary measurements from carefully calibrated high-resolution Lidars, surround stereo cameras, and RTK-GPS. We also provide 4DRT-based object detection baseline neural networks (baseline NNs) and show that the height information is crucial for 3D object detection. And by comparing the baseline NN with a similarly-structured Lidar-based neural network, we demonstrate that 4D Radar is a more robust sensor for adverse weather conditions. All codes are available at this https URL.}},
  citation         = {3},
  file             = {:/home/kvnptl/HBRS/semester_3/advance_scientific_writing/papers/dataset/K-Radar 4D Radar Object Detection forAutonomous _221229_165758.pdf:PDF},
  groups           = {Camera-Radar-Lidar, Adverse Weather, Datasets},
  keywords         = {Computer Vision and Pattern Recognition (cs.CV), Artificial Intelligence (cs.AI)},
  modificationdate = {2023-04-02T19:57:50},
  priority         = {prio1},
  ranking          = {rank5},
  readstatus       = {read},
  url              = {https://arxiv.org/abs/2206.08171v2},
}

@Article{Zhang2021Dec,
  author           = {Zhang, Yuxiao and Carballo, Alexander and Yang, Hanting and Takeda, Kazuya},
  journal          = {arXiv},
  title            = {{Autonomous Driving in Adverse Weather Conditions: A Survey}},
  year             = {2021},
  month            = dec,
  abstract         = {{Automated Driving Systems (ADS) open up a new domain for the automotive industry and offer new possibilities for future transportation with higher efficiency and comfortable experiences. However, autonomous driving under adverse weather conditions has been the problem that keeps autonomous vehicles (AVs) from going to level 4 or higher autonomy for a long time. This paper assesses the influences and challenges that weather brings to ADS sensors in an analytic and statistical way, and surveys the solutions against inclement weather conditions. State-of-the-art techniques on perception enhancement with regard to each kind of weather are thoroughly reported. External auxiliary solutions like V2X technology, weather conditions coverage in currently available datasets, simulators, and experimental facilities with weather chambers are distinctly sorted out. By pointing out all kinds of major weather problems the autonomous driving field is currently facing, and reviewing both hardware and computer science solutions in recent years, this survey contributes a holistic overview on the obstacles and directions of ADS development in terms of adverse weather driving conditions.}},
  citation         = {11},
  file             = {:/home/kvnptl/HBRS/semester_3/advance_scientific_writing/papers/survey/Autonomous Driving in Adverse Weather Conditions_230223_230047.pdf:PDF},
  groups           = {Survey, Adverse Weather},
  keywords         = {Robotics (cs.RO)},
  modificationdate = {2023-04-02T17:06:24},
  priority         = {prio2},
  readstatus       = {read},
  url              = {https://arxiv.org/abs/2112.08936v1},
}

@Article{Yan2019Sep,
  author           = {Yan, Zhi and Sun, Li and Krajnik, Tomas and Ruichek, Yassine},
  journal          = {arXiv},
  title            = {{EU Long-term Dataset with Multiple Sensors for Autonomous Driving}},
  year             = {2019},
  month            = sep,
  abstract         = {{The field of autonomous driving has grown tremendously over the past few years, along with the rapid progress in sensor technology. One of the major purposes of using sensors is to provide environment perception for vehicle understanding, learning and reasoning, and ultimately interacting with the environment. In this paper, we first introduce a multisensor platform allowing vehicle to perceive its surroundings and locate itself in a more efficient and accurate way. The platform integrates eleven heterogeneous sensors including various cameras and lidars, a radar, an IMU (Inertial Measurement Unit), and a GPS-RTK (Global Positioning System / Real-Time Kinematic), while exploits a ROS (Robot Operating System) based software to process the sensory data. Then, we present a new dataset (this https URL) for autonomous driving captured many new research challenges (e.g. highly dynamic environment), and especially for long-term autonomy (e.g. creating and maintaining maps), collected with our instrumented vehicle, publicly available to the community.}},
  citation         = {48},
  eprint           = {1909.03330},
  groups           = {Camera-Radar-Lidar, Datasets},
  keywords         = {Robotics (cs.RO), Databases (cs.DB)},
  modificationdate = {2023-01-01T22:46:17},
  priority         = {prio3},
  url              = {https://arxiv.org/abs/1909.03330v3},
}

@Article{Nabati2020Nov,
  author           = {Nabati, Ramin and Qi, Hairong},
  journal          = {arXiv},
  title            = {{CenterFusion: Center-based Radar and Camera Fusion for 3D Object Detection}},
  year             = {2020},
  month            = nov,
  abstract         = {{The perception system in autonomous vehicles is responsible for detecting and tracking the surrounding objects. This is usually done by taking advantage of several sensing modalities to increase robustness and accuracy, which makes sensor fusion a crucial part of the perception system. In this paper, we focus on the problem of radar and camera sensor fusion and propose a middle-fusion approach to exploit both radar and camera data for 3D object detection. Our approach, called CenterFusion, first uses a center point detection network to detect objects by identifying their center points on the image. It then solves the key data association problem using a novel frustum-based method to associate the radar detections to their corresponding object's center point. The associated radar detections are used to generate radar-based feature maps to complement the image features, and regress to object properties such as depth, rotation and velocity. We evaluate CenterFusion on the challenging nuScenes dataset, where it improves the overall nuScenes Detection Score (NDS) of the state-of-the-art camera-based algorithm by more than 12{\%}. We further show that CenterFusion significantly improves the velocity estimation accuracy without using any additional temporal information. The code is available at this https URL .}},
  citation         = {95},
  eprint           = {2011.04841},
  groups           = {Camera-Radar fusion, 3D},
  keywords         = {Computer Vision and Pattern Recognition (cs.CV)},
  modificationdate = {2023-04-04T09:56:11},
  url              = {https://arxiv.org/abs/2011.04841v1},
}

@InCollection{Kim2021Jan,
  author           = {Kim, Youngseok and Choi, Jun Won and Kum, Dongsuk},
  booktitle        = {{2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}},
  publisher        = {IEEE},
  title            = {{GRIF Net: Gated Region of Interest Fusion Network for Robust 3D Object Detection from Radar Point Cloud and Monocular Image}},
  year             = {2021},
  month            = jan,
  pages            = {10857--10864},
  abstract         = {{Robust and accurate scene representation is essential for advanced driver assistance systems (ADAS) such as automated driving. The radar and camera are two widely used sensors for commercial vehicles due to their low-cost, high-reliability, and low-maintenance. Despite their strengths, radar and camera have very limited performance when used individually. In this paper, we propose a low-level sensor fusion 3D object detector that combines two Region of Interest (RoI) from radar and camera feature maps by a Gated RoI Fusion (GRIF) to perform robust vehicle detection. To take advantage of sensors and utilize a sparse radar point cloud, we design a GRIF that employs the explicit gating mechanism to adaptively select the appropriate data when one of the sensors is abnormal. Our experimental evaluations on nuScenes show that our fusion method GRIF not only has significant performance improvement over single radar and image method but achieves comparable performance to the LiDAR detection method. We also observe that the proposed GRIF achieve higher recall than mean or concatenation fusion operation when points are sparse.}},
  citation         = {14},
  groups           = {Camera-Radar fusion, 3D},
  issn             = {2153-0866},
  journal          = {2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  modificationdate = {2023-04-04T09:56:29},
  url              = {https://ieeexplore.ieee.org/document/9341177},
}

@InCollection{Danapal2022Oct,
  author           = {Danapal, Gokulesh and Mayr, Christian and Kariminezhad, Ali and Vriesmann, Daniel and Zimmer, Alessandro},
  booktitle        = {{2022 Sensor Data Fusion: Trends, Solutions, Applications (SDF)}},
  publisher        = {IEEE},
  title            = {{Attention Empowered Feature-level Radar-Camera Fusion for Object Detection}},
  year             = {2022},
  month            = oct,
  pages            = {1--6},
  abstract         = {{Safe autonomous driving can not be realizable unless with robust environment perception. However, robustness can only be guaranteed if the system functions reliably in all weather conditions with any darkness level, while capturing all corner cases. Perception has heavily relied on cameras for object detection purposes. Due to operating at visible-light frequencies, there exists a plethora of corner cases for a sole camera-based perception system. In this paper, radar data is constructively fused with the RGB images for improving perception performance. Radar data that is in the form of point cloud is pre-processed by domain conversion from a bird-eye-view perspective into an image coordinate system. These alongside with RGB images from the camera are given as inputs to our proposed fusion network, which extracts the features of each sensor independently. These features are then fused to perform a joint detection. The robustness in adverse conditions like fog is validated via synthetically foggified images for different levels of fog densities. A channel attention module is integrated into the fusion network, which helps to prevent the drop in performance up to a fog density of 25. The network is trained and tested on NuScenes [1] dataset. Our proposed fusion network is capable of outperforming the other state-of-the-art radar-camera fusion networks by at least 8{\%}.}},
  citation         = {0},
  groups           = {Camera-Radar fusion, Adverse Weather},
  issn             = {2333-7427},
  journal          = {2022 Sensor Data Fusion: Trends, Solutions, Applications (SDF)},
  modificationdate = {2023-01-07T20:31:09},
  priority         = {prio1},
  url              = {https://ieeexplore.ieee.org/document/9931956},
}

@InCollection{Li2020Dec,
  author    = {Li, Liang-qun and Xie, Yuan-liang},
  booktitle = {{2020 15th IEEE International Conference on Signal Processing (ICSP)}},
  publisher = {IEEE},
  title     = {{A Feature Pyramid Fusion Detection Algorithm Based on Radar and Camera Sensor}},
  year      = {2020},
  month     = dec,
  pages     = {366--370},
  volume    = {1},
  abstract  = {{Considering the development of object detection based on deep learning framework in recent years, it has brought a new scope for multi-source fusion in the field of autonomous driving. In this paper, we propose a new architecture with a feature pyramid attention module to fuse the projected radar data and camera data. In the proposed algorithm, the detection model of YOLOv3 is employed by us and the feature pyramid module is extended with the input interface of the radar projection image and attention module. Additionally, in order to reduce the interference information from different scales of radar projected block, a new generation mechanism of radar projection images is introduced. Finally, the radar projection image is fused in feature pyramid layers with an attention module. The result shows that the proposed fusion algorithm outperforms better than image-only network for the nuScenes dataset. The code for this research will be made available to the public at: https://github.com/yuanliangxie/nuscenes{$\_$}data{$\_$}process.}},
  citation  = {6},
  groups    = {Camera-Radar fusion},
  issn      = {2164-5221},
  journal   = {2020 15th IEEE International Conference on Signal Processing (ICSP)},
  url       = {https://ieeexplore.ieee.org/document/9320985},
}

@Article{Nabati2020Sep,
  author           = {Nabati, Ramin and Qi, Hairong},
  journal          = {arXiv},
  title            = {{Radar-Camera Sensor Fusion for Joint Object Detection and Distance Estimation in Autonomous Vehicles}},
  year             = {2020},
  month            = sep,
  abstract         = {{In this paper we present a novel radar-camera sensor fusion framework for accurate object detection and distance estimation in autonomous driving scenarios. The proposed architecture uses a middle-fusion approach to fuse the radar point clouds and RGB images. Our radar object proposal network uses radar point clouds to generate 3D proposals from a set of 3D prior boxes. These proposals are mapped to the image and fed into a Radar Proposal Refinement (RPR) network for objectness score prediction and box refinement. The RPR network utilizes both radar information and image feature maps to generate accurate object proposals and distance estimations. The radar-based proposals are combined with image-based proposals generated by a modified Region Proposal Network (RPN). The RPN has a distance regression layer for estimating distance for every generated proposal. The radar-based and image-based proposals are merged and used in the next stage for object classification. Experiments on the challenging nuScenes dataset show our method outperforms other existing radar-camera fusion methods in the 2D object detection task while at the same time accurately estimates objects' distances.}},
  citation         = {18},
  eprint           = {2009.08428},
  groups           = {Camera-Radar fusion, 3D},
  keywords         = {Computer Vision and Pattern Recognition (cs.CV)},
  modificationdate = {2023-04-04T09:56:48},
  url              = {https://arxiv.org/abs/2009.08428v1},
}

@Article{Sengupta2022Jan,
  author    = {Sengupta, Arindam and Yoshizawa, Atsushi and Cao, Siyang},
  journal   = {IEEE Rob. Autom. Lett.},
  title     = {{Automatic Radar-Camera Dataset Generation for Sensor-Fusion Applications}},
  year      = {2022},
  issn      = {2377-3766},
  month     = jan,
  number    = {2},
  pages     = {2875--2882},
  volume    = {7},
  abstract  = {{Withheterogeneous sensors offering complementary advantages in perception, there has been a significant growth in sensor-fusion based research and development in object perception and tracking using classical or deep neural networks based approaches. However, supervised learning requires massive labeled data-sets, that require expensive manual labor to generate. This paper presents a novel approach that leverages YOLOv3 based highly accurate object detection from camera to automatically label point cloud data obtained from a co-calibrated radar sensor to generate labeled radar-image and radar-only data-sets to aid learning algorithms for different applications. To achieve this we first co-calibrate the vision and radar sensors and obtain a radar-to-camera transformation matrix. The collected radar returns are segregated by different targets using a density based clustering scheme and the cluster centroids are projected onto the camera image using the transformation matrix. The Hungarian Algorithm is then used to associate the radar cluster centroids with the YOLOv3 generated bounding box centroids, and are labeled with the predicted class. The proposed approach is efficient, easy to implement and aims to encourage rapid development of multi-sensor data-sets, which are extremely limited currently, compared to the optical counterparts. The calibration process, software pipeline and the data-set generation is described in detail. Furthermore preliminary results from two sample applications for object detection using the data-sets are also presented.}},
  citation  = {3},
  publisher = {IEEE},
  url       = {https://ieeexplore.ieee.org/document/9690006},
}

@Article{Ghiasi2022Dec,
  author           = {Ghiasi, Amin and Kazemi, Hamid and Borgnia, Eitan and Reich, Steven and Shu, Manli and Goldblum, Micah and Wilson, Andrew Gordon and Goldstein, Tom},
  journal          = {arXiv},
  title            = {{What do Vision Transformers Learn? A Visual Exploration}},
  year             = {2022},
  month            = dec,
  abstract         = {{Vision transformers (ViTs) are quickly becoming the de-facto architecture for computer vision, yet we understand very little about why they work and what they learn. While existing studies visually analyze the mechanisms of convolutional neural networks, an analogous exploration of ViTs remains challenging. In this paper, we first address the obstacles to performing visualizations on ViTs. Assisted by these solutions, we observe that neurons in ViTs trained with language model supervision (e.g., CLIP) are activated by semantic concepts rather than visual features. We also explore the underlying differences between ViTs and CNNs, and we find that transformers detect image background features, just like their convolutional counterparts, but their predictions depend far less on high-frequency information. On the other hand, both architecture types behave similarly in the way features progress from abstract patterns in early layers to concrete objects in late layers. In addition, we show that ViTs maintain spatial information in all layers except the final layer. In contrast to previous works, we show that the last layer most likely discards the spatial information and behaves as a learned global pooling operation. Finally, we conduct large-scale visualizations on a wide range of ViT variants, including DeiT, CoaT, ConViT, PiT, Swin, and Twin, to validate the effectiveness of our method.}},
  citation         = {0},
  eprint           = {2212.06727},
  groups           = {Perception},
  keywords         = {Computer Vision and Pattern Recognition (cs.CV)},
  modificationdate = {2023-01-01T22:34:12},
  url              = {https://arxiv.org/abs/2212.06727v1},
}

@Article{Mafukidze2022Oct,
  author    = {Mafukidze, Harry D. and Mishra, Amit K. and Pidanic, Jan and Francois, Schonken W. P.},
  journal   = {IEEE Access},
  title     = {{Scattering Centers to Point Clouds: A Review of mmWave Radars for Non-Radar-Engineers}},
  year      = {2022},
  issn      = {2169-3536},
  month     = oct,
  pages     = {110992--111021},
  volume    = {10},
  abstract  = {{Recently, mmWave radars have been gaining popularity, thanks to their low cost, ease of use and high-resolution sensing. In this paper, we provide a review of the mmWave radar data processing frameworks, starting from mathematical foundations to applications. Specifically, we focus on the mmWave radar point cloud as a robust data structure representing compressed signatures for target recognition and classification. We first focus on the generation of the radar point clouds, and the signal processing algorithms designed for their unique characteristics. Then, we illustrate how the radar point clouds are prepared for feature extraction and classification using machine learning and deep learning approaches. Finally, we summarize the state-of-the-art applications, open datasets, developments and future research directions in this field.}},
  citation  = {0},
  priority  = {prio3},
  publisher = {IEEE},
  url       = {https://ieeexplore.ieee.org/document/9908570},
}

@Article{Zang2019Mar,
  author           = {Zang, Shizhe and Ding, Ming and Smith, David and Tyler, Paul and Rakotoarivelo, Thierry and Kaafar, Mohamed Ali},
  journal          = {IEEE Veh. Technol. Mag.},
  title            = {{The Impact of Adverse Weather Conditions on Autonomous Vehicles: How Rain, Snow, Fog, and Hail Affect the Performance of a Self-Driving Car}},
  year             = {2019},
  issn             = {1556-6080},
  month            = mar,
  number           = {2},
  pages            = {103--111},
  volume           = {14},
  abstract         = {{Recently, the development of autonomous vehicles and intelligent driver assistance systems has drawn a significant amount of attention from the general public. One of the most critical issues in the development of autonomous vehicles and driver assistance systems is their poor performance under adverse weather conditions, such as rain, snow, fog, and hail. However, no current study provides a systematic and unified review of the effect that weather has on the various types of sensors used in autonomous vehicles. In this article, we first present a literature review about the impact of adverse weather conditions on state-ofthe-art sensors, such as lidar, GPS, camera, and radar. Then, we characterize the effect of rainfall on millimeter-wave (mmwave) radar, which considers both the rain attenuation and the backscatter effects. Our simulation results show that the detection range of mm-wave radar can be reduced by up to 45{\%} under severe rainfall conditions. Moreover, the rain backscatter effect is significantly different for targets with different radar cross-section (RCS) areas.}},
  citation         = {184},
  groups           = {Adverse Weather},
  modificationdate = {2023-04-02T20:04:06},
  priority         = {prio1},
  publisher        = {IEEE},
  url              = {https://ieeexplore.ieee.org/document/8666747},
}

@Article{Park2020Aug,
  author           = {Park, Ji-Il and Park, Jihyuk and Kim, Kyung-Soo},
  journal          = {IEEE Access},
  title            = {{Fast and Accurate Desnowing Algorithm for LiDAR Point Clouds}},
  year             = {2020},
  issn             = {2169-3536},
  month            = aug,
  pages            = {160202--160212},
  volume           = {8},
  abstract         = {{LiDAR sensors have the advantage of being able to generate high-resolution imaging quickly during both day and night; however, their performance is severely limited in adverse weather conditions such as snow, rain, and dense fog. Consequently, many researchers are actively working to overcome these limitations by applying sensor fusion with radar and optical cameras to LiDAR. While studies on the denoising of point clouds acquired by LiDAR in adverse weather have been conducted recently, the results are still insufficient for application to autonomous vehicles because of speed and accuracy performance limitations. Therefore, we propose a new intensity-based filter that differs from the existing distance-based filter, which limits the speed. The proposed method showed overwhelming performance advantages in terms of both speed and accuracy by removing only snow particles while leaving important environmental features. The intensity criteria for snow removal were derived based on an analysis of the properties of laser light and snow particles.}},
  citation         = {35},
  groups           = {Lidar only, Adverse Weather, Restoration},
  modificationdate = {2023-04-02T20:03:23},
  publisher        = {IEEE},
  url              = {https://ieeexplore.ieee.org/document/9180326},
}

@Article{Lin2021Mar,
  author           = {Lin, Shih-Lin and Wu, Bing-Han},
  journal          = {Appl. Sci.},
  title            = {{Application of Kalman Filter to Improve 3D LiDAR Signals of Autonomous Vehicles in Adverse Weather}},
  year             = {2021},
  issn             = {2076-3417},
  month            = mar,
  number           = {7},
  pages            = {3018},
  volume           = {11},
  abstract         = {{A worldwide increase in the number of vehicles on the road has led to an increase in the frequency of serious traffic accidents, causing loss of life and property. Autonomous vehicles could be part of the solution, but their safe operation is dependent on the onboard LiDAR (light detection and ranging) systems used for the detection of the environment outside the vehicle. Unfortunately, problems with the application of LiDAR in autonomous vehicles remain, for example, the weakening of the echo detection capability in adverse weather conditions. The signal is also affected, even drowned out, by sensory noise outside the vehicles, and the problem can become so severe that the autonomous vehicle cannot move. Clearly, the accuracy of the stereo images sensed by the LiDAR must be improved. In this study, we developed a method to improve the acquisition of LiDAR data in adverse weather by using a combination of a Kalman filter and nearby point cloud denoising. The overall LiDAR framework was tested in experiments in a space 2 m in length and width and 0.6 m high. Normal weather and three kinds of adverse weather conditions (rain, thick smoke, and rain and thick smoke) were simulated. The results show that this system can be used to recover normal weather data from data measured by LiDAR even in adverse weather conditions. The results showed an effective improvement of 10{\%} to 30{\%} in the LiDAR stereo images. This method can be developed and widely applied in the future. Keywords: autopilot; LiDAR of autonomous vehicles; Kalman filter}},
  citation         = {6},
  groups           = {Lidar only, Adverse Weather},
  keywords         = {autopilot, LiDAR of autonomous vehicles, Kalman filter},
  modificationdate = {2023-01-01T22:34:21},
  publisher        = {Multidisciplinary Digital Publishing Institute},
  url              = {https://www.mdpi.com/2076-3417/11/7/3018},
}

@Article{Wang2022Mar,
  author           = {Wang, Weiqi and You, Xiong and Chen, Lingyu and Tian, Jiangpeng and Tang, Fen and Zhang, Lantian},
  journal          = {Remote Sens.},
  title            = {{A Scalable and Accurate De-Snowing Algorithm for LiDAR Point Clouds in Winter}},
  year             = {2022},
  issn             = {2072-4292},
  month            = mar,
  number           = {6},
  pages            = {1468},
  volume           = {14},
  abstract         = {{Accurate and efficient environmental awareness is a fundamental capability of autonomous driving technology and the real-time data collected by sensors offer autonomous vehicles an intuitive impression of their environment. Unfortunately, the ambient noise caused by varying weather conditions immediately affects the ability of autonomous vehicles to accurately understand their environment and its expected impact. In recent years, researchers have improved the environmental perception capabilities of simultaneous localization and mapping (SLAM), object detection and tracking, semantic segmentation and panoptic segmentation, but relatively few studies have focused on enhancing environmental perception capabilities in adverse weather conditions, such as rain, snow and fog. To enhance the environmental perception of autonomous vehicles in adverse weather, we developed a dynamic filtering method called Dynamic Distance{\textendash}Intensity Outlier Removal (DDIOR), which integrates the distance and intensity of points based on the systematic and accurate analysis of LiDAR point cloud data characteristics in snowy weather. Experiments on the publicly available WADS dataset (Winter Adverse Driving dataSet) showed that our method can efficiently remove snow noise while fully preserving the detailed features of the environment. Keywords: autonomous driving; de-snowing algorithm; LiDAR point clouds; data processing}},
  citation         = {5},
  groups           = {Lidar only, Adverse Weather, Restoration},
  keywords         = {autonomous driving, de-snowing algorithm, LiDAR point clouds, data processing},
  modificationdate = {2023-04-02T20:02:47},
  publisher        = {Multidisciplinary Digital Publishing Institute},
  url              = {https://www.mdpi.com/2072-4292/14/6/1468},
}

@Article{Fursa2021Mar,
  author           = {Fursa, Ivan and Fandi, Elias and Musat, Valentina and Culley, Jacob and Gil, Enric and Teeti, Izzeddin and Bilous, Louise and Sluis, Isaac Vander and Rast, Alexander and Bradley, Andrew},
  journal          = {arXiv},
  title            = {{Worsening Perception: Real-time Degradation of Autonomous Vehicle Perception Performance for Simulation of Adverse Weather Conditions}},
  year             = {2021},
  month            = mar,
  abstract         = {{Autonomous vehicles rely heavily upon their perception subsystems to see the environment in which they operate. Unfortunately, the effect of variable weather conditions presents a significant challenge to object detection algorithms, and thus it is imperative to test the vehicle extensively in all conditions which it may experience. However, development of robust autonomous vehicle subsystems requires repeatable, controlled testing - while real weather is unpredictable and cannot be scheduled. Real-world testing in adverse conditions is an expensive and time-consuming task, often requiring access to specialist facilities. Simulation is commonly relied upon as a substitute, with increasingly visually realistic representations of the real-world being developed. In the context of the complete autonomous vehicle control pipeline, subsystems downstream of perception need to be tested with accurate recreations of the perception system output, rather than focusing on subjective visual realism of the input - whether in simulation or the real world. This study develops the untapped potential of a lightweight weather augmentation method in an autonomous racing vehicle - focusing not on visual accuracy, but rather the effect upon perception subsystem performance in real time. With minimal adjustment, the prototype developed in this study can replicate the effects of water droplets on the camera lens, and fading light conditions. This approach introduces a latency of less than 8 ms using compute hardware well suited to being carried in the vehicle - rendering it ideal for real-time implementation that can be run during experiments in simulation, and augmented reality testing in the real world.}},
  citation         = {4},
  eprint           = {2103.02760},
  groups           = {Adverse Weather},
  keywords         = {Robotics (cs.RO), Computer Vision and Pattern Recognition (cs.CV)},
  modificationdate = {2023-01-01T22:34:21},
  url              = {https://arxiv.org/abs/2103.02760v4},
}

@Article{Wu2020Jun,
  author           = {Wu, Jianqing and Xu, Hao and Tian, Yuan and Pi, Rendong and Yue, Rui},
  journal          = {Sensors},
  title            = {{Vehicle Detection under Adverse Weather from Roadside LiDAR Data}},
  year             = {2020},
  issn             = {1424-8220},
  month            = jun,
  number           = {12},
  pages            = {3433},
  volume           = {20},
  abstract         = {{Roadside light detection and ranging (LiDAR) is an emerging traffic data collection device and has recently been deployed in different transportation areas. The current data processing algorithms for roadside LiDAR are usually developed assuming normal weather conditions. Adverse weather conditions, such as windy and snowy conditions, could be challenges for data processing. This paper examines the performance of the state-of-the-art data processing algorithms developed for roadside LiDAR under adverse weather and then composed an improved background filtering and object clustering method in order to process the roadside LiDAR data, which was proven to perform better under windy and snowy weather. The testing results showed that the accuracy of the background filtering and point clustering was greatly improved compared to the state-of-the-art methods. With this new approach, vehicles can be identified with relatively high accuracy under windy and snowy weather. Keywords: vehicle detection; adverse weather; roadside LiDAR; data processing}},
  citation         = {24},
  groups           = {Lidar only, Adverse Weather},
  keywords         = {vehicle detection, adverse weather, roadside LiDAR, data processing},
  modificationdate = {2023-01-01T22:34:21},
  publisher        = {Multidisciplinary Digital Publishing Institute},
  url              = {https://www.mdpi.com/1424-8220/20/12/3433},
}

@InCollection{Charron2018May,
  author           = {Charron, Nicholas and Phillips, Stephen and Waslander, Steven L.},
  booktitle        = {{2018 15th Conference on Computer and Robot Vision (CRV)}},
  publisher        = {IEEE},
  title            = {{De-noising of Lidar Point Clouds Corrupted by Snowfall}},
  year             = {2018},
  month            = may,
  pages            = {254--261},
  abstract         = {{A common problem in autonomous driving is designing a system that can operate in adverse weather conditions. Falling rain and snow tends to corrupt sensor measurements, particularly for lidar sensors. Surprisingly, very little research has been published on methods to de-noise point clouds which are collected by lidar in rainy or snowy weather conditions. In this paper, we present a method for removing snow noise by processing point clouds using a 3D outlier detection algorithm. Our method, the dynamic radius outlier removal filter, accounts for the variation in point cloud density with increasing distance from the sensor, with the goal of removing the noise caused by snow while retaining detail in environmental features (which is necessary for autonomous localization and navigation). The proposed method outperforms other noise-removal methods, including methods which operate on depth image representations of the lidar scans. We show on point clouds obtained while driving in falling snow that we can simultaneously obtain {$>$} 90{\%} precision and recall, indicating that the proposed method is effective at removing snow, without removing environmental features.}},
  citation         = {84},
  groups           = {Lidar only, Adverse Weather},
  journal          = {2018 15th Conference on Computer and Robot Vision (CRV)},
  modificationdate = {2023-01-01T22:34:21},
  url              = {https://ieeexplore.ieee.org/document/8575761},
}

@Article{Gao2020Nov,
  author   = {Gao, Xiangyu and Xing, Guanbin and Roy, Sumit and Liu, Hui},
  journal  = {arXiv},
  title    = {{RAMP-CNN: A Novel Neural Network for Enhanced Automotive Radar Object Recognition}},
  year     = {2020},
  month    = nov,
  abstract = {{Millimeter-wave radars are being increasingly integrated into commercial vehicles to support new advanced driver-assistance systems by enabling robust and high-performance object detection, localization, as well as recognition - a key component of new environmental perception. In this paper, we propose a novel radar multiple-perspectives convolutional neural network (RAMP-CNN) that extracts the location and class of objects based on further processing of the range-velocity-angle (RVA) heatmap sequences. To bypass the complexity of 4D convolutional neural networks (NN), we propose to combine several lower-dimension NN models within our RAMP-CNN model that nonetheless approaches the performance upper-bound with lower complexity. The extensive experiments show that the proposed RAMP-CNN model achieves better average recall and average precision than prior works in all testing scenarios. Besides, the RAMP-CNN model is validated to work robustly under nighttime, which enables low-cost radars as a potential substitute for pure optical sensing under severe conditions.}},
  citation = {32},
  eprint   = {2011.08981},
  keywords = {Signal Processing (eess.SP), Artificial Intelligence (cs.AI), Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), Robotics (cs.RO)},
  url      = {https://arxiv.org/abs/2011.08981v2},
}

@InProceedings{radiant-radar-image-association-network-for-3d-object-detection,
  author           = {Yunfei Long and Abhinav Kumar and Daniel Morris and Xiaoming Liu and Marcos Paul Gerardo Castro and Punarjay Chakravarty},
  booktitle        = {In Proceeding of Thirty-Seventh AAAI Conference on Artificial Intelligence (AAAI)},
  title            = {RADIANT: RADar Image Association NeTwork for 3D Object Detection},
  year             = {2023},
  address          = {Washington, D.C.},
  month            = {February},
  citation         = {0},
  groups           = {Camera-Radar fusion, 3D},
  modificationdate = {2023-04-04T09:57:05},
  url              = {http://cvlab.cse.msu.edu/pdfs/Long_Kumar_Morris_Liu_Castro_Chakravarty_AAAI2023.pdf},
}

@InCollection{Sahba2020Nov,
  author           = {Sahba, Ramin and Sahba, Amin and Sahba, Farshid},
  booktitle        = {{2020 11th IEEE Annual Information Technology, Electronics and Mobile Communication Conference (IEMCON)}},
  publisher        = {IEEE},
  title            = {{Using a Combination of LiDAR, RADAR, and Image Data for 3D Object Detection in Autonomous Vehicles}},
  year             = {2020},
  month            = nov,
  pages            = {0427--0431},
  abstract         = {{One of the topics that is highly regarded and researched in the field of artificial intelligence and machine learning is object detection. Its use is especially important in autonomous vehicles. The various methods used to detect objects are based on different types of data, including image, radar, and lidar. Using a point clouds is one of the new methods for 3D object detection proposed in some recent work. One of the recently presented efficient methods is PointPillars network. It is an encoder that can learn from data available in a point cloud and then organize it as a representation in vertical columns (pillars). This representation can be used for 3D object detection. in this work, we try to develop a high performance model for 3D object detection based on PointPillars network exploiting a combination of lidar, radar, and image data to be used for autonomous vehicles perception. We use lidar, radar, and image data in nuScenes dataset to predict 3D boxes for three classes of objects that are car, pedestrian, and bus. To measure and compare results, we use nuScenes detection score (NDS) that is a combined metric for detection task. Results show that increasing the number of lidar sweeps, and combining them with radar and image data, significantly improve the performance of the 3D object detector. We suggest a method to combine different types of input data (lidar, radar, image) using a weighting system that can be used as the input for the encoder.}},
  citation         = {4},
  groups           = {Sensor Fusion, 3D},
  issn             = {2644-3163},
  journal          = {2020 11th IEEE Annual Information Technology, Electronics and Mobile Communication Conference (IEMCON)},
  modificationdate = {2023-04-04T09:57:16},
  url              = {https://ieeexplore.ieee.org/document/9284930},
}

@Article{Wang2022Dec,
  author           = {Wang, Li and Zhang, Xinyu and Li, Jun and Xv, Baowei and Fu, Rong and Chen, Haifeng and Yang, Lei and Jin, Dafeng and Zhao, Lijun},
  journal          = {IEEE Trans. Veh. Technol.},
  title            = {{Multi-Modal and Multi-Scale Fusion 3D Object Detection of 4D Radar and LiDAR for Autonomous Driving}},
  year             = {2022},
  issn             = {1939-9359},
  month            = dec,
  pages            = {1--15},
  abstract         = {{Multi-modal fusion plays a critical role in 3D object detection, overcoming the inherent limitations of single-sensor perception in autonomous driving. Most fusion methods require data from high-resolution cameras and LiDAR sensors, which are less robust and the detection accuracy drops drastically with the increase of range as the point cloud density decreases. Alternatively, the fusion of Radar and LiDAR alleviates these issues but is still a developing field, especially for 4D Radar with a more robust and broader detection range. Nevertheless, different data characteristics and noise distributions between two sensors hinder performance improvement when directly integrating them. Therefore, we are the first to propose a novel fusion method termed}},
  citation         = {0},
  groups           = {3D},
  modificationdate = {2023-04-04T09:57:23},
  priority         = {prio2},
  publisher        = {IEEE},
  url              = {https://ieeexplore.ieee.org/abstract/document/9991894},
}

@Article{Wang2021Feb,
  author    = {Wang, Yizhou and Jiang, Zhongyu and Li, Yudong and Hwang, Jenq-Neng and Xing, Guanbin and Liu, Hui},
  journal   = {IEEE J. Sel. Top. Signal Process.},
  title     = {{RODNet: A Real-Time Radar Object Detection Network Cross-Supervised by Camera-Radar Fused Object 3D Localization}},
  year      = {2021},
  issn      = {1941-0484},
  month     = feb,
  number    = {4},
  pages     = {954--967},
  volume    = {15},
  abstract  = {{Various autonomous or assisted driving strategies have been facilitated through the accurate and reliable perception of the environment around a vehicle. Among the commonly used sensors, radar has usually been considered as a robust and cost-effective solution even in adverse driving scenarios, e.g., weak/strong lighting or bad weather. Instead of considering fusing the unreliable information from all available sensors, perception from pure radar data becomes a valuable alternative that is worth exploring. In this paper, we propose a deep radar object detection network, named RODNet, which is cross-supervised by a camera-radar fused algorithm without laborious annotation efforts, to effectively detect objects from the radio frequency (RF) images in real-time. First, the raw signals captured by millimeter-wave radars are transformed to RF images in range-azimuth coordinates. Second, our proposed RODNet takes a snippet of RF images as the input to predict the likelihood of objects in the radar field of view (FoV). Two customized modules are also added to handle multi-chirp information and object relative motion. The proposed RODNet is cross-supervised by a novel 3D localization of detected objects using a camera-radar fusion (CRF) strategy in the training stage. Due to no existing public dataset available for our task, we create a new dataset, named CRUW, 1 1}},
  citation  = {35},
  groups    = {Camera-Radar fusion},
  priority  = {prio1},
  publisher = {IEEE},
  url       = {https://ieeexplore.ieee.org/abstract/document/9353210},
}

@Article{Valanarasu2021Nov,
  author           = {Valanarasu, Jeya Maria Jose and Yasarla, Rajeev and Patel, Vishal M.},
  journal          = {arXiv},
  title            = {{TransWeather: Transformer-based Restoration of Images Degraded by Adverse Weather Conditions}},
  year             = {2021},
  month            = nov,
  abstract         = {{Removing adverse weather conditions like rain, fog, and snow from images is an important problem in many applications. Most methods proposed in the literature have been designed to deal with just removing one type of degradation. Recently, a CNN-based method using neural architecture search (All-in-One) was proposed to remove all the weather conditions at once. However, it has a large number of parameters as it uses multiple encoders to cater to each weather removal task and still has scope for improvement in its performance. In this work, we focus on developing an efficient solution for the all adverse weather removal problem. To this end, we propose TransWeather, a transformer-based end-to-end model with just a single encoder and a decoder that can restore an image degraded by any weather condition. Specifically, we utilize a novel transformer encoder using intra-patch transformer blocks to enhance attention inside the patches to effectively remove smaller weather degradations. We also introduce a transformer decoder with learnable weather type embeddings to adjust to the weather degradation at hand. TransWeather achieves improvements across multiple test datasets over both All-in-One network as well as methods fine-tuned for specific tasks. TransWeather is also validated on real world test images and found to be more effective than previous methods. Implementation code can be accessed at this https URL .}},
  citation         = {18},
  eprint           = {2111.14813},
  groups           = {Adverse Weather, Restoration},
  keywords         = {Computer Vision and Pattern Recognition (cs.CV)},
  modificationdate = {2023-04-02T20:02:39},
  url              = {https://arxiv.org/abs/2111.14813v2},
}

@Article{Yoneda2019Dec,
  author           = {Yoneda, Keisuke and Suganuma, Naoki and Yanase, Ryo and Aldibaja, Mohammad},
  journal          = {IATSS Research},
  title            = {{Automated driving recognition technologies for adverse weather conditions}},
  year             = {2019},
  issn             = {0386-1112},
  month            = dec,
  number           = {4},
  pages            = {253--262},
  volume           = {43},
  abstract         = {{During automated driving in urban areas, decisions must be made while recognizing the surrounding environment using sensors such as camera, Light Detection and Ranging (LiDAR), millimeter-wave radar (MWR), and the global navigation satellite system (GNSS). The ability to drive under various environmental conditions is an important issue for automated driving on any road. In order to introduce the automated vehicles into the markets, the ability to evaluate various traffic conditions and navigate safely presents serious challenges. Another important challenge is the development of a robust recognition system can account for adverse weather conditions. Sun glare, rain, fog, and snow are adverse weather conditions that can occur in the driving environment. This paper summarizes research focused on automated driving technologies and discuss challenges to identifying adverse weather and other situations that make driving difficult, thus complicating the introduction of automated vehicles to the market.}},
  citation         = {84},
  groups           = {Adverse Weather},
  keywords         = {Automated vehicle, Self-localization, Surrounding recognition, Path planning, Adverse condition},
  modificationdate = {2023-04-02T19:59:42},
  publisher        = {Elsevier},
  url              = {https://www.sciencedirect.com/science/article/pii/S0386111219301463},
}

@Article{Burnett2022Mar,
  author           = {Burnett, Keenan and Yoon, David J. and Wu, Yuchen and Li, Andrew Zou and Zhang, Haowei and Lu, Shichen and Qian, Jingxing and Tseng, Wei-Kang and Lambert, Andrew and Leung, Keith Y. K. and Schoellig, Angela P. and Barfoot, Timothy D.},
  journal          = {arXiv},
  title            = {{Boreas: A Multi-Season Autonomous Driving Dataset}},
  year             = {2022},
  month            = mar,
  abstract         = {{The Boreas dataset was collected by driving a repeated route over the course of one year, resulting in stark seasonal variations and adverse weather conditions such as rain and falling snow. In total, the Boreas dataset contains over 350km of driving data featuring a 128-channel Velodyne Alpha-Prime lidar, a 360 degree Navtech CIR304-H scanning radar, a 5MP FLIR Blackfly S camera, and centimetre-accurate post-processed ground truth poses. At launch, our dataset will support live leaderboards for odometry, metric localization, and 3D object detection. The dataset and development kit are available at: this https URL}},
  citation         = {20},
  groups           = {Camera-Radar-Lidar, Adverse Weather, Datasets},
  keywords         = {Robotics (cs.RO)},
  modificationdate = {2023-04-02T19:57:57},
  priority         = {prio1},
  url              = {https://arxiv.org/abs/2203.10168v1},
}

@Article{Bai2021Jun,
  author    = {Bai, Jie and Zheng, Lianqing and Li, Sen and Tan, Bin and Chen, Sihan and Huang, Libo},
  journal   = {Sensors},
  title     = {{Radar Transformer: An Object Classification Network Based on 4D MMW Imaging Radar}},
  year      = {2021},
  issn      = {1424-8220},
  month     = jun,
  number    = {11},
  pages     = {3854},
  volume    = {21},
  citation  = {12},
  doi       = {10.3390/s21113854},
  publisher = {Multidisciplinary Digital Publishing Institute},
}

@Article{Deziel2021Feb,
  author           = {D{\ifmmode\acute{e}\else\'{e}\fi}ziel, Jean-Luc and Merriaux, Pierre and Tremblay, Francis and Lessard, Dave and Plourde, Dominique and Stanguennec, Julien and Goulet, Pierre and Olivier, Pierre},
  journal          = {arXiv},
  title            = {{PixSet : An Opportunity for 3D Computer Vision to Go Beyond Point Clouds With a Full-Waveform LiDAR Dataset}},
  year             = {2021},
  month            = feb,
  citation         = {6},
  doi              = {10.48550/arXiv.2102.12010},
  eprint           = {2102.12010},
  groups           = {Camera-Radar-Lidar},
  keywords         = {Robotics (cs.RO), Artificial Intelligence (cs.AI), Computer Vision and Pattern Recognition (cs.CV)},
  modificationdate = {2023-01-01T22:46:04},
  priority         = {prio3},
}

@Article{Li2022Apr,
  author           = {Li, Peizhao and Wang, Pu and Berntorp, Karl and Liu, Hongfu},
  journal          = {arXiv},
  title            = {{Exploiting Temporal Relations on Radar Perception for Autonomous Driving}},
  year             = {2022},
  month            = apr,
  citation         = {5},
  doi              = {10.48550/arXiv.2204.01184},
  eprint           = {2204.01184},
  groups           = {Perception},
  keywords         = {Computer Vision and Pattern Recognition (cs.CV)},
  modificationdate = {2023-01-01T22:43:11},
}

@Article{Popov2022Sep,
  author           = {Popov, Alexander and Gebhardt, Patrik and Chen, Ke and Oldja, Ryan and Lee, Heeseok and Murray, Shane and Bhargava, Ruchi and Smolyanskiy, Nikolai},
  journal          = {arXiv},
  title            = {{NVRadarNet: Real-Time Radar Obstacle and Free Space Detection for Autonomous Driving}},
  year             = {2022},
  month            = sep,
  citation         = {0},
  doi              = {10.48550/arXiv.2209.14499},
  eprint           = {2209.14499},
  groups           = {Perception},
  keywords         = {Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), Robotics (cs.RO)},
  modificationdate = {2023-01-01T22:43:02},
}

@Article{Le2021Aug,
  author           = {Le, Ngan and Rathour, Vidhiwar Singh and Yamazaki, Kashu and Luu, Khoa and Savvides, Marios},
  journal          = {arXiv},
  title            = {{Deep Reinforcement Learning in Computer Vision: A Comprehensive Survey}},
  year             = {2021},
  month            = aug,
  abstract         = {{Deep reinforcement learning augments the reinforcement learning framework and utilizes the powerful representation of deep neural networks. Recent works have demonstrated the remarkable successes of deep reinforcement learning in various domains including finance, medicine, healthcare, video games, robotics, and computer vision. In this work, we provide a detailed review of recent and state-of-the-art research advances of deep reinforcement learning in computer vision. We start with comprehending the theories of deep learning, reinforcement learning, and deep reinforcement learning. We then propose a categorization of deep reinforcement learning methodologies and discuss their advantages and limitations. In particular, we divide deep reinforcement learning into seven main categories according to their applications in computer vision, i.e. (i)landmark localization (ii) object detection; (iii) object tracking; (iv) registration on both 2D image and 3D image volumetric data (v) image segmentation; (vi) videos analysis; and (vii) other applications. Each of these categories is further analyzed with reinforcement learning techniques, network design, and performance. Moreover, we provide a comprehensive analysis of the existing publicly available datasets and examine source code availability. Finally, we present some open issues and discuss future research directions on deep reinforcement learning in computer vision}},
  citation         = {34},
  creationdate     = {2022-12-31T02:18:31},
  eprint           = {2108.11510},
  groups           = {Perception},
  keywords         = {Computer Vision and Pattern Recognition (cs.CV), Artificial Intelligence (cs.AI)},
  modificationdate = {2023-01-01T22:34:12},
  url              = {https://arxiv.org/abs/2108.11510v1},
}

@Article{Zheng2022Apr,
  author           = {Zheng, Lianqing and Ma, Zhixiong and Zhu, Xichan and Tan, Bin and Li, Sen and Long, Kai and Sun, Weiqi and Chen, Sihan and Zhang, Lu and Wan, Mengyue and Huang, Libo and Bai, Jie},
  journal          = {arXiv},
  title            = {{TJ4DRadSet: A 4D Radar Dataset for Autonomous Driving}},
  year             = {2022},
  month            = apr,
  abstract         = {{The new generation of 4D high-resolution imaging radar provides not only a huge amount of point cloud but also additional elevation measurement, which has a great potential of 3D sensing in autonomous driving. In this paper, we introduce an autonomous driving dataset named TJ4DRadSet, including multi-modal sensors that are 4D radar, lidar, camera and GNSS, with about 40K frames in total. 7757 frames within 44 consecutive sequences in various driving scenarios are well annotated with 3D bounding boxes and track id. We provide a 4D radar-based 3D object detection baseline for our dataset to demonstrate the effectiveness of deep learning methods for 4D radar point clouds.}},
  citation         = {4},
  creationdate     = {2023-01-01T22:27:38},
  eprint           = {2204.13483},
  groups           = {Camera-Radar-Lidar, Datasets},
  keywords         = {Computer Vision and Pattern Recognition (cs.CV), Artificial Intelligence (cs.AI)},
  modificationdate = {2023-01-01T22:34:58},
  priority         = {prio1},
  url              = {https://arxiv.org/abs/2204.13483v1},
}

@Article{Sun2021May,
  author           = {Sun, Shunqiao and Zhang, Yimin D.},
  title            = {{4D Automotive Radar Sensing for Autonomous Vehicles: A Sparsity-Oriented Approach}},
  year             = {2021},
  month            = may,
  number           = {4},
  pages            = {879--891},
  volume           = {15},
  abstract         = {{We propose a high-resolution imaging radar system to enable high-fidelity four-dimensional (4D) sensing for autonomous driving, i.e., range, Doppler, azimuth, and elevation, through a joint sparsity design in frequency spectrum and array configurations. To accommodate a high number of automotive radars operating at the same frequency band while avoiding mutual interference, random sparse step-frequency waveform (RSSFW) is proposed to synthesize a large effective bandwidth to achieve high range resolution profiles. To mitigate high range sidelobes in RSSFW radars, optimal weights are designed to minimize the peak sidelobe level such that targets with a relatively small radar cross section are detectable without introducing high probability of false alarm. We extend the RSSFW concept to multi-input multi-output (MIMO) radar by applying phase codes along slow time to synthesize a two-dimensional (2D) sparse array with hundreds of virtual array elements to enable high-resolution direction finding in both azimuth and elevation. The 2D sparse array acts as a sub-Nyquist sampler of the corresponding uniform rectangular array (URA) with half-wavelength interelement spacing, and the corresponding URA response is recovered by completing a low-rank block Hankel matrix. Consequently, the high sidelobes in the azimuth and elevation spectra are greatly suppressed so that weak targets can be reliably detected. The proposed imaging radar provides point clouds with a resolution comparable to LiDAR but with a much lower cost. Numerical simulations are conducted to demonstrate the performance of the proposed 4D imaging radar system with joint sparsity in frequency spectrum and antenna arrays.}},
  citation         = {26},
  creationdate     = {2023-01-01T22:36:57},
  journaltitle     = {IEEE Journal of Selected Topics in Signal Processing},
  modificationdate = {2023-01-01T22:39:20},
  publisher        = {IEEE},
  shortjournal     = {IEEE J. Sel. Top. Signal Process.},
  url              = {https://doi.org/10.1109/JSTSP.2021.3079626},
}

@Misc{Chu2023,
  author           = {Chu, Shih-Yun and Lee, Ming-Sui},
  note             = {[Online; accessed 2. Jan. 2023]},
  title            = {{MT-DETR: Robust End-to-End Multimodal Detection With Confidence Fusion}},
  year             = {2023},
  citation         = {0},
  creationdate     = {2023-01-02T22:00:25},
  groups           = {Camera-Radar-Lidar fusion},
  modificationdate = {2023-01-07T20:18:25},
  pages            = {5252--5261},
  url              = {https://openaccess.thecvf.com/content/WACV2023/html/Chu_MT-DETR_Robust_End-to-End_Multimodal_Detection_With_Confidence_Fusion_WACV_2023_paper.html},
}

@InCollection{Wang2022Oct,
  author           = {Wang, Li and Zhang, Xinyu and Xv, Baowei and Zhang, Jinzhao and Fu, Rong and Wang, Xiaoyu and Zhu, Lei and Ren, Haibing and Lu, Pingping and Li, Jun and Liu, Huaping},
  booktitle        = {{2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}},
  publisher        = {IEEE},
  title            = {{InterFusion: Interaction-based 4D Radar and LiDAR Fusion for 3D Object Detection}},
  year             = {2022},
  month            = oct,
  pages            = {12247--12253},
  abstract         = {{Many recent works detect 3D objects by several sensor modalities for autonomous driving, where high-resolution cameras and high-line LiDARs are mostly used but relatively expensive. To achieve a balance between overall cost and detection accuracy, many multi-modal fusion techniques have been suggested. In recent years, the fusion of LiDAR and Radar has gained ever-increasing attention, especially 4D Radar, which can adapt to bad weather conditions due to its penetrability. Although features have been fused from multiple sensing modalities, most methods cannot learn interactions from different modalities, which does not make for their best use. Inspired by the self-attention mechanism, we present InterFusion, an interaction-based fusion framework, to fuse 16-line LiDAR with 4D Radar. It aggregates features from two modalities and identifies cross-modal relations between Radar and LiDAR features. In experimental evaluations on the Astyx HiRes 2019 dataset, our method outperformed the baseline by 4.20{\%} mAP in 3D and 10.76{\%} BEV mAP for the car class at the moderate level.}},
  citation         = {0},
  creationdate     = {2023-01-02T22:06:53},
  groups           = {Lidar-Radar fusion, 3D},
  issn             = {2153-0866},
  journaltitle     = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  modificationdate = {2023-04-04T09:57:54},
  shortjournal     = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  url              = {https://doi.org/10.1109/IROS47612.2022.9982123},
}

@Article{Barreto-Cubero2021Dec,
  author           = {Barreto-Cubero, Andres J. and G{\ifmmode\acute{o}\else\'{o}\fi}mez-Espinosa, Alfonso and Escobedo Cabello, Jes{\ifmmode\acute{u}\else\'{u}\fi}s Arturo and Cuan-Urquizo, Enrique and Cruz-Ram{\ifmmode\acute{\imath}\else\'{\i}\fi}rez, Sergio R.},
  title            = {{Sensor Data Fusion for a Mobile Robot Using Neural Networks}},
  year             = {2021},
  issn             = {1424-8220},
  month            = dec,
  number           = {1},
  pages            = {305},
  volume           = {22},
  abstract         = {{Mobile robots must be capable to obtain an accurate map of their surroundings to move within it. To detect different materials that might be undetectable to one sensor but not others it is necessary to construct at least a two-sensor fusion scheme. With this, it is possible to generate a 2D occupancy map in which glass obstacles are identified. An artificial neural network is used to fuse data from a tri-sensor (RealSense Stereo camera, 2D  360 {$\circ$} LiDAR, and Ultrasonic Sensors) setup capable of detecting glass and other materials typically found in indoor environments that may or may not be visible to traditional 2D LiDAR sensors, hence the expression improved LiDAR. A preprocessing scheme is implemented to filter all the outliers, project a 3D pointcloud to a 2D plane and adjust distance data. With a Neural Network as a data fusion algorithm, we integrate all the information into a single, more accurate distance-to-obstacle reading to finally generate a 2D Occupancy Grid Map (OGM) that considers all sensors information. The Robotis Turtlebot3 Waffle Pi robot is used as the experimental platform to conduct experiments given the different fusion strategies. Test results show that with such a fusion algorithm, it is possible to detect glass and other obstacles with an estimated root-mean-square error (RMSE) of 3 cm with multiple fusion strategies. Keywords: sensor data fusion; mobile robot; artificial neural network; improved LiDAR; occupancy grid map}},
  citation         = {8},
  creationdate     = {2023-01-04T01:01:06},
  groups           = {Sensor Fusion, Misc fusion},
  journaltitle     = {Sensors},
  keywords         = {sensor data fusion, mobile robot, artificial neural network, improved LiDAR, occupancy grid map},
  modificationdate = {2023-01-04T01:01:54},
  publisher        = {Multidisciplinary Digital Publishing Institute},
  shortjournal     = {Sensors},
  url              = {https://doi.org/10.3390/s22010305},
}

@Article{Rizzoli2022Jul,
  author           = {Rizzoli, Giulia and Barbato, Francesco and Zanuttigh, Pietro},
  title            = {{Multimodal Semantic Segmentation in Autonomous Driving: A Review of Current Approaches and Future Perspectives}},
  year             = {2022},
  issn             = {2227-7080},
  month            = jul,
  number           = {4},
  pages            = {90},
  volume           = {10},
  abstract         = {{The perception of the surrounding environment is a key requirement for autonomous driving systems, yet the computation of an accurate semantic representation of the scene starting from RGB information alone is very challenging. In particular, the lack of geometric information and the strong dependence on weather and illumination conditions introduce critical challenges for approaches tackling this task. For this reason, most autonomous cars exploit a variety of sensors, including color, depth or thermal cameras, LiDARs, and RADARs. How to efficiently combine all these sources of information to compute an accurate semantic description of the scene is still an unsolved task, leading to an active research field. In this survey, we start by presenting the most commonly employed acquisition setups and datasets. Then we review several different deep learning architectures for multimodal semantic segmentation. We will discuss the various techniques to combine color, depth, LiDAR, and other modalities of data at different stages of the learning architectures, and we will show how smart fusion strategies allow us to improve performances with respect to the exploitation of a single source of information. Keywords: semantic segmentation; autonomous driving; multimodal; LiDAR; depth; modality fusion; deep learning}},
  citation         = {2},
  creationdate     = {2023-01-07T20:13:23},
  groups           = {Perception},
  journaltitle     = {Technologies},
  keywords         = {semantic segmentation, autonomous driving, multimodal, LiDAR, depth, modality fusion, deep learning},
  modificationdate = {2023-01-07T20:13:50},
  publisher        = {Multidisciplinary Digital Publishing Institute},
  shortjournal     = {Technologies},
  url              = {https://doi.org/10.3390/technologies10040090},
}

@InCollection{Meyer2019Oct,
  author           = {Meyer, Michael and Kuschk, Georg},
  booktitle        = {{2019 16th European Radar Conference (EuRAD)}},
  publisher        = {IEEE},
  title            = {{Automotive Radar Dataset for Deep Learning Based 3D Object Detection}},
  year             = {2019},
  month            = oct,
  pages            = {129--132},
  abstract         = {{We present a radar-centric automotive dataset based on radar, lidar and camera data for the purpose of 3D object detection. Our main focus is to provide high resolution radar data to the research community, facilitating and stimulating research on algorithms using radar sensor data. To this end, semi-automatically generated and manually refined 3D ground truth data for object detection is provided. We describe the complete process of generating such a dataset, highlight some main features of the corresponding high-resolution radar and demonstrate its usage for level 3-5 autonomous driving applications by showing results of a deep learning based 3D object detection algorithm on this dataset. Our dataset will be available online at: www.astyx.net.}},
  citation         = {113},
  comment          = {Astyx dataset},
  creationdate     = {2023-01-07T21:42:55},
  groups           = {Camera-Radar-Lidar, 3D},
  journaltitle     = {2019 16th European Radar Conference (EuRAD)},
  modificationdate = {2023-04-04T09:58:03},
  shortjournal     = {2019 16th European Radar Conference (EuRAD)},
  url              = {https://ieeexplore.ieee.org/document/8904734},
}

@Article{Drews2022Sep,
  author           = {Drews, Florian and Feng, Di and Faion, Florian and Rosenbaum, Lars and Ulrich, Michael and Gl{\ifmmode\ddot{a}\else\"{a}\fi}ser, Claudius},
  title            = {{DeepFusion: A Robust and Modular 3D Object Detector for Lidars, Cameras and Radars}},
  year             = {2022},
  month            = sep,
  abstract         = {{We propose DeepFusion, a modular multi-modal architecture to fuse lidars, cameras and radars in different combinations for 3D object detection. Specialized feature extractors take advantage of each modality and can be exchanged easily, making the approach simple and flexible. Extracted features are transformed into bird's-eye-view as a common representation for fusion. Spatial and semantic alignment is performed prior to fusing modalities in the feature space. Finally, a detection head exploits rich multi-modal features for improved 3D detection performance. Experimental results for lidar-camera, lidar-camera-radar and camera-radar fusion show the flexibility and effectiveness of our fusion approach. In the process, we study the largely unexplored task of faraway car detection up to 225{\textasciitilde}meters, showing the benefits of our lidar-camera fusion. Furthermore, we investigate the required density of lidar points for 3D object detection and illustrate implications at the example of robustness against adverse weather conditions. Moreover, ablation studies on our camera-radar fusion highlight the importance of accurate depth estimation.}},
  citation         = {0},
  creationdate     = {2023-01-07T22:02:09},
  eprint           = {2209.12729},
  groups           = {3D},
  journaltitle     = {ArXiv e-prints},
  keywords         = {Computer Vision and Pattern Recognition (cs.CV), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), Robotics (cs.RO)},
  modificationdate = {2023-04-04T09:58:12},
  shortjournal     = {arXiv},
  url              = {https://doi.org/10.48550/arXiv.2209.12729},
}

@Article{Pfeuffer2018Jul,
  author           = {Pfeuffer, Andreas and Dietmayer, Klaus},
  title            = {{Optimal Sensor Data Fusion Architecture for Object Detection in Adverse Weather Conditions}},
  year             = {2018},
  month            = jul,
  abstract         = {{A good and robust sensor data fusion in diverse weather conditions is a quite challenging task. There are several fusion architectures in the literature, e.g. the sensor data can be fused right at the beginning (Early Fusion), or they can be first processed separately and then concatenated later (Late Fusion). In this work, different fusion architectures are compared and evaluated by means of object detection tasks, in which the goal is to recognize and localize predefined objects in a stream of data. Usually, state-of-the-art object detectors based on neural networks are highly optimized for good weather conditions, since the well-known benchmarks only consist of sensor data recorded in optimal weather conditions. Therefore, the performance of these approaches decreases enormously or even fails in adverse weather conditions. In this work, different sensor fusion architectures are compared for good and adverse weather conditions for finding the optimal fusion architecture for diverse weather situations. A new training strategy is also introduced such that the performance of the object detector is greatly enhanced in adverse weather scenarios or if a sensor fails. Furthermore, the paper responds to the question if the detection accuracy can be increased further by providing the neural network with a-priori knowledge such as the spatial calibration of the sensors.}},
  citation         = {41},
  creationdate     = {2023-01-07T23:51:48},
  groups           = {Sensor Fusion, Adverse Weather},
  journaltitle     = {ArXiv e-prints},
  keywords         = {Computer Vision and Pattern Recognition (cs.CV)},
  modificationdate = {2023-04-02T20:02:18},
  shortjournal     = {arXiv},
  url              = {https://doi.org/10.48550/arXiv.1807.02323},
}

@Article{Mees2017Jul,
  author           = {Mees, Oier and Eitel, Andreas and Burgard, Wolfram},
  title            = {{Choosing Smartly: Adaptive Multimodal Fusion for Object Detection in Changing Environments}},
  year             = {2017},
  month            = jul,
  abstract         = {{Object detection is an essential task for autonomous robots operating in dynamic and changing environments. A robot should be able to detect objects in the presence of sensor noise that can be induced by changing lighting conditions for cameras and false depth readings for range sensors, especially RGB-D cameras. To tackle these challenges, we propose a novel adaptive fusion approach for object detection that learns weighting the predictions of different sensor modalities in an online manner. Our approach is based on a mixture of convolutional neural network (CNN) experts and incorporates multiple modalities including appearance, depth and motion. We test our method in extensive robot experiments, in which we detect people in a combined indoor and outdoor scenario from RGB-D data, and we demonstrate that our method can adapt to harsh lighting changes and severe camera motion blur. Furthermore, we present a new RGB-D dataset for people detection in mixed in- and outdoor environments, recorded with a mobile robot. Code, pretrained models and dataset are available at this http URL}},
  citation         = {97},
  eprint           = {1707.05733},
  journaltitle     = {ArXiv e-prints},
  keywords         = {Robotics (cs.RO), Artificial Intelligence (cs.AI), Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG)},
  modificationdate = {2023-01-07T23:53:52},
  shortjournal     = {arXiv},
  url              = {https://doi.org/10.1109/IROS.2016.7759048},
}

@Article{Casas2021Jan,
  author           = {Casas, Sergio and Luo, Wenjie and Urtasun, Raquel},
  title            = {{IntentNet: Learning to Predict Intention from Raw Sensor Data}},
  year             = {2021},
  month            = jan,
  abstract         = {{In order to plan a safe maneuver, self-driving vehicles need to understand the intent of other traffic participants. We define intent as a combination of discrete high-level behaviors as well as continuous trajectories describing future motion. In this paper, we develop a one-stage detector and forecaster that exploits both 3D point clouds produced by a LiDAR sensor as well as dynamic maps of the environment. Our multi-task model achieves better accuracy than the respective separate modules while saving computation, which is critical to reducing reaction time in self-driving applications.}},
  citation         = {297},
  comment          = {incorporate temporal cues as mentioned in Bosch survey paper},
  creationdate     = {2023-01-07T23:57:02},
  eprint           = {2101.07907},
  journaltitle     = {ArXiv e-prints},
  keywords         = {Robotics (cs.RO), Artificial Intelligence (cs.AI), Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG)},
  modificationdate = {2023-01-07T23:58:09},
  shortjournal     = {arXiv},
  url              = {https://doi.org/10.48550/arXiv.2101.07907},
}

@Article{Luo2020Dec,
  author           = {Luo, Wenjie and Yang, Bin and Urtasun, Raquel},
  title            = {{Fast and Furious: Real Time End-to-End 3D Detection, Tracking and Motion Forecasting with a Single Convolutional Net}},
  year             = {2020},
  month            = dec,
  abstract         = {{In this paper we propose a novel deep neural network that is able to jointly reason about 3D detection, tracking and motion forecasting given data captured by a 3D sensor. By jointly reasoning about these tasks, our holistic approach is more robust to occlusion as well as sparse data at range. Our approach performs 3D convolutions across space and time over a bird's eye view representation of the 3D world, which is very efficient in terms of both memory and computation. Our experiments on a new very large scale dataset captured in several north american cities, show that we can outperform the state-of-the-art by a large margin. Importantly, by sharing computation we can perform all tasks in as little as 30 ms.}},
  citation         = {536},
  comment          = {incorporate temporal cues as mentioned in Bosch survey paper},
  creationdate     = {2023-01-07T23:58:47},
  eprint           = {2012.12395},
  groups           = {3D},
  journaltitle     = {ArXiv e-prints},
  keywords         = {Computer Vision and Pattern Recognition (cs.CV)},
  modificationdate = {2023-04-04T09:58:20},
  shortjournal     = {arXiv},
  url              = {https://doi.org/10.48550/arXiv.2012.12395},
}

@InCollection{Valada2017May,
  author           = {Valada, Abhinav and Vertens, Johan and Dhall, Ankit and Burgard, Wolfram},
  booktitle        = {{2017 IEEE International Conference on Robotics and Automation (ICRA)}},
  publisher        = {IEEE},
  title            = {{AdapNet: Adaptive semantic segmentation in adverse environmental conditions}},
  year             = {2017},
  month            = may,
  pages            = {4644--4651},
  abstract         = {{Robust scene understanding of outdoor environments using passive optical sensors is a onerous and essential task for autonomous navigation. The problem is heavily characterized by changing environmental conditions throughout the day and across seasons. Robots should be equipped with models that are impervious to these factors in order to be operable and more importantly to ensure safety in the real-world. In this paper, we propose a novel semantic segmentation architecture and the convoluted mixture of deep experts (CMoDE) fusion technique that enables a multi-stream deep neural network to learn features from complementary modalities and spectra, each of which are specialized in a subset of the input space. Our model adaptively weighs class-specific features of expert networks based on the scene condition and further learns fused representations to yield robust segmentation. We present results from experimentation on three publicly available datasets that contain diverse conditions including rain, summer, winter, dusk, fall, night and sunset, and show that our approach exceeds the state-of-the-art. In addition, we evaluate the performance of autonomously traversing several kilometres of a forested environment using only the segmentation for perception.}},
  citation         = {181},
  creationdate     = {2023-01-08T00:01:47},
  journaltitle     = {2017 IEEE International Conference on Robotics and Automation (ICRA)},
  modificationdate = {2023-01-08T00:02:08},
  shortjournal     = {2017 IEEE International Conference on Robotics and Automation (ICRA)},
  url              = {https://doi.org/10.1109/ICRA.2017.7989540},
}

@Article{Eigen2013Dec,
  author           = {Eigen, David and Ranzato, Marc'Aurelio and Sutskever, Ilya},
  title            = {{Learning Factored Representations in a Deep Mixture of Experts}},
  year             = {2013},
  month            = dec,
  abstract         = {{Mixtures of Experts combine the outputs of several "expert" networks, each of which specializes in a different part of the input space. This is achieved by training a "gating" network that maps each input to a distribution over the experts. Such models show promise for building larger networks that are still cheap to compute at test time, and more parallelizable at training time. In this this work, we extend the Mixture of Experts to a stacked model, the Deep Mixture of Experts, with multiple sets of gating and experts. This exponentially increases the number of effective experts by associating each input with a combination of experts at each layer, yet maintains a modest model size. On a randomly translated version of the MNIST dataset, we find that the Deep Mixture of Experts automatically learns to develop location-dependent ("where") experts at the first layer, and class-specific ("what") experts at the second layer. In addition, we see that the different combinations are in use when the model is applied to a dataset of speech monophones. These demonstrate effective use of all expert combinations.}},
  citation         = {181},
  creationdate     = {2023-01-08T00:03:53},
  eprint           = {1312.4314},
  journaltitle     = {ArXiv e-prints},
  keywords         = {Machine Learning (cs.LG)},
  modificationdate = {2023-01-08T00:04:11},
  shortjournal     = {arXiv},
  url              = {https://doi.org/10.48550/arXiv.1312.4314},
}

@Article{Ngiam2019Aug,
  author           = {Ngiam, Jiquan and Caine, Benjamin and Han, Wei and Yang, Brandon and Chai, Yuning and Sun, Pei and Zhou, Yin and Yi, Xi and Alsharif, Ouais and Nguyen, Patrick and Chen, Zhifeng and Shlens, Jonathon and Vasudevan, Vijay},
  title            = {{StarNet: Targeted Computation for Object Detection in Point Clouds}},
  year             = {2019},
  month            = aug,
  abstract         = {{Detecting objects from LiDAR point clouds is an important component of self-driving car technology as LiDAR provides high resolution spatial information. Previous work on point-cloud 3D object detection has re-purposed convolutional approaches from traditional camera imagery. In this work, we present an object detection system called StarNet designed specifically to take advantage of the sparse and 3D nature of point cloud data. StarNet is entirely point-based, uses no global information, has data dependent anchors, and uses sampling instead of learned region proposals. We demonstrate how this design leads to competitive or superior performance on the large Waymo Open Dataset and the KITTI detection dataset, as compared to convolutional baselines. In particular, we show how our detector can outperform a competitive baseline on Pedestrian detection on the Waymo Open Dataset by more than 7 absolute mAP while being more computationally efficient. We show how our redesign---namely using only local information and using sampling instead of learned proposals---leads to a significantly more flexible and adaptable system: we demonstrate how we can vary the computational cost of a single trained StarNet without retraining, and how we can target proposals towards areas of interest with priors and heuristics. Finally, we show how our design allows for incorporating temporal context by using detections from previous frames to target computation of the detector, which leads to further improvements in performance without additional computational cost.}},
  citation         = {78},
  comment          = {In fact, a recent work [164] states that the most performance gain for object detection in the KITTI dataset is due to data augmentation, rather than advances in network architectures

-- from Bosch survey paper},
  creationdate     = {2023-01-08T00:04:51},
  eprint           = {1908.11069},
  journaltitle     = {ArXiv e-prints},
  keywords         = {Computer Vision and Pattern Recognition (cs.CV)},
  modificationdate = {2023-01-08T00:05:49},
  shortjournal     = {arXiv},
  url              = {https://doi.org/10.48550/arXiv.1908.11069},
}

@Article{Ramos2016Dec,
  author           = {Ramos, Sebastian and Gehrig, Stefan and Pinggera, Peter and Franke, Uwe and Rother, Carsten},
  title            = {{Detecting Unexpected Obstacles for Self-Driving Cars: Fusing Deep Learning and Geometric Modeling}},
  year             = {2016},
  month            = dec,
  abstract         = {{The detection of small road hazards, such as lost cargo, is a vital capability for self-driving cars. We tackle this challenging and rarely addressed problem with a vision system that leverages appearance, contextual as well as geometric cues. To utilize the appearance and contextual cues, we propose a new deep learning-based obstacle detection framework. Here a variant of a fully convolutional network is used to predict a pixel-wise semantic labeling of (i) free-space, (ii) on-road unexpected obstacles, and (iii) background. The geometric cues are exploited using a state-of-the-art detection approach that predicts obstacles from stereo input images via model-based statistical hypothesis tests. We present a principled Bayesian framework to fuse the semantic and stereo-based detection results. The mid-level Stixel representation is used to describe obstacles in a flexible, compact and robust manner. We evaluate our new obstacle detection system on the Lost and Found dataset, which includes very challenging scenes with obstacles of only 5 cm height. Overall, we report a major improvement over the state-of-the-art, with relative performance gains of up to 50{\%}. In particular, we achieve a detection rate of over 90{\%} for distances of up to 50 m. Our system operates at 22 Hz on our self-driving platform.}},
  citation         = {254},
  creationdate     = {2023-01-08T00:07:17},
  eprint           = {1612.06573},
  journaltitle     = {ArXiv e-prints},
  keywords         = {Computer Vision and Pattern Recognition (cs.CV), Robotics (cs.RO)},
  modificationdate = {2023-01-08T00:07:43},
  shortjournal     = {arXiv},
  url              = {https://doi.org/10.48550/arXiv.1612.06573},
}

@Article{Chen2015May,
  author           = {Chen, Chenyi and Seff, Ari and Kornhauser, Alain and Xiao, Jianxiong},
  title            = {{DeepDriving: Learning Affordance for Direct Perception in Autonomous Driving}},
  year             = {2015},
  month            = may,
  abstract         = {{Today, there are two major paradigms for vision-based autonomous driving systems: mediated perception approaches that parse an entire scene to make a driving decision, and behavior reflex approaches that directly map an input image to a driving action by a regressor. In this paper, we propose a third paradigm: a direct perception approach to estimate the affordance for driving. We propose to map an input image to a small number of key perception indicators that directly relate to the affordance of a road/traffic state for driving. Our representation provides a set of compact yet complete descriptions of the scene to enable a simple controller to drive autonomously. Falling in between the two extremes of mediated perception and behavior reflex, we argue that our direct perception representation provides the right level of abstraction. To demonstrate this, we train a deep Convolutional Neural Network using recording from 12 hours of human driving in a video game and show that our model can work well to drive a car in a very diverse set of virtual environments. We also train a model for car distance estimation on the KITTI dataset. Results show that our direct perception approach can generalize well to real driving images. Source code and data are available on our project website.}},
  citation         = {1845},
  eprint           = {1505.00256},
  journaltitle     = {ArXiv e-prints},
  keywords         = {Computer Vision and Pattern Recognition (cs.CV)},
  modificationdate = {2023-01-08T00:11:57},
  shortjournal     = {arXiv},
  url              = {https://doi.org/10.48550/arXiv.1505.00256},
}

@Article{Sauer2018Jun,
  author           = {Sauer, Axel and Savinov, Nikolay and Geiger, Andreas},
  title            = {{Conditional Affordance Learning for Driving in Urban Environments}},
  year             = {2018},
  month            = jun,
  abstract         = {{Most existing approaches to autonomous driving fall into one of two categories: modular pipelines, that build an extensive model of the environment, and imitation learning approaches, that map images directly to control outputs. A recently proposed third paradigm, direct perception, aims to combine the advantages of both by using a neural network to learn appropriate low-dimensional intermediate representations. However, existing direct perception approaches are restricted to simple highway situations, lacking the ability to navigate intersections, stop at traffic lights or respect speed limits. In this work, we propose a direct perception approach which maps video input to intermediate representations suitable for autonomous navigation in complex urban environments given high-level directional inputs. Compared to state-of-the-art reinforcement and conditional imitation learning approaches, we achieve an improvement of up to 68 {\%} in goal-directed navigation on the challenging CARLA simulation benchmark. In addition, our approach is the first to handle traffic lights and speed signs by using image-level labels only, as well as smooth car-following, resulting in a significant reduction of traffic accidents in simulation.}},
  citation         = {156},
  creationdate     = {2023-01-08T00:12:32},
  eprint           = {1806.06498},
  journaltitle     = {ArXiv e-prints},
  keywords         = {Robotics (cs.RO), Machine Learning (cs.LG), Systems and Control (eess.SY)},
  modificationdate = {2023-01-08T00:12:58},
  shortjournal     = {arXiv},
  url              = {https://doi.org/10.48550/arXiv.1806.06498},
}

@Article{Tan2022Nov,
  author           = {Tan, Bin and Ma, Zhixiong and Zhu, Xichan and Li, Sen and Zheng, Lianqing and Chen, Sihan and Huang, Libo and Bai, Jie},
  title            = {{3D Object Detection for Multi-frame 4D Automotive Millimeter-wave Radar Point Cloud}},
  year             = {2022},
  issn             = {1558-1748},
  month            = nov,
  pages            = {1},
  abstract         = {{Object detection is a crucial task in autonomous driving. Currently, object-detection methods for autonomous driving systems are primarily based on information from cameras and lidar, which may experience interference from complex lighting or poor weather. At present, the four-dimensional (4D) (x, y, z, v) millimeter-wave radar can provide a denser point cloud to achieve 3D object-detection tasks that are difficult to complete with traditional millimeter-wave radar. Existing 3D object point-cloud detection algorithms are mostly based on 3D lidar, these methods are not necessarily applicable to millimeter-wave radars, which have sparser data and more noise and include velocity information. This study proposes a 3D object-detection framework based on a multi-frame 4D millimeter-wave radar point cloud. First, the ego vehicle velocity information is estimated by the millimeter-wave radar, and the relative velocity information of the millimeter-wave radar point cloud is compensated to the absolute velocity. Second, by matching between millimeter-wave radar frames, the multi-frame millimeter-wave radar point cloud is matched to the last frame. Finally, the object is detected by the proposed multi-frame millimeter-wave radar point cloud detection network. Experiments are performed using our newly recorded TJ4DRadSet dataset in a complex traffic environment. The results showed that the proposed object-detection framework outperformed the comparison methods based on the 3D mean average precision. The experimental results and methods can be used as the baseline for other multi-frame 4D millimeter-wave radar detection algorithms.}},
  citation         = {0},
  comment          = {Using TJ4D dataset},
  creationdate     = {2023-01-09T19:26:20},
  groups           = {3D},
  journaltitle     = {IEEE Sensors Journal},
  modificationdate = {2023-04-04T09:59:04},
  publisher        = {IEEE},
  shortjournal     = {IEEE Sens. J.},
  url              = {https://doi.org/10.1109/JSEN.2022.3219643},
}

@Article{Kowol2020Oct,
  author           = {Kowol, Kamil and Rottmann, Matthias and Bracke, Stefan and Gottschalk, Hanno},
  title            = {{YOdar: Uncertainty-based Sensor Fusion for Vehicle Detection with Camera and Radar Sensors}},
  year             = {2020},
  month            = oct,
  abstract         = {{In this work, we present an uncertainty-based method for sensor fusion with camera and radar data. The outputs of two neural networks, one processing camera and the other one radar data, are combined in an uncertainty aware manner. To this end, we gather the outputs and corresponding meta information for both networks. For each predicted object, the gathered information is post-processed by a gradient boosting method to produce a joint prediction of both networks. In our experiments we combine the YOLOv3 object detection network with a customized $1D$ radar segmentation network and evaluate our method on the nuScenes dataset. In particular we focus on night scenes, where the capability of object detection networks based on camera data is potentially handicapped. Our experiments show, that this approach of uncertainty aware fusion, which is also of very modular nature, significantly gains performance compared to single sensor baselines and is in range of specifically tailored deep learning based fusion approaches.}},
  citation         = {12},
  creationdate     = {2023-01-09T20:18:47},
  eprint           = {2010.03320},
  groups           = {Camera-Radar fusion},
  journaltitle     = {ArXiv e-prints},
  keywords         = {Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG)},
  modificationdate = {2023-01-09T20:19:40},
  shortjournal     = {arXiv},
  url              = {https://doi.org/10.48550/arXiv.2010.03320},
}

@Misc{Lee2021,
  author           = {Lee, Wei-Yu and Jovanov, L. and Philips, W.},
  note             = {[Online; accessed 12. Jan. 2023]},
  title            = {{Semantic-Guided Radar-Vision Fusion for Depth Estimation and Object Detection}},
  year             = {2021},
  abstract         = {{Quantitative and qualitative results on the public nuScenes dataset show that depth estimation results are on par with the state-of-the-art method, and object detection results compare favourably to the baseline and other models. In the last decade, radar is gaining its importance in perception modules of cars and infrastructure, due to its robustness against various weather and light conditions. Although radar has numerous advantages, the properties of its output signal also make the development of fusion scheme a challenging task. Most of the prior work does not exploit full potential of fusion due to the abstraction, sparsity and low quality of radar data. In this paper, we propose a novel fusion scheme to overcome this limitation by introducing semantic understanding to assist the fusion process. The sparse radar point-cloud and vision data is transformed to robust and reliable depth maps and fused in a multi-scale detection network for further exploiting the complementary information. In our experiments, we evaluate the proposed fusion scheme on both depth estimation and 2D object detection problems. Object detection results compare favourably to the state-of-the-art and demonstrate the effectiveness of the proposed scheme. Depth map estimation results are on par with the state-of-the-art on depth from RGB estimation. The ablation studies also show the effectiveness of the proposed components. scheme to perform object detection. Quantitative and qualitative results on the public nuScenes dataset con{fi}rm that our depth estimation results are on par with the state-of-the-art method, and object detection results compare favourably to the baseline and other models. Our ablation studies also clarify the effectiveness of the proposed components.}},
  citation         = {3},
  creationdate     = {2023-01-12T01:18:22},
  groups           = {Camera-Radar fusion},
  modificationdate = {2023-01-12T01:18:56},
  url              = {https://www.semanticscholar.org/paper/Semantic-Guided-Radar-Vision-Fusion-for-Depth-and-Lee-Jovanov/432fd37984cb21c3e4265de9096f50cc6dbc6ea3},
}

@Article{Zhang2023Feb,
  author           = {Zhang, Yuxiao and Carballo, Alexander and Yang, Hanting and Takeda, Kazuya},
  title            = {{Perception and sensing for autonomous vehicles under adverse weather conditions: A survey}},
  year             = {2023},
  issn             = {0924-2716},
  month            = feb,
  pages            = {146--177},
  volume           = {196},
  abstract         = {{Automated Driving Systems (ADS) open up a new domain for the automotive industry and offer new possibilities for future transportation with higher efficiency and comfortable experiences. However, perception and sensing for autonomous driving under adverse weather conditions have been the problem that keeps autonomous vehicles (AVs) from going to higher autonomy for a long time. This paper assesses the influences and challenges that weather brings to ADS sensors in a systematic way, and surveys the solutions against inclement weather conditions. State-of-the-art algorithms and deep learning methods on perception enhancement with regard to each kind of weather, weather status classification, and remote sensing are thoroughly reported. Sensor fusion solutions, weather conditions coverage in currently available datasets, simulators, and experimental facilities are categorized. Additionally, potential ADS sensor candidates and developing research directions such as V2X (Vehicle to Everything) technologies are discussed. By looking into all kinds of major weather problems, and reviewing both sensor and computer science solutions in recent years, this survey points out the main moving trends of adverse weather problems in perception and sensing, i.e., advanced sensor fusion and more sophisticated machine learning techniques; and also the limitations brought by emerging 1550 nm LiDARs. In general, this work contributes a holistic overview of the obstacles and directions of perception and sensing research development in terms of adverse weather conditions.}},
  citation         = {3},
  comment          = {same as "Autonomous Driving in Adverse Weather Conditions: A Survey"},
  creationdate     = {2023-01-14T05:51:58},
  groups           = {Adverse Weather, Survey},
  journaltitle     = {ISPRS Journal of Photogrammetry and Remote Sensing},
  keywords         = {Perception and sensing, Adverse weather conditions, Autonomous driving, LiDAR, Sensor fusion, Deep learning},
  modificationdate = {2023-04-02T19:56:23},
  priority         = {prio3},
  publisher        = {Elsevier},
  readstatus       = {read},
  shortjournal     = {ISPRS J. Photogramm. Remote Sens.},
  url              = {https://doi.org/10.1016/j.isprsjprs.2022.12.021},
}

@InCollection{Christian2022Dec,
  author           = {Christian, Albert Budi and Wu, Yu-Hsuan and Lin, Chih-Yu and Van, Lan-Da and Tseng, Yu-Chee},
  booktitle        = {{2022 IEEE 15th International Symposium on Embedded Multicore/Many-core Systems-on-Chip (MCSoC)}},
  publisher        = {IEEE},
  title            = {{Radar and Camera Fusion for Object Forecasting in Driving Scenarios}},
  year             = {2022},
  month            = dec,
  pages            = {105--111},
  abstract         = {{In this paper, we propose a sensor fusion architecture that combines data collected by the camera and radars and utilizes radar velocity for road users' trajectory prediction in real-world driving scenarios. This architecture is multi-stage, following the detect-track-predict paradigm. In the detection stage, camera images and radar point clouds are used to detect objects in the vehicle's surroundings by adopting two object detection models. The detected objects are tracked by an online tracking method. We also design a radar association method to extract radar velocity for an object. In the prediction stage, we build a recurrent neural network to process an object's temporal sequence of positions and velocities and predict future trajectories. Experiments on the real-world autonomous driving nuScenes dataset show that the radar velocity mainly affects the center of the bounding box representing the position of an object and thus improves the prediction performance.}},
  citation         = {0},
  creationdate     = {2023-01-19T13:11:03},
  issn             = {2771-3075},
  journaltitle     = {2022 IEEE 15th International Symposium on Embedded Multicore/Many-core Systems-on-Chip (MCSoC)},
  modificationdate = {2023-01-19T13:11:26},
  shortjournal     = {2022 IEEE 15th International Symposium on Embedded Multicore/Many-core Systems-on-Chip (MCSoC)},
  url              = {https://doi.org/10.1109/MCSoC57363.2022.00026},
}

@InCollection{Yadav2020Oct,
  author           = {Yadav, Ritu and Vierling, Axel and Berns, Karsten},
  booktitle        = {{2020 IEEE International Conference on Image Processing (ICIP)}},
  publisher        = {IEEE},
  title            = {{Radar + RGB Fusion For Robust Object Detection In Autonomous Vehicle}},
  year             = {2020},
  month            = oct,
  pages            = {1986--1990},
  abstract         = {{This paper presents two variations of architecture referred to as RANet and BIRANet. The proposed architecture aims to use radar signal data along with RGB camera images to form a robust detection network that works efficiently, even in variable lighting and weather conditions such as rain, dust, fog, and others. First, radar information is fused in the feature extractor network. Second, radar points are used to generate guided anchors. Third, a method is proposed to improve region proposal network [1] targets. BIRANet yields 72.3/75.3{\%} average AP/AR on the NuScenes [2] dataset, which is better than the performance of our base network Faster-RCNN with Feature pyramid network(FFPN) [3]. RANet gives 69/71.9{\%} average AP/AR on the same dataset, which is reasonably acceptable performance. Also, both BIRANet and RANet are evaluated to be robust towards the noise.}},
  citation         = {17},
  creationdate     = {2023-01-22T22:06:49},
  groups           = {Camera-Radar fusion},
  issn             = {2381-8549},
  journaltitle     = {2020 IEEE International Conference on Image Processing (ICIP)},
  modificationdate = {2023-01-22T22:07:24},
  shortjournal     = {2020 IEEE International Conference on Image Processing (ICIP)},
  url              = {https://doi.org/10.1109/ICIP40778.2020.9191046},
}

@InCollection{Kurup2023Jan,
  author           = {Kurup, Akhil M. and Bos, Jeremy P.},
  booktitle        = {{Optical Engineering, Vol. 62, Issue 3}},
  publisher        = {SPIE},
  title            = {{Winter adverse driving dataset for autonomy in inclement winter weather}},
  year             = {2023},
  month            = jan,
  number           = {3},
  pages            = {031207},
  volume           = {62},
  abstract         = {{The availability of public datasets with annotated light detection and ranging (LiDAR) point clouds has advanced autonomous driving tasks, such as semantic and panoptic segmentation. However, there is a lack of datasets focused on inclement weather. Snow and rain degrade visibility and introduce noise in LiDAR point clouds. In this article, summarize a 3-year winter weather data collection effort and introduce the winter adverse driving dataset. It is the first multimodal dataset featuring moderate to severe winter weather{\ifmmode---\else\textemdash\fi}weather that would cause an experienced driver to alter their driving behavior. Our dataset features exclusively events with heavy snowfall and occasional white-out conditions. Data are collected using high-resolution LiDAR, visible as well as near infrared (IR) cameras, a long wave IR camera, forward-facing radio detection and ranging, and Global Navigation Satellite Systems/Inertial Measurement Unit units. Our dataset is unique in the range of sensors and the severity of the conditions observed. It is also one of the only data sets to focus on rural and semi-rural environments. Over 36 TB of adverse winter data have been collected over 3 years. We also provide dense point-wise labels to sequential LiDAR scans collected in severe winter weather. We have labeled and will make available around 1000 sequential LiDAR scenes, amounting to over 7 GB or 3.6 billion labeled points. This is the first point-wise semantically labeled dataset to include falling snow.}},
  citation         = {0},
  creationdate     = {2023-01-27T02:56:47},
  groups           = {Camera-Radar-Lidar},
  issn             = {0091-3286},
  journaltitle     = {Optical Engineering},
  keywords         = {LIDAR, Adverse weather, Sensors, Long wavelength infrared, Cameras, Optical engineering, Point clouds, Near infrared, Visibility, Roads},
  modificationdate = {2023-03-09T00:08:23},
  priority         = {prio2},
  shortjournal     = {Opt. Eng.},
  url              = {https://doi.org/10.1117/1.OE.62.3.031207},
}

@Misc{Bijelic2019,
  author           = {Bijelic, Mario and Gruber, Tobias and Ritter, W. and Dietmayer, K.},
  note             = {[Online; accessed 1. Feb. 2023]},
  title            = {{Automotive Sensor Performance in Adverse Weather}},
  year             = {2019},
  abstract         = {{To assess both enhancement and robust sensor technologies for autonomous driving in various difficult weather situations, novel evaluation metrics as well as dataset baselines are necessary. Vision in all seasons is one of the key components enabling the perception for autonomous driving in various difficult weather situations aside sunny California. Towards achieving this ultimate goal, different kind of problems have to be faced. Adverse weather noise is complex, it can have multiple appearances and disturbs each sensor technology differently. This can be circumvented by enhanced and robust sensor technologies, where the performance is increased and robust downstream algorithms can interpret the perceived sensor signals. To assess both enhancement directions novel evaluation metrics as well as dataset baselines are necessary.}},
  citation         = {0},
  creationdate     = {2023-02-01T22:47:26},
  modificationdate = {2023-02-01T22:47:47},
  url              = {https://www.semanticscholar.org/paper/Automotive-Sensor-Performance-in-Adverse-Weather-Bijelic-Gruber/1a5c09b014a7a5c28826e6d8f39f9e55bf57770f},
}

@Article{Zou2023Feb,
  author           = {Zou, Junyi and Zheng, Hongyi and Wang, Feng},
  title            = {{Real-Time Target Detection System for Intelligent Vehicles Based on Multi-Source Data Fusion}},
  year             = {2023},
  issn             = {1424-8220},
  month            = feb,
  number           = {4},
  pages            = {1823},
  volume           = {23},
  abstract         = {{To improve the identification accuracy of target detection for intelligent vehicles, a real-time target detection system based on the multi-source fusion method is proposed. Based on the ROS melodic software development environment and the NVIDIA Xavier hardware development platform, this system integrates sensing devices such as millimeter-wave radar and camera, and it can realize functions such as real-time target detection and tracking. At first, the image data can be processed by the You Only Look Once v5 network, which can increase the speed and accuracy of identification; secondly, the millimeter-wave radar data are processed to provide a more accurate distance and velocity of the targets. Meanwhile, in order to improve the accuracy of the system, the sensor fusion method is used. The radar point cloud is projected onto the image, then through space-time synchronization, region of interest (ROI) identification, and data association, the target-tracking information is presented. At last, field tests of the system are conducted, the results of which indicate that the system has a more accurate recognition effect and scene adaptation ability in complex scenes. Keywords: machine vision; millimeter-wave radar; multi-source data fusion; YOLOv5 algorithm; target detection}},
  citation         = {0},
  creationdate     = {2023-02-08T22:00:09},
  groups           = {Camera-Radar fusion},
  journaltitle     = {Sensors},
  keywords         = {machine vision, millimeter-wave radar, multi-source data fusion, YOLOv5 algorithm, target detection},
  modificationdate = {2023-02-08T22:00:58},
  publisher        = {Multidisciplinary Digital Publishing Institute},
  shortjournal     = {Sensors},
  url              = {https://doi.org/10.3390/s23041823},
}

@Article{Senel2023Feb,
  author           = {Senel, Numan and Elger, Gordon and Kefferp{\ifmmode\ddot{u}\else\"{u}\fi}tz, Klaus and Doycheva, Kristina},
  title            = {{Multi-Sensor Data Fusion for Real-Time Multi-Object Tracking}},
  year             = {2023},
  issn             = {2227-9717},
  month            = feb,
  number           = {2},
  pages            = {501},
  volume           = {11},
  abstract         = {{Sensor data fusion is essential for environmental perception within smart traffic applications. By using multiple sensors cooperatively, the accuracy and probability of the perception are increased, which is crucial for critical traffic scenarios or under bad weather conditions. In this paper, a modular real-time capable multi-sensor fusion framework is presented and tested to fuse data on the object list level from distributed automotive sensors (cameras, radar, and LiDAR). The modular multi-sensor fusion architecture receives an object list (untracked objects) from each sensor. The fusion framework combines classical data fusion algorithms, as it contains a coordinate transformation module, an object association module (Hungarian algorithm), an object tracking module (unscented Kalman filter), and a movement compensation module. Due to the modular design, the fusion framework is adaptable and does not rely on the number of sensors or their types. Moreover, the method continues to operate because of this adaptable design in case of an individual sensor failure. This is an essential feature for safety-critical applications. The architecture targets environmental perception in challenging time-critical applications. The developed fusion framework is tested using simulation and public domain experimental data. Using the developed framework, sensor fusion is obtained well below 10 milliseconds of computing time using an AMD Ryzen 7 5800H mobile processor and the Python programming language. Furthermore, the object-level multi-sensor approach enables the detection of changes in the extrinsic calibration of the sensors and potential sensor failures. A concept was developed to use the multi-sensor framework to identify sensor malfunctions. This feature will become extremely important in ensuring the functional safety of the sensors for autonomous driving.}},
  citation         = {0},
  creationdate     = {2023-02-12T13:30:05},
  journaltitle     = {Processes},
  keywords         = {environmental perception, sensor fusion, autonomous vehicle, unscented Kalman filter, object tracking, roadside units},
  modificationdate = {2023-02-12T13:30:29},
  publisher        = {Multidisciplinary Digital Publishing Institute},
  shortjournal     = {Processes},
  url              = {https://doi.org/10.3390/pr11020501},
}

@Article{Shi2023Feb,
  author           = {Shi, Peilun and Peng, Jiachuan and Qiu, Jianing and Ju, Xinwei and Lo, Frank Po Wen and Lo, Benny},
  title            = {{EVEN: An Event-Based Framework for Monocular Depth Estimation at Adverse Night Conditions}},
  year             = {2023},
  month            = feb,
  abstract         = {{Accurate depth estimation under adverse night conditions has practical impact and applications, such as on autonomous driving and rescue robots. In this work, we studied monocular depth estimation at night time in which various adverse weather, light, and different road conditions exist, with data captured in both RGB and event modalities. Event camera can better capture intensity changes by virtue of its high dynamic range (HDR), which is particularly suitable to be applied at adverse night conditions in which the amount of light is limited in the scene. Although event data can retain visual perception that conventional RGB camera may fail to capture, the lack of texture and color information of event data hinders its applicability to accurately estimate depth alone. To tackle this problem, we propose an event-vision based framework that integrates low-light enhancement for the RGB source, and exploits the complementary merits of RGB and event data. A dataset that includes paired RGB and event streams, and ground truth depth maps has been constructed. Comprehensive experiments have been conducted, and the impact of different adverse weather combinations on the performance of framework has also been investigated. The results have shown that our proposed framework can better estimate monocular depth at adverse nights than six baselines.}},
  citation         = {0},
  creationdate     = {2023-02-12T13:33:43},
  eprint           = {2302.03860},
  groups           = {Event Camera},
  journaltitle     = {ArXiv e-prints},
  keywords         = {Computer Vision and Pattern Recognition (cs.CV)},
  modificationdate = {2023-04-04T00:22:45},
  shortjournal     = {arXiv},
  url              = {https://doi.org/10.48550/arXiv.2302.03860},
}

@Article{Liu2021Feb,
  author           = {Liu, Ze and Cai, Yingfeng and Wang, Hai and Chen, Long and Gao, Hongbo and Jia, Yunyi and Li, Yicheng},
  title            = {{Robust Target Recognition and Tracking of Self-Driving Cars With Radar and Camera Information Fusion Under Severe Weather Conditions}},
  year             = {2021},
  issn             = {1558-0016},
  month            = feb,
  number           = {7},
  pages            = {6640--6653},
  volume           = {23},
  abstract         = {{Radar and camera information fusion sensing methods are used to solve the inherent shortcomings of the single sensor in severe weather. Our fusion scheme uses radar as the main hardware and camera as the auxiliary hardware framework. At the same time, the Mahalanobis distance is used to match the observed values of the target sequence. Data fusion based on the joint probability function method. Moreover, the algorithm was tested using actual sensor data collected from a vehicle, performing real-time environment perception. The test results show that radar and camera fusion algorithms perform better than single sensor environmental perception in severe weather, which can effectively reduce the missed detection rate of autonomous vehicle environment perception in severe weather. The fusion algorithm improves the robustness of the environment perception system and provides accurate environment perception information for the decision-making system and control system of autonomous vehicles.}},
  citation         = {102},
  creationdate     = {2023-02-15T00:20:31},
  groups           = {Camera-Radar fusion},
  journaltitle     = {IEEE Transactions on Intelligent Transportation Systems},
  modificationdate = {2023-04-01T20:59:48},
  publisher        = {IEEE},
  shortjournal     = {IEEE Trans. Intell. Transp. Syst.},
  url              = {https://doi.org/10.1109/TITS.2021.3059674},
}

@Article{Li2023Feb,
  author           = {Li, Hongkang and Wang, Meng and Liu, Sijia and Chen, Pin-yu},
  title            = {{A Theoretical Understanding of shallow Vision Transformers: Learning, Generalization, and Sample Complexity}},
  year             = {2023},
  month            = feb,
  abstract         = {{Vision Transformers (ViTs) with self-attention modules have recently achieved great empirical success in many vision tasks. Due to non-convex interactions across layers, however, theoretical learning and generalization analysis is mostly elusive. Based on a data model characterizing both label-relevant and label-irrelevant tokens, this paper provides the first theoretical analysis of training a shallow ViT, i.e., one self-attention layer followed by a two-layer perceptron, for a classification task. We characterize the sample complexity to achieve a zero generalization error. Our sample complexity bound is positively correlated with the inverse of the fraction of label-relevant tokens, the token noise level, and the initial model error. We also prove that a training process using stochastic gradient descent (SGD) leads to a sparse attention map, which is a formal verification of the general intuition about the success of attention. Moreover, this paper indicates that a proper token sparsification can improve the test performance by removing label-irrelevant and/or noisy tokens, including spurious correlations. Empirical experiments on synthetic data and CIFAR-10 dataset justify our theoretical results and generalize to deeper ViTs.}},
  citation         = {1},
  creationdate     = {2023-02-15T00:38:47},
  eprint           = {2302.06015},
  groups           = {Perception},
  journaltitle     = {ArXiv e-prints},
  keywords         = {Machine Learning (cs.LG), Computer Vision and Pattern Recognition (cs.CV), Machine Learning (stat.ML)},
  modificationdate = {2023-02-15T00:39:51},
  shortjournal     = {arXiv},
  url              = {https://doi.org/10.48550/arXiv.2302.06015},
}

@Article{Singh2023Feb,
  author           = {Singh, Apoorv},
  title            = {{Vision-RADAR fusion for Robotics BEV Detections: A Survey}},
  year             = {2023},
  month            = feb,
  abstract         = {{Due to the trending need of building autonomous robotic perception system, sensor fusion has attracted a lot of attention amongst researchers and engineers to make best use of cross-modality information. However, in order to build a robotic platform at scale we need to emphasize on autonomous robot platform bring-up cost as well. Cameras and radars, which inherently includes complementary perception information, has potential for developing autonomous robotic platform at scale. However, there is a limited work around radar fused with Vision, compared to LiDAR fused with vision work. In this paper, we tackle this gap with a survey on Vision-Radar fusion approaches for a BEV object detection system. First we go through the background information viz., object detection tasks, choice of sensors, sensor setup, benchmark datasets and evaluation metrics for a robotic perception system. Later, we cover per-modality (Camera and RADAR) data representation, then we go into detail about sensor fusion techniques based on sub-groups viz., early-fusion, deep-fusion, and late-fusion to easily understand the pros and cons of each method. Finally, we propose possible future trends for vision-radar fusion to enlighten future research. Regularly updated summary can be found at: this https URL}},
  citation         = {3},
  comment          = {3D object detection},
  creationdate     = {2023-02-18T22:15:30},
  eprint           = {2302.06643},
  groups           = {3D},
  journaltitle     = {ArXiv e-prints},
  keywords         = {Computer Vision and Pattern Recognition (cs.CV)},
  modificationdate = {2023-04-04T09:59:12},
  priority         = {prio1},
  shortjournal     = {arXiv},
  url              = {https://doi.org/10.48550/arXiv.2302.06643},
}

@Article{Zheng2023Feb,
  author           = {Zheng, Tianyue and Li, Ang and Chen, Zhe and Wang, Hongbo and Luo, Jun},
  title            = {{AutoFed: Heterogeneity-Aware Federated Multimodal Learning for Robust Autonomous Driving}},
  year             = {2023},
  month            = feb,
  abstract         = {{Object detection with on-board sensors (e.g., lidar, radar, and camera) play a crucial role in autonomous driving (AD), and these sensors complement each other in modalities. While crowdsensing may potentially exploit these sensors (of huge quantity) to derive more comprehensive knowledge, {\ifmmode\backslash\else\textbackslash\fi}textit{$\lbrace$}federated learning{$\rbrace$} (FL) appears to be the necessary tool to reach this potential: it enables autonomous vehicles (AVs) to train machine learning models without explicitly sharing raw sensory data. However, the multimodal sensors introduce various data heterogeneity across distributed AVs (e.g., label quantity skews and varied modalities), posing critical challenges to effective FL. To this end, we present AutoFed as a heterogeneity-aware FL framework to fully exploit multimodal sensory data on AVs and thus enable robust AD. Specifically, we first propose a novel model leveraging pseudo-labeling to avoid mistakenly treating unlabeled objects as the background. We also propose an autoencoder-based data imputation method to fill missing data modality (of certain AVs) with the available ones. To further reconcile the heterogeneity, we finally present a client selection mechanism exploiting the similarities among client models to improve both training stability and convergence rate. Our experiments on benchmark dataset confirm that AutoFed substantially improves over status quo approaches in both precision and recall, while demonstrating strong robustness to adverse weather conditions.}},
  citation         = {0},
  creationdate     = {2023-02-23T17:53:30},
  groups           = {Lidar-Radar fusion},
  journaltitle     = {ArXiv e-prints},
  keywords         = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Computer Vision and Pattern Recognition (cs.CV)},
  modificationdate = {2023-02-23T17:54:41},
  shortjournal     = {arXiv},
  url              = {https://doi.org/10.48550/arXiv.2302.08646},
}

@Article{Singh2023Feb,
  author           = {Singh, Apoorv},
  title            = {{Transformer-Based Sensor Fusion for Autonomous Driving: A Survey}},
  year             = {2023},
  month            = feb,
  abstract         = {{Sensor fusion is an essential topic in many perception systems, such as autonomous driving and robotics. Transformers-based detection head and CNN-based feature encoder to extract features from raw sensor-data has emerged as one of the best performing sensor-fusion 3D-detection-framework, according to the dataset leaderboards. In this work we provide an in-depth literature survey of transformer based 3D-object detection task in the recent past, primarily focusing on the sensor fusion. We also briefly go through the Vision transformers (ViT) basics, so that readers can easily follow through the paper. Moreover, we also briefly go through few of the non-transformer based less-dominant methods for sensor fusion for autonomous driving. In conclusion we summarize with sensor-fusion trends to follow and provoke future research. More updated summary can be found at: this https URL}},
  citation         = {0},
  creationdate     = {2023-02-25T11:23:39},
  eprint           = {2302.11481},
  journaltitle     = {ArXiv e-prints},
  keywords         = {Computer Vision and Pattern Recognition (cs.CV)},
  modificationdate = {2023-02-25T11:24:00},
  shortjournal     = {arXiv},
  url              = {https://doi.org/10.48550/arXiv.2302.11481},
}

@Article{Palffy2022Feb,
  author           = {Palffy, Andras and Pool, Ewoud and Baratam, Srimannarayana and Kooij, Julian F. P. and Gavrila, Dariu M.},
  title            = {{Multi-Class Road User Detection With 3+1D Radar in the View-of-Delft Dataset}},
  year             = {2022},
  issn             = {2377-3766},
  month            = feb,
  number           = {2},
  pages            = {4961--4968},
  volume           = {7},
  abstract         = {{Next-generation automotive radars provide elevation data in addition to range-, azimuth- and Doppler velocity. In this experimental study, we apply a state-of-the-art object detector (PointPillars), previously used for LiDAR 3D data, to such 3+1D radar data (where 1D refers to Doppler). In ablation studies, we first explore the benefits of the additional elevation information, together with that of Doppler, radar cross section and temporal accumulation, in the context of multi-class road user detection. We subsequently compare object detection performance on the radar and LiDAR point clouds, object class-wise and as a function of distance. To facilitate our experimental study, we present the novel View-of-Delft (VoD) automotive dataset. It contains 8693 frames of synchronized and calibrated 64-layer LiDAR-, (stereo) camera-, and 3+1D radar-data acquired in complex, urban traffic. It consists of 123106 3D bounding box annotations of both moving and static objects, including 26587 pedestrian, 10800 cyclist and 26949 car labels. Our results show that object detection on 64-layer LiDAR data still outperforms that on 3+1D radar data, but the addition of elevation information and integration of successive radar scans helps close the gap. The VoD dataset is made freely available for scientific benchmarking at https://intelligent-vehicles.org/datasets/view-of-delft/ .}},
  citation         = {11},
  creationdate     = {2023-02-26T13:25:25},
  journaltitle     = {IEEE Robotics and Automation Letters},
  modificationdate = {2023-02-26T13:25:49},
  publisher        = {IEEE},
  shortjournal     = {IEEE Rob. Autom. Lett.},
  url              = {https://doi.org/10.1109/LRA.2022.3147324},
}

@Article{Prakash2021Apr,
  author           = {Prakash, Aditya and Chitta, Kashyap and Geiger, Andreas},
  title            = {{Multi-Modal Fusion Transformer for End-to-End Autonomous Driving}},
  year             = {2021},
  month            = apr,
  abstract         = {{How should representations from complementary sensors be integrated for autonomous driving? Geometry-based sensor fusion has shown great promise for perception tasks such as object detection and motion forecasting. However, for the actual driving task, the global context of the 3D scene is key, e.g. a change in traffic light state can affect the behavior of a vehicle geometrically distant from that traffic light. Geometry alone may therefore be insufficient for effectively fusing representations in end-to-end driving models. In this work, we demonstrate that imitation learning policies based on existing sensor fusion methods under-perform in the presence of a high density of dynamic agents and complex scenarios, which require global contextual reasoning, such as handling traffic oncoming from multiple directions at uncontrolled intersections. Therefore, we propose TransFuser, a novel Multi-Modal Fusion Transformer, to integrate image and LiDAR representations using attention. We experimentally validate the efficacy of our approach in urban settings involving complex scenarios using the CARLA urban driving simulator. Our approach achieves state-of-the-art driving performance while reducing collisions by 76{\%} compared to geometry-based fusion.}},
  citation         = {191},
  creationdate     = {2023-02-28T21:23:14},
  eprint           = {2104.09224},
  groups           = {Camera-Lidar fusion},
  journaltitle     = {ArXiv e-prints},
  keywords         = {Computer Vision and Pattern Recognition (cs.CV), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), Robotics (cs.RO)},
  modificationdate = {2023-02-28T21:35:32},
  shortjournal     = {arXiv},
  url              = {https://doi.org/10.48550/arXiv.2104.09224},
}

@Article{Pinto2023Feb,
  author           = {Pinto, Andr{\ifmmode\acute{e}\else\'{e}\fi} Susano and Kolesnikov, Alexander and Shi, Yuge and Beyer, Lucas and Zhai, Xiaohua},
  title            = {{Tuning computer vision models with task rewards}},
  year             = {2023},
  month            = feb,
  abstract         = {{Misalignment between model predictions and intended usage can be detrimental for the deployment of computer vision models. The issue is exacerbated when the task involves complex structured outputs, as it becomes harder to design procedures which address this misalignment. In natural language processing, this is often addressed using reinforcement learning techniques that align models with a task reward. We adopt this approach and show its surprising effectiveness across multiple computer vision tasks, such as object detection, panoptic segmentation, colorization and image captioning. We believe this approach has the potential to be widely useful for better aligning models with a diverse range of computer vision tasks.}},
  citation         = {0},
  creationdate     = {2023-03-07T10:43:38},
  eprint           = {2302.08242},
  journaltitle     = {ArXiv e-prints},
  keywords         = {Computer Vision and Pattern Recognition (cs.CV)},
  modificationdate = {2023-03-07T10:44:06},
  shortjournal     = {arXiv},
  url              = {https://doi.org/10.48550/arXiv.2302.08242},
}

@Article{Lin2023Mar,
  author           = {Lin, Jia-Jheng and Guo, Jiun-In and Shivanna, Vinay Malligere and Chang, Ssu-Yuan},
  title            = {{Deep Learning Derived Object Detection and Tracking Technology Based on Sensor Fusion of Millimeter-Wave Radar/Video and Its Application on Embedded Systems}},
  year             = {2023},
  issn             = {1424-8220},
  month            = mar,
  number           = {5},
  pages            = {2746},
  volume           = {23},
  abstract         = {{This paper proposes a deep learning-based mmWave radar and RGB camera sensor early fusion method for object detection and tracking and its embedded system realization for ADAS applications. The proposed system can be used not only in ADAS systems but also to be applied to smart Road Side Units (RSU) in transportation systems to monitor real-time traffic flow and warn road users of probable dangerous situations. As the signals of mmWave radar are less affected by bad weather and lighting such as cloudy, sunny, snowy, night-light, and rainy days, it can work efficiently in both normal and adverse conditions. Compared to using an RGB camera alone for object detection and tracking, the early fusion of the mmWave radar and RGB camera technology can make up for the poor performance of the RGB camera when it fails due to bad weather and/or lighting conditions. The proposed method combines the features of radar and RGB cameras and directly outputs the results from an end-to-end trained deep neural network. Additionally, the complexity of the overall system is also reduced such that the proposed method can be implemented on PCs as well as on embedded systems like NVIDIA Jetson Xavier at 17.39 fps. Keywords: millimeter-wave radar; depth sensor; sensor fusion; object detection and tracking; early fusion; deep learning}},
  citation         = {0},
  creationdate     = {2023-03-07T13:58:26},
  groups           = {Camera-Radar fusion},
  journaltitle     = {Sensors},
  keywords         = {millimeter-wave radar, depth sensor, sensor fusion, object detection and tracking, early fusion, deep learning},
  modificationdate = {2023-03-07T13:58:50},
  publisher        = {Multidisciplinary Digital Publishing Institute},
  shortjournal     = {Sensors},
  url              = {https://doi.org/10.3390/s23052746},
}

@Article{Ouaknine2022Mar,
  author           = {Ouaknine, Arthur},
  title            = {{Deep learning for radar data exploitation of autonomous vehicle}},
  year             = {2022},
  month            = mar,
  abstract         = {{Autonomous driving requires a detailed understanding of complex driving scenes. The redundancy and complementarity of the vehicle's sensors provide an accurate and robust comprehension of the environment, thereby increasing the level of performance and safety. This thesis focuses the on automotive RADAR, which is a low-cost active sensor measuring properties of surrounding objects, including their relative speed, and has the key advantage of not being impacted by adverse weather conditions. With the rapid progress of deep learning and the availability of public driving datasets, the perception ability of vision-based driving systems has considerably improved. The RADAR sensor is seldom used for scene understanding due to its poor angular resolution, the size, noise, and complexity of RADAR raw data as well as the lack of available datasets. This thesis proposes an extensive study of RADAR scene understanding, from the construction of an annotated dataset to the conception of adapted deep learning architectures. First, this thesis details approaches to tackle the current lack of data. A simple simulation as well as generative methods for creating annotated data will be presented. It will also describe the CARRADA dataset, composed of synchronised camera and RADAR data with a semi-automatic annotation method. This thesis then present a proposed set of deep learning architectures with their associated loss functions for RADAR semantic segmentation. It also introduces a method to open up research into the fusion of LiDAR and RADAR sensors for scene understanding. Finally, this thesis exposes a collaborative contribution, the RADIal dataset with synchronised High-Definition (HD) RADAR, LiDAR and camera. A deep learning architecture is also proposed to estimate the RADAR signal processing pipeline while performing multitask learning for object detection and free driving space segmentation.}},
  citation         = {0},
  creationdate     = {2023-03-09T00:06:01},
  groups           = {Survey},
  journaltitle     = {ArXiv e-prints},
  keywords         = {Computer Vision and Pattern Recognition (cs.CV)},
  modificationdate = {2023-04-02T17:05:28},
  priority         = {prio2},
  shortjournal     = {arXiv},
  url              = {https://doi.org/10.48550/arXiv.2203.08038},
}

@Article{Barbosa2023Mar,
  author           = {Barbosa, Felipe Manfio and Os{\ifmmode\acute{o}\else\'{o}\fi}rio, Fernando Santos},
  title            = {{Camera-Radar Perception for Autonomous Vehicles and ADAS: Concepts, Datasets and Metrics}},
  year             = {2023},
  month            = mar,
  abstract         = {{One of the main paths towards the reduction of traffic accidents is the increase in vehicle safety through driver assistance systems or even systems with a complete level of autonomy. In these types of systems, tasks such as obstacle detection and segmentation, especially the Deep Learning-based ones, play a fundamental role in scene understanding for correct and safe navigation. Besides that, the wide variety of sensors in vehicles nowadays provides a rich set of alternatives for improvement in the robustness of perception in challenging situations, such as navigation under lighting and weather adverse conditions. Despite the current focus given to the subject, the literature lacks studies on radar-based and radar-camera fusion-based perception. Hence, this work aims to carry out a study on the current scenario of camera and radar-based perception for ADAS and autonomous vehicles. Concepts and characteristics related to both sensors, as well as to their fusion, are presented. Additionally, we give an overview of the Deep Learning-based detection and segmentation tasks, and the main datasets, metrics, challenges, and open questions in vehicle perception.}},
  citation         = {0},
  creationdate     = {2023-03-10T22:20:21},
  eprint           = {2303.04302},
  journaltitle     = {ArXiv e-prints},
  keywords         = {Computer Vision and Pattern Recognition (cs.CV), Artificial Intelligence (cs.AI), Robotics (cs.RO)},
  modificationdate = {2023-03-10T22:20:44},
  shortjournal     = {arXiv},
  url              = {https://doi.org/10.48550/arXiv.2303.04302},
}

@Article{Paek2023Mar,
  author           = {Paek, Dong-Hee and Kong, Seung-Hyun and Wijaya, Kevin Tirta},
  title            = {{Enhanced K-Radar: Optimal Density Reduction to Improve Detection Performance and Accessibility of 4D Radar Tensor-based Object Detection}},
  year             = {2023},
  month            = mar,
  abstract         = {{Recent works have shown the superior robustness of four-dimensional (4D) Radar-based three-dimensional (3D) object detection in adverse weather conditions. However, processing 4D Radar data remains a challenge due to the large data size, which require substantial amount of memory for computing and storage. In previous work, an online density reduction is performed on the 4D Radar Tensor (4DRT) to reduce the data size, in which the density reduction level is chosen arbitrarily. However, the impact of density reduction on the detection performance and memory consumption remains largely unknown. In this paper, we aim to address this issue by conducting extensive hyperparamter tuning on the density reduction level. Experimental results show that increasing the density level from 0.01{\%} to 50{\%} of the original 4DRT density level proportionally improves the detection performance, at a cost of memory consumption. However, when the density level is increased beyond 5{\%}, only the memory consumption increases, while the detection performance oscillates below the peak point. In addition to the optimized density hyperparameter, we also introduce 4D Sparse Radar Tensor (4DSRT), a new representation for 4D Radar data with offline density reduction, leading to a significantly reduced raw data size. An optimized development kit for training the neural networks is also provided, which along with the utilization of 4DSRT, improves training speed by a factor of 17.1 compared to the state-of-the-art 4DRT-based neural networks. All codes are available at: this https URL.}},
  citation         = {0},
  eprint           = {2303.06342},
  journaltitle     = {ArXiv e-prints},
  keywords         = {Computer Vision and Pattern Recognition (cs.CV), Image and Video Processing (eess.IV)},
  modificationdate = {2023-03-16T10:46:11},
  priority         = {prio3},
  shortjournal     = {arXiv},
  url              = {https://doi.org/10.48550/arXiv.2303.06342},
}

@Article{Wei2022Mar,
  author           = {Wei, Zhiqing and Zhang, Fengkai and Chang, Shuo and Liu, Yangyang and Wu, Huici and Feng, Zhiyong},
  title            = {{MmWave Radar and Vision Fusion for Object Detection in Autonomous Driving: A Review}},
  year             = {2022},
  issn             = {1424-8220},
  month            = mar,
  number           = {7},
  pages            = {2542},
  volume           = {22},
  abstract         = {{With autonomous driving developing in a booming stage, accurate object detection in complex scenarios attract wide attention to ensure the safety of autonomous driving. Millimeter wave (mmWave) radar and vision fusion is a mainstream solution for accurate obstacle detection. This article presents a detailed survey on mmWave radar and vision fusion based obstacle detection methods. First, we introduce the tasks, evaluation criteria, and datasets of object detection for autonomous driving. The process of mmWave radar and vision fusion is then divided into three parts: sensor deployment, sensor calibration, and sensor fusion, which are reviewed comprehensively. Specifically, we classify the fusion methods into data level, decision level, and feature level fusion methods. In addition, we introduce three-dimensional(3D) object detection, the fusion of lidar and vision in autonomous driving and multimodal information fusion, which are promising for the future. Finally, we summarize this article. Keywords: autonomous driving; radar and vision fusion; radar and camera fusion; object detection; data level fusion; decision level fusion; feature level fusion; lidar; survey; review}},
  citation         = {19},
  groups           = {Camera-Radar fusion, Survey, 3D},
  journaltitle     = {Sensors},
  keywords         = {autonomous driving, radar and vision fusion, radar and camera fusion, object detection, data level fusion, decision level fusion, feature level fusion, lidar, survey, review},
  modificationdate = {2023-04-04T10:00:08},
  priority         = {prio1},
  publisher        = {Multidisciplinary Digital Publishing Institute},
  shortjournal     = {Sensors},
  url              = {https://doi.org/10.3390/s22072542},
}

@InCollection{Seo2023Feb,
  author           = {Seo, Hyojeong and Han, Dong Seog},
  booktitle        = {{2023 International Conference on Artificial Intelligence in Information and Communication (ICAIIC)}},
  publisher        = {IEEE},
  title            = {{Radar Signal Abnormal Point Classification based on Camera-Radar Sensor Fusion}},
  year             = {2023},
  month            = feb,
  pages            = {590--594},
  abstract         = {{For safe driving, it is essential to accept reliable information from recognition sensors. In this paper, we present a deep learning model that classifies whether radar signals coming in are normal or abnormal. The abnormal signal is defined as noise from the radar and all signals received when the radar fails or is in trouble. It is difficult to determine whether reflected signals are normal or not based only on radar data. Therefore, the camera and radar sensors are used together, considering the radar cross section (RCS) distribution varies by the angle and distance of the object. The proposed model uses data received from camera and radar sensors to determine the normality of object signals. The model shows an accuracy of 96.24{\%}. Through the results of this study, the reliability of radar signals can be determined in the actual driving environment, thereby ensuring the safety of vehicles and pedestrians.}},
  citation         = {0},
  issn             = {2831-6983},
  journaltitle     = {2023 International Conference on Artificial Intelligence in Information and Communication (ICAIIC)},
  modificationdate = {2023-03-28T23:05:42},
  shortjournal     = {2023 International Conference on Artificial Intelligence in Information and Communication (ICAIIC)},
  url              = {https://doi.org/10.1109/ICAIIC57133.2023.10067112},
}

@Article{Yang2023Mar,
  author           = {Yang, Bo and Khatri, Ishan and Happold, Michael and Chen, Chulong},
  title            = {{ADCNet: End-to-end perception with raw radar ADC data}},
  year             = {2023},
  month            = mar,
  abstract         = {{There is a renewed interest in radar sensors in the autonomous driving industry. As a relatively mature technology, radars have seen steady improvement over the last few years, making them an appealing alternative or complement to the commonly used LiDARs. An emerging trend is to leverage rich, low-level radar data for perception. In this work we push this trend to the extreme -- we propose a method to perform end-to-end learning on the raw radar analog-to-digital (ADC) data. Specifically, we design a learnable signal processing module inside the neural network, and a pre-training method guided by traditional signal processing algorithms. Experiment results corroborate the overall efficacy of the end-to-end learning method, while an ablation study validates the effectiveness of our individual innovations.}},
  citation         = {0},
  eprint           = {2303.11420},
  journaltitle     = {ArXiv e-prints},
  keywords         = {Signal Processing (eess.SP), Artificial Intelligence (cs.AI), Computer Vision and Pattern Recognition (cs.CV)},
  modificationdate = {2023-03-28T23:41:37},
  shortjournal     = {arXiv},
  url              = {https://doi.org/10.48550/arXiv.2303.11420},
}

@Article{Rebut2021Dec,
  author           = {Rebut, Julien and Ouaknine, Arthur and Malik, Waqas and P{\ifmmode\acute{e}\else\'{e}\fi}rez, Patrick},
  title            = {{Raw High-Definition Radar for Multi-Task Learning}},
  year             = {2021},
  month            = dec,
  abstract         = {{With their robustness to adverse weather conditions and ability to measure speeds, radar sensors have been part of the automotive landscape for more than two decades. Recent progress toward High Definition (HD) Imaging radar has driven the angular resolution below the degree, thus approaching laser scanning performance. However, the amount of data a HD radar delivers and the computational cost to estimate the angular positions remain a challenge. In this paper, we propose a novel HD radar sensing model, FFT-RadNet, that eliminates the overhead of computing the range-azimuth-Doppler 3D tensor, learning instead to recover angles from a range-Doppler spectrum. FFT-RadNet is trained both to detect vehicles and to segment free driving space. On both tasks, it competes with the most recent radar-based models while requiring less compute and memory. Also, we collected and annotated 2-hour worth of raw data from synchronized automotive-grade sensors (camera, laser, HD radar) in various environments (city street, highway, countryside road). This unique dataset, nick-named RADIal for "Radar, Lidar et al.", is available at this https URL.}},
  citation         = {8},
  eprint           = {2112.10646},
  groups           = {Camera-Radar-Lidar},
  journaltitle     = {ArXiv e-prints},
  keywords         = {Computer Vision and Pattern Recognition (cs.CV), Image and Video Processing (eess.IV)},
  modificationdate = {2023-03-28T23:46:40},
  shortjournal     = {arXiv},
  url              = {https://doi.org/10.48550/arXiv.2112.10646},
}

@Article{Broedermann2022Jun,
  author           = {Broedermann, Tim and Sakaridis, Christos and Dai, Dengxin and Van Gool, Luc},
  title            = {{HRFuser: A Multi-resolution Sensor Fusion Architecture for 2D Object Detection}},
  year             = {2022},
  month            = jun,
  abstract         = {{Besides standard cameras, autonomous vehicles typically include multiple additional sensors, such as lidars and radars, which help acquire richer information for perceiving the content of the driving scene. While several recent works focus on fusing certain pairs of sensors - such as camera and lidar or camera and radar - by using architectural components specific to the examined setting, a generic and modular sensor fusion architecture is missing from the literature. In this work, we focus on 2D object detection, a fundamental high-level task which is defined on the 2D image domain, and propose HRFuser, a multi-resolution sensor fusion architecture that scales straightforwardly to an arbitrary number of input modalities. The design of HRFuser is based on state-of-the-art high-resolution networks for image-only dense prediction and incorporates a novel multi-window cross-attention block as the means to perform fusion of multiple modalities at multiple resolutions. Even though cameras alone provide very informative features for 2D detection, we demonstrate via extensive experiments on the nuScenes and Seeing Through Fog datasets that our model effectively leverages complementary features from additional modalities, substantially improving upon camera-only performance and consistently outperforming state-of-the-art fusion methods for 2D detection both in normal and adverse conditions. The source code will be made publicly available.}},
  citation         = {1},
  eprint           = {2206.15157},
  groups           = {Camera-Radar-Lidar fusion},
  journaltitle     = {arXiv.org},
  modificationdate = {2023-03-30T23:35:30},
  priority         = {prio1},
  url              = {https://aps.arxiv.org/abs/2206.15157#},
}

@InCollection{Wang2022Mar,
  author           = {Wang, Yinan and Guan, Yingzhou and Li, Shuguang and Wu, Jiajun and Cheng, Hong},
  booktitle        = {{ICCAI '22: Proceedings of the 8th International Conference on Computing and Artificial Intelligence}},
  publisher        = {Association for Computing Machinery},
  title            = {{Fusion Perception of Vision and Millimeter Wave Radar for Autonomous Driving}},
  year             = {2022},
  address          = {New York, NY, USA},
  isbn             = {978-1-45039611-0},
  month            = mar,
  pages            = {767--772},
  abstract         = {{Perceiving the surrounding target is one of the key technologies of automatic driving system. We propose a fusion sensing method based on camera and millimeter wave radar. The millimeter wave radar point cloud is projected onto the image plane by the calibration relation, and the point cloud frustum corresponding to each image target is intercepted. After predict the size, position, category and other information of the target through the image and point cloud, the radial basis function neural network is used to match the radar points with targets. Finally, multi-target tracking results based on target confidence and projection detection results are fused to complete the fusion perception of the target. We build a real vehicle experiment platform containing multiple sets of camera-millimeter wave radar, and verify the ability of our method to perceive and locate the surrounding target. The results show that the average positioning error for various types of targets does not exceed 0.3m.}},
  citation         = {0},
  groups           = {Camera-Radar fusion},
  modificationdate = {2023-04-01T12:40:59},
  url              = {https://doi.org/10.1145/3532213.3532330},
}

@Article{Abdu2021Mar,
  author           = {Abdu, Fahad Jibrin and Zhang, Yixiong and Fu, Maozhong and Li, Yuhan and Deng, Zhenmiao},
  title            = {{Application of Deep Learning on Millimeter-Wave Radar Signals: A Review}},
  year             = {2021},
  issn             = {1424-8220},
  month            = mar,
  number           = {6},
  pages            = {1951},
  volume           = {21},
  abstract         = {{The progress brought by the deep learning technology over the last decade has inspired many research domains, such as radar signal processing, speech and audio recognition, etc., to apply it to their respective problems. Most of the prominent deep learning models exploit data representations acquired with either Lidar or camera sensors, leaving automotive radars rarely used. This is despite the vital potential of radars in adverse weather conditions, as well as their ability to simultaneously measure an object{'}s range and radial velocity seamlessly. As radar signals have not been exploited very much so far, there is a lack of available benchmark data. However, recently, there has been a lot of interest in applying radar data as input to various deep learning algorithms, as more datasets are being provided. To this end, this paper presents a survey of various deep learning approaches processing radar signals to accomplish some significant tasks in an autonomous driving application, such as detection and classification. We have itemized the review based on different radar signal representations, as it is one of the critical aspects while using radar data with deep learning models. Furthermore, we give an extensive review of the recent deep learning-based multi-sensor fusion models exploiting radar signals and camera images for object detection tasks. We then provide a summary of the available datasets containing radar data. Finally, we discuss the gaps and important innovations in the reviewed papers and highlight some possible future research prospects. Keywords: automotive radars; object detection; object classification; deep learning; multi-sensor fusion; datasets; autonomous driving}},
  citation         = {34},
  journaltitle     = {Sensors},
  keywords         = {automotive radars, object detection, object classification, deep learning, multi-sensor fusion, datasets, autonomous driving},
  modificationdate = {2023-04-01T12:47:12},
  publisher        = {Multidisciplinary Digital Publishing Institute},
  shortjournal     = {Sensors},
  url              = {https://doi.org/10.3390/s21061951},
}

@InCollection{Prabhakara2022Oct,
  author           = {Prabhakara, Akarsh and Zhang, Diana and Li, Chao and Munir, Sirajum and Sankaranarayanan, Aswin C. and Rowe, Anthony and Kumar, Swarun},
  booktitle        = {{2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}},
  publisher        = {IEEE},
  title            = {{Exploring mmWave Radar and Camera Fusion for High-Resolution and Long-Range Depth Imaging}},
  year             = {2022},
  month            = oct,
  pages            = {3995--4002},
  abstract         = {{Robotic geo-fencing and surveillance systems require accurate monitoring of objects if/when they violate perimeter restrictions. In this paper, we seek a solution for depth imaging of such objects of interest at high accuracy (few tens of cm) over extended ranges (up to 300 meters) from a single vantage point, such as a pole mounted platform. Unfortunately, the rich literature in depth imaging using camera, lidar and radar in isolation struggles to meet these tight requirements in real-world conditions. This paper proposes Metamoran, a solution that explores long-range depth imaging of objects of interest by fusing the strengths of two complementary technologies: mmWave radar and camera. Unlike cameras, mmWave radars offer excellent cm-scale depth resolution even at very long ranges. However, their angular resolution is at least 10x worse than camera systems. Fusing these two modalities is natural, but in scenes with high clutter and at long ranges, radar reflections are weak and experience spurious artifacts. Metamoran's core contribution is to leverage image segmentation and monocular depth estimation on camera images to help declutter radar and discover true object reflections. We perform a detailed evaluation of Metamoran's depth imaging capabilities in 400 diverse scenarios. Our evaluation shows that Metamoran estimates the depth of static objects up to 90 m away and moving objects up to 305 m away and with a median error of 28 cm, an improvement of 13 x over a naive radar+camera baseline and 23 x compared to monocular depth estimation.}},
  citation         = {0},
  issn             = {2153-0866},
  journaltitle     = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  modificationdate = {2023-04-01T21:22:44},
  shortjournal     = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  url              = {https://doi.org/10.1109/IROS47612.2022.9982080},
}

@Article{Gallego2020Jul,
  author           = {Gallego, Guillermo and Delbr{\ifmmode\ddot{u}\else\"{u}\fi}ck, Tobi and Orchard, Garrick and Bartolozzi, Chiara and Taba, Brian and Censi, Andrea and Leutenegger, Stefan and Davison, Andrew J. and Conradt, J{\ifmmode\ddot{o}\else\"{o}\fi}rg and Daniilidis, Kostas and Scaramuzza, Davide},
  title            = {{Event-Based Vision: A Survey}},
  year             = {2020},
  issn             = {1939-3539},
  month            = jul,
  number           = {1},
  pages            = {154--180},
  volume           = {44},
  abstract         = {{Event cameras are bio-inspired sensors that differ from conventional frame cameras: Instead of capturing images at a fixed rate, they asynchronously measure per-pixel brightness changes, and output a stream of events that encode the time, location and sign of the brightness changes. Event cameras offer attractive properties compared to traditional cameras: high temporal resolution (in the order of}},
  citation         = {868},
  groups           = {Event Camera},
  journaltitle     = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  modificationdate = {2023-04-04T00:25:31},
  publisher        = {IEEE},
  shortjournal     = {IEEE Trans. Pattern Anal. Mach. Intell.},
  url              = {https://doi.org/10.1109/TPAMI.2020.3008413},
}

@Article{Gehrig2021Mar,
  author           = {Gehrig, Mathias and Aarents, Willem and Gehrig, Daniel and Scaramuzza, Davide},
  title            = {{DSEC: A Stereo Event Camera Dataset for Driving Scenarios}},
  year             = {2021},
  issn             = {2377-3766},
  month            = mar,
  number           = {3},
  pages            = {4947--4954},
  volume           = {6},
  abstract         = {{Once an academic venture, autonomous driving has received unparalleled corporate funding in the last decade. Still, operating conditions of current autonomous cars are mostly restricted to ideal scenarios. This means that driving in challenging illumination conditions such as night, sunrise, and sunset remains an open problem. In these cases, standard cameras are being pushed to their limits in terms of low light and high dynamic range performance. To address these challenges, we propose, DSEC, a new dataset that contains such demanding illumination conditions and provides a rich set of sensory data. DSEC offers data from a wide-baseline stereo setup of two color frame cameras and two high-resolution monochrome event cameras. In addition, we collect lidar data and RTK GPS measurements, both hardware synchronized with all camera data. One of the distinctive features of this dataset is the inclusion of high-resolution event cameras. Event cameras have received increasing attention for their high temporal resolution and high dynamic range performance. However, due to their novelty, event camera datasets in driving scenarios are rare. This work presents the first high resolution, large scale stereo dataset with event cameras. The dataset contains 53 sequences collected by driving in a variety of illumination conditions and provides ground truth disparity for the development and evaluation of event-based stereo algorithms.}},
  citation         = {79},
  groups           = {Event Camera},
  journaltitle     = {IEEE Robotics and Automation Letters},
  modificationdate = {2023-04-04T00:26:00},
  publisher        = {IEEE},
  shortjournal     = {IEEE Rob. Autom. Lett.},
  url              = {https://doi.org/10.1109/LRA.2021.3068942},
}

@Article{Zhou2022Sep,
  author           = {Zhou, Zhuyun and Wu, Zongwei and Boutteau, R{\ifmmode\acute{e}\else\'{e}\fi}mi and Yang, Fan and Demonceaux, C{\ifmmode\acute{e}\else\'{e}\fi}dric and Ginhac, Dominique},
  title            = {{RGB-Event Fusion for Moving Object Detection in Autonomous Driving}},
  year             = {2022},
  month            = sep,
  abstract         = {{Moving Object Detection (MOD) is a critical vision task for successfully achieving safe autonomous driving. Despite plausible results of deep learning methods, most existing approaches are only frame-based and may fail to reach reasonable performance when dealing with dynamic traffic participants. Recent advances in sensor technologies, especially the Event camera, can naturally complement the conventional camera approach to better model moving objects. However, event-based works often adopt a pre-defined time window for event representation, and simply integrate it to estimate image intensities from events, neglecting much of the rich temporal information from the available asynchronous events. Therefore, from a new perspective, we propose RENet, a novel RGB-Event fusion Network, that jointly exploits the two complementary modalities to achieve more robust MOD under challenging scenarios for autonomous driving. Specifically, we first design a temporal multi-scale aggregation module to fully leverage event frames from both the RGB exposure time and larger intervals. Then we introduce a bi-directional fusion module to attentively calibrate and fuse multi-modal features. To evaluate the performance of our network, we carefully select and annotate a sub-MOD dataset from the commonly used DSEC dataset. Extensive experiments demonstrate that our proposed method performs significantly better than the state-of-the-art RGB-Event fusion alternatives. The source code and dataset are publicly available at: this https URL.}},
  citation         = {0},
  groups           = {Event Camera},
  journaltitle     = {ArXiv e-prints},
  keywords         = {Computer Vision and Pattern Recognition (cs.CV), Robotics (cs.RO)},
  modificationdate = {2023-04-04T00:40:23},
  shortjournal     = {arXiv},
  url              = {https://doi.org/10.48550/arXiv.2209.08323},
}

@InCollection{Tomy2022May,
  author           = {Tomy, Abhishek and Paigwar, Anshul and Mann, Khushdeep S. and Renzaglia, Alessandro and Laugier, Christian},
  booktitle        = {{2022 International Conference on Robotics and Automation (ICRA)}},
  publisher        = {IEEE},
  title            = {{Fusing Event-based and RGB camera for Robust Object Detection in Adverse Conditions}},
  year             = {2022},
  month            = may,
  pages            = {933--939},
  abstract         = {{The ability to detect objects, under image corruptions and different weather conditions is vital for deep learning models especially when applied to real-world applications such as autonomous driving. Traditional RGB-based detection fails under these conditions and it is thus important to design a sensor suite that is redundant to failures of the primary frame-based detection. Event-based cameras can complement frame-based cameras in low-light conditions and high dynamic range scenarios that an autonomous vehicle can encounter during navigation. Accordingly, we propose a redundant sensor fusion model of event-based and frame-based cameras that is robust to common image corruptions. The method utilizes a voxel grid representation for events as input and proposes a two-parallel feature extractor network for frames and events. Our sensor fusion approach is more robust to corruptions by over 30{\%} compared to only frame-based detections and outperforms the only event-based detection. The model is trained and evaluated on the publicly released DSEC dataset.}},
  citation         = {6},
  groups           = {Event Camera},
  journaltitle     = {2022 International Conference on Robotics and Automation (ICRA)},
  modificationdate = {2023-04-04T00:41:27},
  shortjournal     = {2022 International Conference on Robotics and Automation (ICRA)},
  url              = {https://doi.org/10.1109/ICRA46639.2022.9812059},
}

@Article{Cai2023Mar,
  author           = {Cai, Hongxiang and Zhang, Zeyuan and Zhou, Zhenyu and Li, Ziyin and Ding, Wenbo and Zhao, Jiuhua},
  title            = {{BEVFusion4D: Learning LiDAR-Camera Fusion Under Bird's-Eye-View via Cross-Modality Guidance and Temporal Aggregation}},
  year             = {2023},
  month            = mar,
  abstract         = {{Integrating LiDAR and Camera information into Bird's-Eye-View (BEV) has become an essential topic for 3D object detection in autonomous driving. Existing methods mostly adopt an independent dual-branch framework to generate LiDAR and camera BEV, then perform an adaptive modality fusion. Since point clouds provide more accurate localization and geometry information, they could serve as a reliable spatial prior to acquiring relevant semantic information from the images. Therefore, we design a LiDAR-Guided View Transformer (LGVT) to effectively obtain the camera representation in BEV space and thus benefit the whole dual-branch fusion system. LGVT takes camera BEV as the primitive semantic query, repeatedly leveraging the spatial cue of LiDAR BEV for extracting image features across multiple camera views. Moreover, we extend our framework into the temporal domain with our proposed Temporal Deformable Alignment (TDA) module, which aims to aggregate BEV features from multiple historical frames. Including these two modules, our framework dubbed BEVFusion4D achieves state-of-the-art results in 3D object detection, with 72.0{\%} mAP and 73.5{\%} NDS on the nuScenes validation set, and 73.3{\%} mAP and 74.7{\%} NDS on nuScenes test set, respectively.}},
  citation         = {0},
  groups           = {3D},
  journaltitle     = {ArXiv e-prints},
  keywords         = {Computer Vision and Pattern Recognition (cs.CV)},
  modificationdate = {2023-04-04T09:54:27},
  shortjournal     = {arXiv},
  url              = {https://doi.org/10.48550/arXiv.2303.17099},
}

@Article{Jamil2022Nov,
  author           = {Jamil, Sonain and Piran, {\relax Md}. Jalil and Kwon, Oh-Jin},
  title            = {{A Comprehensive Survey of Transformers for Computer Vision}},
  year             = {2022},
  month            = nov,
  abstract         = {{As a special type of transformer, Vision Transformers (ViTs) are used to various computer vision applications (CV), such as image recognition. There are several potential problems with convolutional neural networks (CNNs) that can be solved with ViTs. For image coding tasks like compression, super-resolution, segmentation, and denoising, different variants of the ViTs are used. The purpose of this survey is to present the first application of ViTs in CV. The survey is the first of its kind on ViTs for CVs to the best of our knowledge. In the first step, we classify different CV applications where ViTs are applicable. CV applications include image classification, object detection, image segmentation, image compression, image super-resolution, image denoising, and anomaly detection. Our next step is to review the state-of-the-art in each category and list the available models. Following that, we present a detailed analysis and comparison of each model and list its pros and cons. After that, we present our insights and lessons learned for each category. Moreover, we discuss several open research challenges and future research directions.}},
  citation         = {0},
  groups           = {Perception},
  journaltitle     = {ArXiv e-prints},
  keywords         = {Computer Vision and Pattern Recognition (cs.CV)},
  modificationdate = {2023-04-04T10:12:51},
  shortjournal     = {arXiv},
  url              = {https://doi.org/10.48550/arXiv.2211.06004},
}

@Article{Li2022Apr,
  author       = {Li, Peizhao and Wang, Pu and Berntorp, Karl and Liu, Hongfu},
  title        = {{Exploiting Temporal Relations on Radar Perception for Autonomous Driving}},
  year         = {2022},
  month        = apr,
  abstract     = {{We consider the object recognition problem in autonomous driving using automotive radar sensors. Comparing to Lidar sensors, radar is cost-effective and robust in all-weather conditions for perception in autonomous driving. However, radar signals suffer from low angular resolution and precision in recognizing surrounding objects. To enhance the capacity of automotive radar, in this work, we exploit the temporal information from successive ego-centric bird-eye-view radar image frames for radar object recognition. We leverage the consistency of an object's existence and attributes (size, orientation, etc.), and propose a temporal relational layer to explicitly model the relations between objects within successive radar images. In both object detection and multiple object tracking, we show the superiority of our method compared to several baseline approaches.}},
  citation     = {9},
  journaltitle = {ArXiv e-prints},
  keywords     = {Computer Vision and Pattern Recognition (cs.CV)},
  shortjournal = {arXiv},
  url          = {https://doi.org/10.48550/arXiv.2204.01184},
}

@Article{Li2023Apr,
  author       = {Li, You and Moreau, Julien and Ibanez-Guzman, Javier},
  title        = {{Emergent Visual Sensors for Autonomous Vehicles}},
  year         = {2023},
  issn         = {1558-0016},
  month        = apr,
  pages        = {1--22},
  abstract     = {{For vehicles to navigate autonomously, they need to perceive and understand their immediate surroundings. Currently, cameras are the preferred sensors, due to their high performance and relatively low-cost compared with other sensors like LiDARs and Radars. However, their performance is limited by inherent imaging constraints, a standard RGB camera may perform poorly in extreme conditions, including low illumination, high contrast, bad weather (e.g. fog, rain, snow, etc.), glare, etc. Further, when using monocular cameras, it is more challenging to determine spatial distances than when using active range sensors such as LiDARs or Radars. Over the past years, novel image sensors, namely, infrared cameras, range-gated cameras, polarization cameras, and event cameras, have demonstrated strong potential. Some of them could be game-changers for future autonomous vehicles, they are the result of progress in sensor technology and the development of the accompanying perception algorithms. This paper presents in a systematic manner their principles, comparative advantages, data processing algorithms, and related applications. The purpose is to provide practitioners with an in-depth overview of novel sensing technologies that can contribute to the safe deployment of autonomous vehicles.}},
  citation     = {0},
  journaltitle = {IEEE Transactions on Intelligent Transportation Systems},
  publisher    = {IEEE},
  shortjournal = {IEEE Trans. Intell. Transp. Syst.},
  url          = {https://doi.org/10.1109/TITS.2023.3248483},
}

@Article{Yang2022Nov,
  author       = {Yang, Yanlong and Liu, Jianan and Huang, Tao and Han, Qing-Long and Ma, Gang and Zhu, Bing},
  title        = {{RaLiBEV: Radar and LiDAR BEV Fusion Learning for Anchor Box Free Object Detection System}},
  year         = {2022},
  month        = nov,
  abstract     = {{In autonomous driving systems, LiDAR and radar play important roles in the perception of the surrounding environment.LiDAR provides accurate 3D spatial sensing information but cannot work in adverse weather like fog. On the other hand, the radar signal can be diffracted when encountering raindrops or mist particles thanks to its wavelength, but it suffers from large noise. Recent state-of-the-art works reveal that fusion of radar and LiDAR can lead to robust detection in adverse weather. The existing works adopt convolutional neural network architecture to extract features from each sensor data stream, then align and aggregate the two branch features to predict object detection results. However, these methods have low accuracy of bounding box estimations due to a simple design of label assignment and fusion strategies. In this paper, we propose a bird's-eye view fusion learning-based anchor box-free object detection system, which fuses the feature derived from the radar range-azimuth heatmap and the LiDAR point cloud to estimate the possible objects. Different label assignment strategies have been designed to facilitate the consistency between the classification of foreground or background anchor points and the corresponding bounding box regressions. In addition, the performance of the proposed object detector is further enhanced by employing a novel interactive transformer module. The superior performance of the proposed methods in this paper has been demonstrated using the recently published Oxford radar robotCar dataset, showing that the average precision of our system significantly outperforms the best state-of-the-art method by 14.4{\%} and 20.5{\%} at IoU equals 0.8 in clear and foggy weather testing, respectively.}},
  citation     = {1},
  groups       = {Lidar-Radar fusion},
  journaltitle = {ArXiv e-prints},
  keywords     = {Computer Vision and Pattern Recognition (cs.CV), Artificial Intelligence (cs.AI), Machine Learning (cs.LG)},
  shortjournal = {arXiv},
  url          = {https://doi.org/10.48550/arXiv.2211.06108},
}

@Article{Zhou2022Aug,
  author       = {Zhou, Taohua and Shi, Yining and Chen, Junjie and Jiang, Kun and Yang, Mengmeng and Yang, Diange},
  title        = {{Bridging the View Disparity Between Radar and Camera Features for Multi-modal Fusion 3D Object Detection}},
  year         = {2022},
  month        = aug,
  abstract     = {{Environmental perception with the multi-modal fusion of radar and camera is crucial in autonomous driving to increase accuracy, completeness, and robustness. This paper focuses on utilizing millimeter-wave (MMW) radar and camera sensor fusion for 3D object detection. A novel method that realizes the feature-level fusion under the bird's-eye view (BEV) for a better feature representation is proposed. Firstly, radar points are augmented with temporal accumulation and sent to a spatial-temporal encoder for radar feature extraction. Meanwhile, multi-scale image 2D features which adapt to various spatial scales are obtained by image backbone and neck model. Then, image features are transformed to BEV with the designed view transformer. In addition, this work fuses the multi-modal features with a two-stage fusion model called point-fusion and ROI-fusion, respectively. Finally, a detection head regresses objects category and 3D locations. Experimental results demonstrate that the proposed method realizes the state-of-the-art (SOTA) performance under the most crucial detection metrics-mean average precision (mAP) and nuScenes detection score (NDS) on the challenging nuScenes dataset.}},
  citation     = {3},
  groups       = {3D},
  journaltitle = {ArXiv e-prints},
  keywords     = {Computer Vision and Pattern Recognition (cs.CV)},
  shortjournal = {arXiv},
  url          = {https://doi.org/10.48550/arXiv.2208.12079},
}

@Misc{Li2022,
  author   = {Li, Peizhao and Wang, Pu and Berntorp, Karl and Liu, Hongfu},
  note     = {[Online; accessed 8. Apr. 2023]},
  title    = {{Exploiting Temporal Relations on Radar Perception for Autonomous Driving}},
  year     = {2022},
  abstrat  = {We consider the object recognition problem in autonomous driving using automotive radar sensors. Comparing to Lidar sensors, radar is cost-effective and robust in all-weather conditions for perception in autonomous driving. However, radar signals suffer from low angular resolution and precision in recognizing surrounding objects. To enhance the capacity of automotive radar, in this work, we exploit the temporal information from successive ego-centric bird-eye-view radar image frames for radar object recognition. We leverage the consistency of an object's existence and attributes (size, orientation, etc.), and propose a temporal relational layer to explicitly model the relations between objects within successive radar images. In both object detection and multiple object tracking, we show the superiority of our method compared to several baseline approaches.},
  citation = {9},
  groups   = {Temporal},
  pages    = {17071--17080},
  url      = {https://openaccess.thecvf.com/content/CVPR2022/html/Li_Exploiting_Temporal_Relations_on_Radar_Perception_for_Autonomous_Driving_CVPR_2022_paper.html},
}

@Article{Scheiner2021Dec,
  author       = {Scheiner, Nicolas and Kraus, Florian and Appenrodt, Nils and Dickmann, J{\ifmmode\ddot{u}\else\"{u}\fi}rgen and Sick, Bernhard},
  title        = {{Object detection for automotive radar point clouds {\textendash} a comparison}},
  year         = {2021},
  issn         = {2523-398X},
  month        = dec,
  number       = {1},
  pages        = {1--23},
  volume       = {3},
  abstract     = {{Automotive radar perception is an integral part of automated driving systems. Radar sensors benefit from their excellent robustness against adverse weather conditions such as snow, fog, or heavy rain. Despite the fact that machine-learning-based object detection is traditionally a camera-based domain, vast progress has been made for lidar sensors, and radar is also catching up. Recently, several new techniques for using machine learning algorithms towards the correct detection and classification of moving road users in automotive radar data have been introduced. However, most of them have not been compared to other methods or require next generation radar sensors which are far more advanced than current conventional automotive sensors. This article makes a thorough comparison of existing and novel radar object detection algorithms with some of the most successful candidates from the image and lidar domain. All experiments are conducted using a conventional automotive radar system. In addition to introducing all architectures, special attention is paid to the necessary point cloud preprocessing for all methods. By assessing all methods on a large and open real world data set, this evaluation provides the first representative algorithm comparison in this domain and outlines future research directions.}},
  citation     = {14},
  journaltitle = {AI Perspectives},
  keywords     = {Engineering, general},
  publisher    = {SpringerOpen},
  shortjournal = {AI Perspect.},
  url          = {https://doi.org/10.1186/s42467-021-00012-z},
}

@Article{Bansal2022Aug,
  author       = {Bansal, Kshitiz and Rungta, Keshav and Bharadia, Dinesh},
  title        = {{RadSegNet: A Reliable Approach to Radar Camera Fusion}},
  year         = {2022},
  month        = aug,
  abstract     = {{Perception systems for autonomous driving have seen significant advancements in their performance over last few years. However, these systems struggle to show robustness in extreme weather conditions because sensors like lidars and cameras, which are the primary sensors in a sensor suite, see a decline in performance under these conditions. In order to solve this problem, camera-radar fusion systems provide a unique opportunity for all weather reliable high quality perception. Cameras provides rich semantic information while radars can work through occlusions and in all weather conditions. In this work, we show that the state-of-the-art fusion methods perform poorly when camera input is degraded, which essentially results in losing the all-weather reliability they set out to achieve. Contrary to these approaches, we propose a new method, RadSegNet, that uses a new design philosophy of independent information extraction and truly achieves reliability in all conditions, including occlusions and adverse weather. We develop and validate our proposed system on the benchmark Astyx dataset and further verify these results on the RADIATE dataset. When compared to state-of-the-art methods, RadSegNet achieves a 27{\%} improvement on Astyx and 41.46{\%} increase on RADIATE, in average precision score and maintains a significantly better performance in adverse weather conditions}},
  citation     = {0},
  groups       = {Camera-Radar fusion},
  journaltitle = {ArXiv e-prints},
  keywords     = {Computer Vision and Pattern Recognition (cs.CV), Artificial Intelligence (cs.AI)},
  shortjournal = {arXiv},
  url          = {https://doi.org/10.48550/arXiv.2208.03849},
}

@Article{Deng2022Sep,
  author       = {Deng, Kaikai and Zhao, Dong and Han, Qiaoyue and Wang, Shuyue and Zhang, Zihan and Zhou, Anfu and Ma, Huadong},
  title        = {{Geryon: Edge Assisted Real-time and Robust Object Detection on Drones via mmWave Radar and Camera Fusion}},
  year         = {2022},
  issn         = {2474-9567},
  month        = sep,
  number       = {3},
  pages        = {1--27},
  volume       = {6},
  abstract     = {{Vision-based drone-view object detection suffers from severe performance degradation under adverse conditions (e.g., foggy weather, poor illumination). To remedy this, leveraging complementary mmWave radar has become a trend. However, existing fusion approaches seldom apply to drones due to i) the aggravated sparsity and noise of point clouds from low-cost commodity radars, and ii) explosive sensing data and intensive computations leading to high latency. To address these issues, we design Geryon, an edge assisted object detection system on drones, which utilizes a suit of approaches to fully exploit the complementary advantages of camera and mmWave radar on three levels: (i) a novel multi-frame compositing approach utilizes camera to assist radar to address the aggravated sparsity and noise of radar point clouds; (ii) a saliency area extraction and encoding approach utilizes radar to assist camera to reduce the bandwidth consumption and offloading latency; (iii) a parallel transmission and inference approach with a lightweight box enhancement scheme further reduces the offloading latency while ensuring the edge-side accuracy-latency trade-off by the parallelism and better camera-radar fusion. We implement and evaluate Geryon with four datasets we collect under foggy/rainy/snowy weather and poor illumination conditions, demonstrating its great advantages over other state-of-the-art approaches in terms of both accuracy and latency.}},
  citation     = {1},
  groups       = {Camera-Radar fusion},
  journaltitle = {Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies},
  keywords     = {drone, edge network orchestration, mmWave radar sensing, multimodal fusion, real-time object detection},
  publisher    = {Association for Computing Machinery},
  shortjournal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
  url          = {https://doi.org/10.1145/3550298},
}

@InCollection{Wang2020Aug,
  author       = {Wang, Xin and Yu, Fisher and Dunlap, Lisa and Ma, Yi-An and Wang, Ruth and Mirhoseini, Azalia and Darrell, Trevor and Gonzalez, Joseph E.},
  booktitle    = {{Uncertainty in Artificial Intelligence}},
  publisher    = {PMLR},
  title        = {{Deep Mixture of Experts via Shallow Embedding}},
  year         = {2020},
  month        = aug,
  pages        = {552--562},
  abstract     = {{Deep Mixture of Experts via Shallow EmbeddingXin Wang, Fisher Yu, Lisa Dunlap, Yi-An Ma, Ruth Wang, Azalia Mirhoseini, Trevor D...}},
  citation     = {63},
  issn         = {2640-3498},
  journaltitle = {PMLR},
  url          = {https://proceedings.mlr.press/v115/wang20d.html},
}

@Article{Kim2023Apr,
  author       = {Kim, Youngseok and Kim, Sanmin and Shin, Juyeb and Choi, Jun Won and Kum, Dongsuk},
  title        = {{CRN: Camera Radar Net for Accurate, Robust, Efficient 3D Perception}},
  year         = {2023},
  month        = apr,
  abstract     = {{Autonomous driving requires an accurate and fast 3D perception system that includes 3D object detection, tracking, and segmentation. Although recent low-cost camera-based approaches have shown promising results, they are susceptible to poor illumination or bad weather conditions and have a large localization error. Hence, fusing camera with low-cost radar, which provides precise long-range measurement and operates reliably in all environments, is promising but has not yet been thoroughly investigated. In this paper, we propose Camera Radar Net (CRN), a novel camera-radar fusion framework that generates a semantically rich and spatially accurate bird's-eye-view (BEV) feature map for various tasks. To overcome the lack of spatial information in an image, we transform perspective view image features to BEV with the help of sparse but accurate radar points. We further aggregate image and radar feature maps in BEV using multi-modal deformable attention designed to tackle the spatial misalignment between inputs. CRN with real-time setting operates at 20 FPS while achieving comparable performance to LiDAR detectors on nuScenes, and even outperforms at a far distance on 100m setting. Moreover, CRN with offline setting yields 62.4{\%} NDS, 57.5{\%} mAP on nuScenes test set and ranks first among all camera and camera-radar 3D object detectors.}},
  citation     = {0},
  groups       = {3D},
  journaltitle = {ArXiv e-prints},
  keywords     = {Computer Vision and Pattern Recognition (cs.CV), Artificial Intelligence (cs.AI), Robotics (cs.RO)},
  shortjournal = {arXiv},
  url          = {https://doi.org/10.48550/arXiv.2304.00670},
}

@Article{Abdu2023Apr,
  author       = {Abdu, Fahad Jibrin and Zhang, Yixiong and Deng, Zhenmiao},
  title        = {{CCA-Based Fusion of Camera and Radar Features for Target Classification Under Adverse Weather Conditions}},
  year         = {2023},
  issn         = {1573-773X},
  month        = apr,
  pages        = {1--27},
  abstract     = {{Deep learning models such as deep convolutional neural networks (DCNNs) image classifiers have achieved outstanding performance over the last decade. However, these models are mostly trained with high-quality images drawn from publicly available datasets such as ImageNet. Recently, many researchers have evaluated the impact of low-quality image degradations on the performance of different neural network-based image classifiers. But, most of these studies generate low-quality images by synthetic modification of the high-quality images. Besides, most of the studies employed various image processing techniques to remove the image degradations and trained the DCNNs again to achieve better performance. But it has since been discovered that such methods could not improve the classification accuracy of DCNNs. The robustness of DCNNs based image classifiers trained on low-quality images resulting from natural factors common in autonomous driving and other intelligent system settings was rarely studied over the recent years. In this paper, we proposed a canonical correlation analysis (CCA) based fusion of camera and radar features for improving the performance of DCNNs image classifiers trained on natural adverse weather data. CCA is a statistical approach that creates a highly discriminative feature vector by measuring the linear relationship between the camera and radar features. A spatial attention network was designed to re-weight the camera features before associating them with radar features in the CCA-feature fusion block. Our findings based on experimental evaluations have proven that, indeed, the performance of the DCNN models (i.e., Alex-Net and VGG-16-Net) is heavily affected by degradations arising from natural factors. Specifically, the DCNN models are more affected by the degradations arising from rainfall, foggy and nighttime conditions using Radiate and Carrada datasets. However, the proposed fusion frameworks have improved the performance of the individual sensing modalities significantly. The radar data has helped substantially in enhancing the fusion performance, mainly using rainfall data where the camera data is heavily affected.}},
  citation     = {0},
  journaltitle = {Neural Processing Letters},
  keywords     = {Artificial Intelligence, Complex Systems, Computational Intelligence},
  publisher    = {Springer US},
  shortjournal = {Neural Process. Lett.},
  url          = {https://doi.org/10.1007/s11063-023-11261-w},
}

@Article{Yao2023Apr,
  author       = {Yao, Shanliang and Guan, Runwei and Huang, Xiaoyu and Li, Zhuoxiao and Sha, Xiangyu and Yue, Yong and Lim, Eng Gee and Seo, Hyungjoon and Man, Ka Lok and Zhu, Xiaohui and Yue, Yutao},
  title        = {{Radar-Camera Fusion for Object Detection and Semantic Segmentation in Autonomous Driving: A Comprehensive Review}},
  year         = {2023},
  month        = apr,
  abstract     = {{Driven by deep learning techniques, perception technology in autonomous driving has developed rapidly in recent years. To achieve accurate and robust perception capabilities, autonomous vehicles are often equipped with multiple sensors, making sensor fusion a crucial part of the perception system. Among these fused sensors, radars and cameras enable a complementary and cost-effective perception of the surrounding environment regardless of lighting and weather conditions. This review aims to provide a comprehensive guideline for radar-camera fusion, particularly concentrating on perception tasks related to object detection and semantic segmentation. Based on the principles of the radar and camera sensors, we delve into the data processing process and representations, followed by an in-depth analysis and summary of radar-camera fusion datasets. In the review of methodologies in radar-camera fusion, we address interrogative questions, including "why to fuse", "what to fuse", "where to fuse", "when to fuse", and "how to fuse", subsequently discussing various challenges and potential research directions within this domain. To ease the retrieval and comparison of datasets and fusion methods, we also provide an interactive website: this https URL.}},
  citation     = {0},
  groups       = {Camera-Radar fusion, Survey},
  journaltitle = {ArXiv e-prints},
  keywords     = {Computer Vision and Pattern Recognition (cs.CV), Artificial Intelligence (cs.AI), Robotics (cs.RO)},
  priority     = {prio1},
  shortjournal = {arXiv},
  url          = {https://doi.org/10.48550/arXiv.2304.10410},
}

@Article{Liu2023Apr,
  author       = {Liu, Zhanwen and Cheng, Juanru and Fan, Jin and Lin, Shan and Wang, Yang and Zhao, Xiangmo},
  title        = {{Multi-Modal Fusion Based on Depth Adaptive Mechanism for 3D Object Detection}},
  year         = {2023},
  issn         = {1941-0077},
  month        = apr,
  pages        = {1--11},
  abstract     = {{Lidars and cameras are critical sensors for 3D object detection in autonomous driving. Despite the increasing popularity of sensor fusion in this field, accurate and robust fusion methods are still under exploration due to non-homogenous representations. In this paper, we find that the complementary roles of point clouds and images vary with depth. An important reason is that the point cloud appearance changes significantly with increasing distance from the Lidar, while the image's edge, color, and texture information are not sensitive to depth. To address this, we propose a fusion module based on the Depth Attention Mechanism (DAM), which mainly consists of two operations: gated feature generation and point cloud division. The former adaptively learns the importance of bimodal features without additional annotations, while the latter divides point clouds to achieve differential fusion of multi-modal features at different depths. This fusion module can enhance the representation ability of original features for different point sets and provide more comprehensive features by using the dual splicing strategy of concatenation and index connection. Additionally, considering point density as a feature and its negative correlation with depth, we build an Adaptive Threshold Generation Network (ATGN) to generate the depth threshold by extracting density information, which can divide point clouds more reasonably. Experiments on the KITTI dataset demonstrate the effectiveness and competitiveness of our proposed models.}},
  citation     = {0},
  groups       = {3D},
  journaltitle = {IEEE Transactions on Multimedia},
  publisher    = {IEEE},
  shortjournal = {IEEE Trans. Multimedia},
  url          = {https://doi.org/10.1109/TMM.2023.3270638},
}

@Article{Liu2023Apr,
  author       = {Liu, Zhanwen and Cheng, Juanru and Fan, Jin and Lin, Shan and Wang, Yang and Zhao, Xiangmo},
  title        = {{Multi-Modal Fusion Based on Depth Adaptive Mechanism for 3D Object Detection}},
  year         = {2023},
  issn         = {1941-0077},
  month        = apr,
  pages        = {1--11},
  abstract     = {{Lidars and cameras are critical sensors for 3D object detection in autonomous driving. Despite the increasing popularity of sensor fusion in this field, accurate and robust fusion methods are still under exploration due to non-homogenous representations. In this paper, we find that the complementary roles of point clouds and images vary with depth. An important reason is that the point cloud appearance changes significantly with increasing distance from the Lidar, while the image's edge, color, and texture information are not sensitive to depth. To address this, we propose a fusion module based on the Depth Attention Mechanism (DAM), which mainly consists of two operations: gated feature generation and point cloud division. The former adaptively learns the importance of bimodal features without additional annotations, while the latter divides point clouds to achieve differential fusion of multi-modal features at different depths. This fusion module can enhance the representation ability of original features for different point sets and provide more comprehensive features by using the dual splicing strategy of concatenation and index connection. Additionally, considering point density as a feature and its negative correlation with depth, we build an Adaptive Threshold Generation Network (ATGN) to generate the depth threshold by extracting density information, which can divide point clouds more reasonably. Experiments on the KITTI dataset demonstrate the effectiveness and competitiveness of our proposed models.}},
  citation     = {0},
  journaltitle = {IEEE Transactions on Multimedia},
  publisher    = {IEEE},
  shortjournal = {IEEE Trans. Multimedia},
  url          = {https://doi.org/10.1109/TMM.2023.3270638},
}

@Article{Liu2023Apr,
  author       = {Liu, Zhanwen and Cheng, Juanru and Fan, Jin and Lin, Shan and Wang, Yang and Zhao, Xiangmo},
  title        = {{Multi-Modal Fusion Based on Depth Adaptive Mechanism for 3D Object Detection}},
  year         = {2023},
  issn         = {1941-0077},
  month        = apr,
  pages        = {1--11},
  abstract     = {{Lidars and cameras are critical sensors for 3D object detection in autonomous driving. Despite the increasing popularity of sensor fusion in this field, accurate and robust fusion methods are still under exploration due to non-homogenous representations. In this paper, we find that the complementary roles of point clouds and images vary with depth. An important reason is that the point cloud appearance changes significantly with increasing distance from the Lidar, while the image's edge, color, and texture information are not sensitive to depth. To address this, we propose a fusion module based on the Depth Attention Mechanism (DAM), which mainly consists of two operations: gated feature generation and point cloud division. The former adaptively learns the importance of bimodal features without additional annotations, while the latter divides point clouds to achieve differential fusion of multi-modal features at different depths. This fusion module can enhance the representation ability of original features for different point sets and provide more comprehensive features by using the dual splicing strategy of concatenation and index connection. Additionally, considering point density as a feature and its negative correlation with depth, we build an Adaptive Threshold Generation Network (ATGN) to generate the depth threshold by extracting density information, which can divide point clouds more reasonably. Experiments on the KITTI dataset demonstrate the effectiveness and competitiveness of our proposed models.}},
  citation     = {0},
  groups       = {3D},
  journaltitle = {IEEE Transactions on Multimedia},
  publisher    = {IEEE},
  shortjournal = {IEEE Trans. Multimedia},
  url          = {https://doi.org/10.1109/TMM.2023.3270638},
}

@Article{Togelius2023Mar,
  author       = {Togelius, Julian and Yannakakis, Georgios N.},
  title        = {{Choose Your Weapon: Survival Strategies for Depressed AI Academics}},
  year         = {2023},
  month        = mar,
  abstract     = {{Are you an AI researcher at an academic institution? Are you anxious you are not coping with the current pace of AI advancements? Do you feel you have no (or very limited) access to the computational and human resources required for an AI research breakthrough? You are not alone; we feel the same way. A growing number of AI academics can no longer find the means and resources to compete at a global scale. This is a somewhat recent phenomenon, but an accelerating one, with private actors investing enormous compute resources into cutting edge AI research. Here, we discuss what you can do to stay competitive while remaining an academic. We also briefly discuss what universities and the private sector could do improve the situation, if they are so inclined. This is not an exhaustive list of strategies, and you may not agree with all of them, but it serves to start a discussion.}},
  citation     = {0},
  journaltitle = {ArXiv e-prints},
  keywords     = {Other Computer Science (cs.OH), Computers and Society (cs.CY), Neural and Evolutionary Computing (cs.NE)},
  priority     = {prio1},
  shortjournal = {arXiv},
  url          = {https://doi.org/10.48550/arXiv.2304.06035},
}

@Article{Sezgin2023May,
  author       = {Sezgin, Fatih and Vriesman, Daniel and Steinhauser, Dagmar and Lugner, Robert and Brandmeier, Thomas},
  title        = {{Safe Autonomous Driving in Adverse Weather: Sensor Evaluation and Performance Monitoring}},
  year         = {2023},
  month        = may,
  abstract     = {{The vehicle's perception sensors radar, lidar and camera, which must work continuously and without restriction, especially with regard to automated/autonomous driving, can lose performance due to unfavourable weather conditions. This paper analyzes the sensor signals of these three sensor technologies under rain and fog as well as day and night. A data set of a driving test vehicle as an object target under different weather conditions was recorded in a controlled environment with adjustable, defined, and reproducible weather conditions. Based on the sensor performance evaluation, a method has been developed to detect sensor degradation, including determining the affected data areas and estimating how severe they are. Through this sensor monitoring, measures can be taken in subsequent algorithms to reduce the influences or to take them into account in safety and assistance systems to avoid malfunctions.}},
  citation     = {0},
  journaltitle = {ArXiv e-prints},
  keywords     = {Robotics (cs.RO)},
  shortjournal = {arXiv},
  url          = {https://doi.org/10.48550/arXiv.2305.01336},
}

@Article{Huch2023May,
  author       = {Huch, Sebastian and Sauerbeck, Florian and Betz, Johannes},
  title        = {{DeepSTEP -- Deep Learning-Based Spatio-Temporal End-To-End Perception for Autonomous Vehicles}},
  year         = {2023},
  month        = may,
  abstract     = {{Autonomous vehicles demand high accuracy and robustness of perception algorithms. To develop efficient and scalable perception algorithms, the maximum information should be extracted from the available sensor data. In this work, we present our concept for an end-to-end perception architecture, named DeepSTEP. The deep learning-based architecture processes raw sensor data from the camera, LiDAR, and RaDAR, and combines the extracted data in a deep fusion network. The output of this deep fusion network is a shared feature space, which is used by perception head networks to fulfill several perception tasks, such as object detection or local mapping. DeepSTEP incorporates multiple ideas to advance state of the art: First, combining detection and localization into a single pipeline allows for efficient processing to reduce computational overhead and further improves overall performance. Second, the architecture leverages the temporal domain by using a self-attention mechanism that focuses on the most important features. We believe that our concept of DeepSTEP will advance the development of end-to-end perception systems. The network will be deployed on our research vehicle, which will be used as a platform for data collection, real-world testing, and validation. In conclusion, DeepSTEP represents a significant advancement in the field of perception for autonomous vehicles. The architecture's end-to-end design, time-aware attention mechanism, and integration of multiple perception tasks make it a promising solution for real-world deployment. This research is a work in progress and presents the first concept of establishing a novel perception pipeline.}},
  citation     = {0},
  groups       = {Camera-Radar-Lidar fusion},
  journaltitle = {ArXiv e-prints},
  keywords     = {Computer Vision and Pattern Recognition (cs.CV)},
  shortjournal = {arXiv},
  url          = {https://doi.org/10.48550/arXiv.2305.06820},
}

@Article{Gu2023May,
  author       = {Gu, Junyi and Lind, Artjom and Chhetri, Tek Raj and Bellone, Mauro and Sell, Raivo},
  title        = {{End-to-End Multimodal Sensor Dataset Collection Framework for Autonomous Vehicles}},
  year         = {2023},
  month        = may,
  abstract     = {{Autonomous driving vehicles rely on sensors for the robust perception of surroundings. Such vehicles are equipped with multiple perceptive sensors with a high level of redundancy to ensure safety and reliability in any driving condition. However, multi-sensor systems bring up the requirements related to sensor calibration and synchronization, which are the fundamental blocks of any autonomous system. On the other hand, sensor fusion and integration have become important aspects of autonomous driving research and directly determine the efficiency and accuracy of advanced functions such as object detection and path planning. Classical model-based estimation and data-driven models are two mainstream approaches to achieving such integration. Most recent research is shifting to the latter, showing high robustness in real-world applications but requiring large quantities of data to be collected, synchronized, and properly categorized. To generalize the implementation of the multi-sensor perceptive system, we introduce an end-to-end generic sensor dataset collection framework that includes both hardware deploying solutions and sensor fusion algorithms. The framework prototype combines camera and range sensors LiDAR and radar. Furthermore, we present a universal toolbox to calibrate and synchronize three types of sensors based on their characteristics. The framework also includes the fusion algorithms, which utilize the merits of three sensors, and fuse their sensory information in a manner that is helpful for object detection and tracking research. The generality of this framework makes it applicable in any robotic or autonomous applications, also suitable for quick and large-scale practical deployment.}},
  citation     = {0},
  groups       = {Camera-Radar-Lidar fusion},
  journaltitle = {Preprints},
  keywords     = {multimodal sensors, autonomous driving, dataset collection framework, sensor calibration and synchronization, sensor fusion},
  publisher    = {Preprints},
  url          = {https://doi.org/10.20944/preprints202305.1376.v1},
}

@Article{Cui2023May,
  author       = {Cui, Can and Ma, Yunsheng and Lu, Juanwu and Wang, Ziran},
  title        = {{Radar Enlighten the Dark: Enhancing Low-Visibility Perception for Automated Vehicles with Camera-Radar Fusion}},
  year         = {2023},
  month        = may,
  abstract     = {{Sensor fusion is a crucial augmentation technique for improving the accuracy and reliability of perception systems for automated vehicles under diverse driving conditions. However, adverse weather and low-light conditions remain challenging, where sensor performance degrades significantly, exposing vehicle safety to potential risks. Advanced sensors such as LiDARs can help mitigate the issue but with extremely high marginal costs. In this paper, we propose a novel transformer-based 3D object detection model "REDFormer" to tackle low visibility conditions, exploiting the power of a more practical and cost-effective solution by leveraging bird's-eye-view camera-radar fusion. Using the nuScenes dataset with multi-radar point clouds, weather information, and time-of-day data, our model outperforms state-of-the-art (SOTA) models on classification and detection accuracy. Finally, we provide extensive ablation studies of each model component on their contributions to address the above-mentioned challenges. Particularly, it is shown in the experiments that our model achieves a significant performance improvement over the baseline model in low-visibility scenarios, specifically exhibiting a 31.31{\%} increase in rainy scenes and a 46.99{\%} enhancement in nighttime scenes.The source code of this study is publicly available.}},
  citation     = {0},
  groups       = {Camera-Radar fusion},
  journaltitle = {ArXiv e-prints},
  keywords     = {Computer Vision and Pattern Recognition (cs.CV), Artificial Intelligence (cs.AI), Machine Learning (cs.LG)},
  shortjournal = {arXiv},
  url          = {https://doi.org/10.48550/arXiv.2305.17318},
}

@Article{Chen2022Mar,
  author       = {Chen, Xuanyao and Zhang, Tianyuan and Wang, Yue and Wang, Yilun and Zhao, Hang},
  title        = {{FUTR3D: A Unified Sensor Fusion Framework for 3D Detection}},
  year         = {2022},
  month        = mar,
  abstract     = {{Sensor fusion is an essential topic in many perception systems, such as autonomous driving and robotics. Existing multi-modal 3D detection models usually involve customized designs depending on the sensor combinations or setups. In this work, we propose the first unified end-to-end sensor fusion framework for 3D detection, named FUTR3D, which can be used in (almost) any sensor configuration. FUTR3D employs a query-based Modality-Agnostic Feature Sampler (MAFS), together with a transformer decoder with a set-to-set loss for 3D detection, thus avoiding using late fusion heuristics and post-processing tricks. We validate the effectiveness of our framework on various combinations of cameras, low-resolution LiDARs, high-resolution LiDARs, and Radars. On NuScenes dataset, FUTR3D achieves better performance over specifically designed methods across different sensor combinations. Moreover, FUTR3D achieves great flexibility with different sensor configurations and enables low-cost autonomous driving. For example, only using a 4-beam LiDAR with cameras, FUTR3D (58.0 mAP) achieves on par performance with state-of-the-art 3D detection model CenterPoint (56.6 mAP) using a 32-beam LiDAR.}},
  citation     = {0},
  groups       = {Camera-Radar-Lidar fusion},
  journaltitle = {ArXiv e-prints},
  keywords     = {Computer Vision and Pattern Recognition (cs.CV)},
  shortjournal = {arXiv},
  url          = {https://doi.org/10.48550/arXiv.2203.10642},
}

@Misc{Man2023,
  author   = {Man, Yunze and Gui, Liang-Yan and Wang, Yu-Xiong},
  note     = {[Online; accessed 1. Jun. 2023]},
  title    = {{BEV-Guided Multi-Modality Fusion for Driving Perception}},
  year     = {2023},
  citation = {0},
  groups   = {Camera-Radar-Lidar fusion},
  pages    = {21960--21969},
  url      = {https://openaccess.thecvf.com/content/CVPR2023/html/Man_BEV-Guided_Multi-Modality_Fusion_for_Driving_Perception_CVPR_2023_paper.html},
}

@Article{Ogunrinde2023May,
  author       = {Ogunrinde, Isaac Oluwadunsin and Bernadin, Shonda},
  title        = {{Deep Camera-Radar Fusion with Attention Framework for Autonomous Vehicle Vision in Foggy Weather Conditions}},
  year         = {2023},
  month        = may,
  abstract     = {{AVs suffer reduced maneuverability and performance due to the degradation in sensor performances in fog. Such degradation causes significant object detection errors essential for AVs' safety-critical conditions. For instance, YOLOv5 performs significantly well under favorable weather but suffers miss detections and false positives due to atmospheric scattering caused by fog particles. Existing deep object detection techniques often exhibit a high degree of accuracy. The drawback is being sluggish at object detection in fog. Object detection methods with fast detection speed have been obtained using deep learning at the expense of accuracy. The problem of the lack of balance between detection speed and accuracy in fog persist. This paper presents an improved YOLOv5-based multi-sensor fusion network that combines radar's object detection with a camera image bounding box. We transformed radar detection by mapping the radar detections into a two-dimensional image coordinate and projected the resultant radar image on the camera image. Using the attention mechanism, we emphasized and improved important feature representation used for object detection while reducing high-level feature information loss. We trained and tested our multi-sensor fusion network on clear and multi-fog weather datasets obtained from the CARLA simulator. Our result shows that the proposed method significantly enhances the detection of distant and small objects. Our small CR-YOLOnet model best strikes a balance between accuracy and speed with an accuracy of 0.849 at 69 fps.}},
  citation     = {0},
  groups       = {Camera-Radar fusion},
  journaltitle = {Preprints},
  keywords     = {Sensor fusion, object detection, deep learning, autonomous vehicles, camera-radar, adverse weather, fog, attention module},
  publisher    = {Preprints},
  url          = {https://doi.org/10.20944/preprints202305.2180.v1},
}

@Article{Zhang2022Jul,
  author       = {Zhang, Cheng and Wang, Hai and Cai, Yingfeng and Chen, Long and Li, Yicheng and Sotelo, Miguel Angel and Li, Zhixiong},
  title        = {{Robust-FusionNet: Deep Multimodal Sensor Fusion for 3-D Object Detection Under Severe Weather Conditions}},
  year         = {2022},
  issn         = {1557-9662},
  month        = jul,
  pages        = {1--13},
  volume       = {71},
  abstract     = {{The LiDAR point cloud data and camera images are distorted to a different degree under various severe weather conditions. Due to this, the traditional single-modal object detection methods are unable to use the complementary information between different sensors. Consequently, these algorithms are unable to address various issues caused by severe weather conditions. Recently, the multimodal data fusion methods are applied to the road object detection under severe weather conditions. However, the multimodal algorithms suffer from low data alignment accuracy and the inability to suppress the changes in exposure under severe weather conditions. In this work, we propose a new multimodal sensor fusion object detection network. The proposed network effectively overcomes the shortcomings caused by the camera and LiDAR distortions in severe weather conditions and achieves robust environment perception. We propose: 1) pointwise aligned data fusion method based on K-means++ clustering to improve the accuracy of data alignment; 2) implicit feature pyramid network (i-FPN) to fuse the image features for suppressing the distortions caused by the changes in exposure; and 3) hybrid attention mechanism (HAM) to deal with the fusion features and improve the adaptability toward different working conditions. We perform experiments using the One Million Scenes for Autonomous Driving (ONCE) and Karlsruhe Institute of Technology and Toyota Technological Institute (KITTI) datasets. The experimental results and analysis show that the proposed method effectively improves the performance of multimodal deep fusion network under both clear and severe weather conditions.}},
  citation     = {2},
  journaltitle = {IEEE Transactions on Instrumentation and Measurement},
  publisher    = {IEEE},
  shortjournal = {IEEE Trans. Instrum. Meas.},
  url          = {https://doi.org/10.1109/TIM.2022.3191724},
}

@Article{Srivastav2023Jun,
  author       = {Srivastav, Arvind and Mandal, Soumyajit},
  title        = {{Radars for Autonomous Driving: A Review of Deep Learning Methods and Challenges}},
  year         = {2023},
  month        = jun,
  abstract     = {{Radar is a key component of the suite of perception sensors used for safe and reliable navigation of autonomous vehicles. Its unique capabilities include high-resolution velocity imaging, detection of agents in occlusion and over long ranges, and robust performance in adverse weather conditions. However, the usage of radar data presents some challenges: it is characterized by low resolution, sparsity, clutter, high uncertainty, and lack of good datasets. These challenges have limited radar deep learning research. As a result, current radar models are often influenced by lidar and vision models, which are focused on optical features that are relatively weak in radar data, thus resulting in under-utilization of radar's capabilities and diminishing its contribution to autonomous perception. This review seeks to encourage further deep learning research on autonomous radar data by 1) identifying key research themes, and 2) offering a comprehensive overview of current opportunities and challenges in the field. Topics covered include early and late fusion, occupancy flow estimation, uncertainty modeling, and multipath detection. The paper also discusses radar fundamentals and data representation, presents a curated list of recent radar datasets, and reviews state-of-the-art lidar and vision models relevant for radar research. For a summary of the paper and more results, visit the website: this http URL.}},
  citation     = {0},
  journaltitle = {ArXiv e-prints},
  keywords     = {Computer Vision and Pattern Recognition (cs.CV)},
  priority     = {prio1},
  shortjournal = {arXiv},
  url          = {https://doi.org/10.48550/arXiv.2306.09304},
}

@Article{Sun2023Jul,
  author   = {Sun, Huawei and Feng, Hao and Stettinger, Georg and Servadei, Lorenzo and Wille, Robert},
  journal  = {arXiv},
  title    = {{Multi-Task Cross-Modality Attention-Fusion for 2D Object Detection}},
  year     = {2023},
  month    = jul,
  abstract = {{Accurate and robust object detection is critical for autonomous driving. Image-based detectors face difficulties caused by low visibility in adverse weather conditions. Thus, radar-camera fusion is of particular interest but presents challenges in optimally fusing heterogeneous data sources. To approach this issue, we propose two new radar preprocessing techniques to better align radar and camera data. In addition, we introduce a Multi-Task Cross-Modality Attention-Fusion Network (MCAF-Net) for object detection, which includes two new fusion blocks. These allow for exploiting information from the feature maps more comprehensively. The proposed algorithm jointly detects objects and segments free space, which guides the model to focus on the more relevant part of the scene, namely, the occupied space. Our approach outperforms current state-of-the-art radar-camera fusion-based object detectors in the nuScenes dataset and achieves more robust results in adverse weather conditions and nighttime scenarios.}},
  citation = {0},
  groups   = {Camera-Radar fusion},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Artificial Intelligence (cs.AI), Multimedia (cs.MM)},
  priority = {prio1},
  url      = {https://arxiv.org/abs/2307.08339v1},
  urldate  = {2023-07-24},
}

@Article{Guan2023Jul,
  author   = {Guan, Runwei and Yao, Shanliang and Zhu, Xiaohui and Man, Ka Lok and Lim, Eng Gee and Smith, Jeremy and Yue, Yong and Yue, Yutao},
  journal  = {arXiv},
  title    = {{Achelous: A Fast Unified Water-surface Panoptic Perception Framework based on Fusion of Monocular Camera and 4D mmWave Radar}},
  year     = {2023},
  month    = jul,
  abstract = {{Current perception models for different tasks usually exist in modular forms on Unmanned Surface Vehicles (USVs), which infer extremely slowly in parallel on edge devices, causing the asynchrony between perception results and USV position, and leading to error decisions of autonomous navigation. Compared with Unmanned Ground Vehicles (UGVs), the robust perception of USVs develops relatively slowly. Moreover, most current multi-task perception models are huge in parameters, slow in inference and not scalable. Oriented on this, we propose Achelous, a low-cost and fast unified panoptic perception framework for water-surface perception based on the fusion of a monocular camera and 4D mmWave radar. Achelous can simultaneously perform five tasks, detection and segmentation of visual targets, drivable-area segmentation, waterline segmentation and radar point cloud segmentation. Besides, models in Achelous family, with less than around 5 million parameters, achieve about 18 FPS on an NVIDIA Jetson AGX Xavier, 11 FPS faster than HybridNets, and exceed YOLOX-Tiny and Segformer-B0 on our collected dataset about 5 mAP$_{\text{50-95}}$ and 0.7 mIoU, especially under situations of adverse weather, dark environments and camera failure. To our knowledge, Achelous is the first comprehensive panoptic perception framework combining vision-level and point-cloud-level tasks for water-surface perception. To promote the development of the intelligent transportation community, we release our codes in {\ifmmode\backslash\else\textbackslash\fi}url{$\lbrace$}this https URL{$\rbrace$}.}},
  citation = {0},
  groups   = {Camera-Radar fusion},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Robotics (cs.RO)},
  url      = {https://arxiv.org/abs/2307.07102v1},
  urldate  = {2023-07-24},
}

@Article{Liu2023Jul,
  author   = {Liu, Liu and Zhi, Shuaifeng and Du, Zhenhua and Liu, Li and Zhang, Xinyu and Huo, Kai and Jiang, Weidong},
  journal  = {arXiv},
  title    = {{ROFusion: Efficient Object Detection using Hybrid Point-wise Radar-Optical Fusion}},
  year     = {2023},
  month    = jul,
  abstract = {{Radars, due to their robustness to adverse weather conditions and ability to measure object motions, have served in autonomous driving and intelligent agents for years. However, Radar-based perception suffers from its unintuitive sensing data, which lack of semantic and structural information of scenes. To tackle this problem, camera and Radar sensor fusion has been investigated as a trending strategy with low cost, high reliability and strong maintenance. While most recent works explore how to explore Radar point clouds and images, rich contextual information within Radar observation are discarded. In this paper, we propose a hybrid point-wise Radar-Optical fusion approach for object detection in autonomous driving scenarios. The framework benefits from dense contextual information from both the range-doppler spectrum and images which are integrated to learn a multi-modal feature representation. Furthermore, we propose a novel local coordinate formulation, tackling the object detection task in an object-centric coordinate. Extensive results show that with the information gained from optical images, we could achieve leading performance in object detection (97.69{\ifmmode\backslash\else\textbackslash\fi}{\%} recall) compared to recent state-of-the-art methods FFT-RadNet (82.86{\ifmmode\backslash\else\textbackslash\fi}{\%} recall). Ablation studies verify the key design choices and practicability of our approach given machine generated imperfect detections. The code will be available at this https URL.}},
  citation = {0},
  groups   = {Camera-Radar fusion},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Artificial Intelligence (cs.AI)},
  url      = {https://arxiv.org/abs/2307.08233v1},
  urldate  = {2023-07-24},
}

@Article{Xiong2023Jul,
  author   = {Xiong, Weiyi and Liu, Jianan and Huang, Tao and Han, Qing-Long and Xia, Yuxuan and Zhu, Bing},
  journal  = {arXiv},
  title    = {{LXL: LiDAR Excluded Lean 3D Object Detection with 4D Imaging Radar and Camera Fusion}},
  year     = {2023},
  month    = jul,
  abstract = {{As an emerging technology and a relatively affordable device, the 4D imaging radar has already been confirmed effective in performing 3D object detection in autonomous driving. Nevertheless, the sparsity and noisiness of 4D radar point clouds hinder further performance improvement, and in-depth studies about its fusion with other modalities are lacking. On the other hand, most of the camera-based perception methods transform the extracted image perspective view features into the bird's-eye view geometrically via "depth-based splatting" proposed in Lift-Splat-Shoot (LSS), and some researchers exploit other modals such as LiDARs or ordinary automotive radars for enhancement. Recently, a few works have applied the "sampling" strategy for image view transformation, showing that it outperforms "splatting" even without image depth prediction. However, the potential of "sampling" is not fully unleashed. In this paper, we investigate the "sampling" view transformation strategy on the camera and 4D imaging radar fusion-based 3D object detection. In the proposed model, LXL, predicted image depth distribution maps and radar 3D occupancy grids are utilized to aid image view transformation, called "radar occupancy-assisted depth-based sampling". Experiments on VoD and TJ4DRadSet datasets show that the proposed method outperforms existing 3D object detection methods by a significant margin without bells and whistles. Ablation studies demonstrate that our method performs the best among different enhancement settings.}},
  citation = {0},
  groups   = {Camera-Radar fusion, 3D},
  keywords = {Computer Vision and Pattern Recognition (cs.CV)},
  url      = {https://arxiv.org/abs/2307.00724v2},
  urldate  = {2023-07-27},
}

@Article{Liu2023Jul,
  author   = {Liu, Jianan and Zhao, Qiuchi and Xiong, Weiyi and Huang, Tao and Han, Qing-Long and Zhu, Bing},
  journal  = {arXiv},
  title    = {{SMURF: Spatial Multi-Representation Fusion for 3D Object Detection with 4D Imaging Radar}},
  year     = {2023},
  month    = jul,
  abstract = {{The 4D Millimeter wave (mmWave) radar is a promising technology for vehicle sensing due to its cost-effectiveness and operability in adverse weather conditions. However, the adoption of this technology has been hindered by sparsity and noise issues in radar point cloud data. This paper introduces spatial multi-representation fusion (SMURF), a novel approach to 3D object detection using a single 4D imaging radar. SMURF leverages multiple representations of radar detection points, including pillarization and density features of a multi-dimensional Gaussian mixture distribution through kernel density estimation (KDE). KDE effectively mitigates measurement inaccuracy caused by limited angular resolution and multi-path propagation of radar signals. Additionally, KDE helps alleviate point cloud sparsity by capturing density features. Experimental evaluations on View-of-Delft (VoD) and TJ4DRadSet datasets demonstrate the effectiveness and generalization ability of SMURF, outperforming recently proposed 4D imaging radar-based single-representation models. Moreover, while using 4D imaging radar only, SMURF still achieves comparable performance to the state-of-the-art 4D imaging radar and camera fusion-based method, with an increase of 1.22{\%} in the mean average precision on bird's-eye view of TJ4DRadSet dataset and 1.32{\%} in the 3D mean average precision on the entire annotated area of VoD dataset. Our proposed method demonstrates impressive inference time and addresses the challenges of real-time detection, with the inference time no more than 0.05 seconds for most scans on both datasets. This research highlights the benefits of 4D mmWave radar and is a strong benchmark for subsequent works regarding 3D object detection with 4D imaging radar.}},
  citation = {0},
  groups   = {3D},
  keywords = {Computer Vision and Pattern Recognition (cs.CV)},
  url      = {https://arxiv.org/abs/2307.10784v1},
  urldate  = {2023-07-27},
}

@Comment{jabref-meta: databaseType:bibtex;}

@Comment{jabref-meta: grouping:
0 AllEntriesGroup:;
1 StaticGroup:Robotics\;0\;1\;0xccb3ffff\;\;\;;
1 StaticGroup:Perception\;2\;0\;0x008080ff\;\;\;;
2 StaticGroup:Adverse Weather\;2\;0\;0x00ffffff\;\;\;;
3 StaticGroup:Lidar only\;0\;0\;0x8a8a8aff\;\;\;;
3 StaticGroup:Restoration\;0\;1\;0x8a8a8aff\;\;\;;
2 StaticGroup:Event Camera\;0\;1\;0x8a8a8aff\;\;\;;
2 StaticGroup:Temporal\;0\;1\;0x8a8a8aff\;\;\;;
1 StaticGroup:Sensor Fusion\;2\;1\;0x0000ffff\;\;\;;
2 StaticGroup:3D\;0\;1\;0x8a8a8aff\;\;\;;
2 StaticGroup:Datasets\;2\;0\;0xff00ffff\;\;\;;
3 StaticGroup:Camera-Lidar\;0\;0\;0x800080ff\;\;\;;
3 StaticGroup:Camera-Radar\;0\;1\;0xff0000ff\;\;\;;
3 StaticGroup:Camera-Radar-Lidar\;0\;1\;0xff9999ff\;\;\;;
3 StaticGroup:Misc\;0\;1\;0x8a8a8aff\;\;\;;
2 StaticGroup:Fusion Methods\;2\;1\;0xffff00ff\;\;\;;
3 StaticGroup:Camera-Radar-Lidar fusion\;0\;1\;0xffe666ff\;\;\;;
3 StaticGroup:Camera-Radar fusion\;0\;0\;0x008000ff\;\;\;;
3 StaticGroup:Camera-Lidar fusion\;0\;0\;0x808000ff\;\;\;;
3 StaticGroup:Lidar-Radar fusion\;0\;1\;0x003300ff\;\;\;;
3 StaticGroup:Misc fusion\;0\;1\;0x800000ff\;\;\;;
2 StaticGroup:Review\;0\;1\;0x333333ff\;\;\;;
2 StaticGroup:Survey\;0\;1\;0x8a8a8aff\;\;\;;
}
